{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07bd2d3c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <div align=\"center\"> SPECIAL TOPICS III </div>\n",
    "## <div align=\"center\"> Data Science for Social Scientists  </div>\n",
    "### <div align=\"center\"> ECO 4199 </div>\n",
    "#### <div align=\"center\">Class 5 - Introduction to Prediction</div>\n",
    "<div align=\"center\"> Jonathan Holmes, (he/him)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58269dc",
   "metadata": {},
   "source": [
    "## From now on, we cover Statistical Learning\n",
    "- For almost all lectures we will follow closely the book [Introduction to Statistical Learning](http://faculty.marshall.usc.edu/gareth-james/) 2nd edition\n",
    "- Coding will no longer be at the heart of what we are doing.\n",
    "- But we will illustrate intuitions using code.\n",
    "- Unlike before, the code will already be made available to you. You will only need to run it.\n",
    "    - Some of this code is coming from this [set of scripts](https://github.com/JWarmenhoven/ISLR-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830b8b42",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Roadmap: What we'll see in the rest of this course\n",
    "\n",
    "1. This class: Introduction to ML terms, how to evaluate models\n",
    "2. Linear Regression: Your econometrics class re-interpreted\n",
    "3. Classification: Predicting Y when it is a Boolean\n",
    "4. Re-sampling: Clever ways of using random samples\n",
    "5. Model Selection: Better linear models for prediction\n",
    "6. Deep Learning: The hottest thing in Silicon Valley\n",
    "7. Social Bias and Prediction: What's wrong with ML? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce95341a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction: Is my house on fire?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ee2e0d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"> <img src=\"Smoke_detector.JPG\" /> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0745b151",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Simple Fire Alarm\n",
    "#### \n",
    "\n",
    "\n",
    "<div align=\"center\"> Smoke sensor --> Fire Detection Algorithm --> Is there a fire? </div>\n",
    "\n",
    "Simple fire detection algorithm: \n",
    "- The alarm rings if smoke > some threshold ($\\alpha$)\n",
    "- Alarm company chooses $\\alpha$ to make accurate predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01424f43",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notation: \n",
    "- Smoke sensor measures $x$\n",
    "- Fire detection algorithm: $\\hat{f}(x) = x > \\alpha$\n",
    "- Output: $\\hat{y} = \\hat{f}(x)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fcca7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Simple Fire Alarm\n",
    "#### \n",
    "\n",
    "\n",
    "<div align=\"center\"> Smoke sensor --> Fire Detection Algorithm --> Is there a fire? </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de08875",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "True relationship:\n",
    "<div align=\"center\">  $x \\, \\,$ --> $\\, \\,f(x)\\, \\,$ -- > $\\, \\,y$ </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a71683",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Estimated relationship:\n",
    "<div align=\"center\">  $x \\, \\,$ --> $\\, \\, \\hat{f}(x) = x > \\alpha \\, \\,$ --> $\\, \\, \\hat{y}$ </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65800c77",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $x$ is an _input variable_ (also known as predictors, indpendent variables, features)\n",
    "- $y$ is an _output variable_ (also known as response or dependent variable)\n",
    "- $f(x)$ is some function linking $x$ to $y$\n",
    "- $\\alpha$ is a _parameter_ \n",
    "- $\\hat{y}$ and $\\hat{f}$ are _estimates_ (Or predicted values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4776d95",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Complicated fire alarm\n",
    "\n",
    "\n",
    "- We can have many sensors: Heat, noise, smoke, light, etc. \n",
    "- Call these $x_1$, $x_2$, $x_3$, $x_4$, etc. \n",
    "- True relationship: $y = f(x_1, x_2, x_3, x_4) + \\varepsilon$ \n",
    "- $\\varepsilon$ is a random error term. \n",
    "\n",
    "\n",
    "Question: In general, we don't know $f$. How do we figure out what it is?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc42c18",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notation: \n",
    "- Bold face variables are _vectors_\n",
    "- For example: $X = \\{x_1, x_2, x_3, x_4\\}$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40bfae4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Machine Learning Approach\n",
    "\n",
    "1. Gather lots of data on $Y$ along with $X = \\{x_1, x_2, x_3, x_4\\}$\n",
    "\n",
    "2. Select an appropriate function $\\hat{f}(\\mathbf{X}; \\mathbf{\\psi})$ that allows us to get a good approximation of $f$.   \n",
    "\n",
    "3. Split the data into two parts: \n",
    "    - Estimate the parameters $\\hat{\\psi}$ from the function $\\hat{f}$ using the \"training\" data\n",
    "    - Evaluate the performance of $\\hat{f}$ using the \"test\" data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc57d5f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why are ML models getting so good? \n",
    "1. Gigantic datasets (GPT3 used 45 Terabytes of text)\n",
    "2. Gigantic models (GPT3 has 175 billion parameters)\n",
    "3. LOTS of computing power, advances in algorithm design and estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5358be06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning Model Examples ($\\hat{f}$)\n",
    "\n",
    "- Linear models (Least Squares)\n",
    "- Lasso \n",
    "- Regression Trees, Random Forest\n",
    "- Deep Learning\n",
    "- Transformer Models (eg: ChatGPT, Dall-E)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2770f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assessing Model Accuracy \n",
    "- Why introduce so many different statistical learning approaches? \n",
    "    - No one method dominates all others for every question and every dataset\n",
    "     \n",
    "- Selecting the best approach can be one of the most challenging parts of performing statistical learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c14148",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choosing a fire alarm algorithm\n",
    "\n",
    "Question: You are the boss of a fire alarm company. Your engineers give you two different fire detection algorithms. They gathered data on many cases of fires and non-fires, and here are the results\n",
    "\n",
    "Algorithm A: \n",
    "- When there is no fire: alarm rings 5% of the time\n",
    "- When there is a fire, alarm rings 99% of the time\n",
    "\n",
    "Algorithm B: \n",
    "- When there is no fire, alarm rings 2% of the time\n",
    "- When there is a fire, alarm rings 90% of the time\n",
    "\n",
    "Which algorithm would you choose and why?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bfa233",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Assessing Model Accuracy\n",
    "\n",
    "The truth: \n",
    "\n",
    "<div align=\"center\"> $Y = f(\\mathbf{X}) + \\varepsilon$ </div>\n",
    "\n",
    "What we estimate:\n",
    "\n",
    "<div align=\"center\"> $\\hat{Y} = \\hat{f}(\\mathbf{X})$ </div>\n",
    "\n",
    "The difference: \n",
    "\n",
    "<div align=\"center\"> $Y - \\hat{Y} = f(\\mathbf{X}) - \\hat{f}(\\mathbf{X}) + \\varepsilon$ </div>\n",
    "\n",
    "$Y - \\hat{Y}$ is the _error_\n",
    "- Reducible error: $f(\\mathbf{X}) - \\hat{f}(\\mathbf{X})$\n",
    "- Irreducible error: $\\varepsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537aeebb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Assessing Model Accuracy 2\n",
    "\n",
    "Traditionally, we refer to the _variance_ of the error, not the error itself.\n",
    "- Variance is always positive\n",
    "- Higher penalty for larger errors\n",
    "\n",
    "We care about the expected value (average) of the error\n",
    "\n",
    "\n",
    "<div align=\"center\"> $E[Y - \\hat{f}(\\mathbf{X})]^2 = E(f(\\mathbf{X}) - \\hat{f}(\\mathbf{X}))^2 + Var[\\varepsilon]$ </div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eda7d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Measuring the quality of fit in regressions\n",
    "\n",
    "- In the regression setting, the most commonly-used measure is the __mean squared error__ (MSE)\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{MSE} = \\large \\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\hat{f}(x_i))^2\n",
    "\\end{equation*}\n",
    "\n",
    "- $\\hat{f}$ represents the prediction of our function for the data point $x_i$\n",
    "- The better the prediction the smaller the distance between $y_i$ and $\\hat{f}(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08deb2ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: \n",
    "\n",
    "Recall the two different fire alarm algorithms. \n",
    "\n",
    "Algorithm A: \n",
    "- When there is no fire: alarm rings 5% of the time\n",
    "- When there is a fire, alarm rings 99% of the time\n",
    "\n",
    "Algorithm B: \n",
    "- When there is no fire, alarm rings 2% of the time\n",
    "- When there is a fire, alarm rings 90% of the time\n",
    "\n",
    "Q1: Which of these two algorithms do you think has a lower mean squared error? (Reminder: Fires are rare)\n",
    "\n",
    "Q2: When should we try to minimize mean squared error, and when is it appropriate to have a different goal? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecf12e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Assessing Model Accuracy: An example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1d6275",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# the basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "# Some reporting tools\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "#the stats packages\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "# Checking the versions of our packages\n",
    "for p in [np,pd, matplotlib, sklearn,sns,statsmodels]:\n",
    "    print(f\"{p.__name__.capitalize()} uses the version {p.__version__}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe01182",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mean square error in practice\n",
    "\n",
    "- Let's first define a function that relates y to X\n",
    "\n",
    "    $$y=f(x)=\\cos(1.5\\times \\large\\pi x)$$\n",
    "\n",
    "- In the population there is some noise around the relationship so that:\n",
    "$$ y_i = \\cos(1.5\\times \\pi x_i) + \\varepsilon_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f19feb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's generate some fake data corresponding to our function\n",
    "# True function is 1.5*pi\n",
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "# set seed to random numbers are always the same\n",
    "np.random.seed(0)\n",
    "# set the number of observations\n",
    "n_samples = 30\n",
    "# Generate X and y\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
    "# store in a dataframe\n",
    "df=pd.DataFrame({'x':X,'y':y})\n",
    "#display(df.head()) # show head\n",
    "# scatter the relationship between X and y in our \"population\"\n",
    "df.plot.scatter('x','y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289a4e86",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding the right function\n",
    "- Say that you did not plot the data before estimating your function (sad!)\n",
    "- Being trained as an economist your best guess is that the relationship between X and y is... linear!\n",
    "- Let's find the line of best fit and compute the MSE.\n",
    "- We will use [Scikit-learn](https://scikit-learn.org/stable/) tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9310591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_test = np.linspace(0, 1, 100)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab30d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "###### estimate the line of best fit and compare to the true function\n",
    "\n",
    "### estimate the linear relationship between X and y\n",
    "linear_regression = LinearRegression(fit_intercept=True) # initiate a linear regression class from scikit-learn LinearRegression\n",
    "linear_regression.fit(df['x'].values.reshape(-1,1),y) # fit the data using this class; .reshape(-1, 1) allows to make is an (n,) vector\n",
    "\n",
    "# Summary of the results\n",
    "print(f\" Estimated slope: {linear_regression.coef_[0]}\")\n",
    "def estimated_fun(x):\n",
    "    return linear_regression.intercept_+ linear_regression.coef_[0]*x\n",
    "\n",
    "# Create 500 equally spaced values for X between 0 and 1\n",
    "X_test = np.linspace(0, 1, 500)\n",
    "# predict values of y use X_test\n",
    "y_pred=estimated_fun(X_test)\n",
    "\n",
    "# initiate plot\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,10))\n",
    "# plot the true function\n",
    "sns.lineplot( x=X_test, y=true_fun(X_test),color='darkorange',label='True Function',ax=ax)\n",
    "#plot the estimated (linear) relationship\n",
    "sns.lineplot( x=X_test, y=y_pred,color='darkgreen',label='Estimated function',ax=ax)\n",
    "# plot the scatter plot of the actual data\n",
    "sns.scatterplot(x=X, y=y, label='Observations',ax=ax,marker=\"P\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e35d7dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How good is this linear fit?\n",
    "- Given the true function it was unlikely our line would give us a satisfying fit\n",
    "- Let's see if we can improve on that\n",
    "- but first what's our mean squared errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47c2c45",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Compare predicted to actual values for y\n",
    "# create your own MSE function\n",
    "def mse_fun(y,y_hat):\n",
    "    \"\"\"\n",
    "    Take a y and a predicted y\n",
    "    returns the MSE\n",
    "    \"\"\"\n",
    "    diff_squared=np.square(y-y_hat)\n",
    "    return np.mean(diff_squared)\n",
    "# predict y using our estimated function, using our 30 observations\n",
    "y_hat=estimated_fun(df['x'])   \n",
    "# Print out our MSE\n",
    "mse_degree1=mse_fun(y=y,y_hat=y_hat)\n",
    "print(f\"The MSE for the linear fit is {round(mse_degree1,3)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef4b08a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Since we are talking about MSE\n",
    "- By the way what do you expect the MSE for the true function is going to be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962ad288",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# predict y, using the true function and our sample of 30 observations\n",
    "y_pred_true_fun=true_fun(X)\n",
    "# Compute the mean squared error of the true function using predicted values\n",
    "mse_true_fun=mse_fun(y=y,y_hat=y_pred_true_fun)\n",
    "print(f\"The MSE for the linear fit is {round(mse_true_fun,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1650fdd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: \n",
    "3: Why is the MSE of the true function not zero?\n",
    "\n",
    "4: How do you expect the MSE of the true function to change as we increase the sample size N?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837acf17",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# New function\n",
    "- How about we introduce the square of our unique variable\n",
    "- This will generate a non linear relationship between X and y, and as we know this relationship is not linear\n",
    "- Before:\n",
    "$y_i = \\beta_0+\\beta_1 x_i$\n",
    "- After:\n",
    "$y_i= \\beta_0+\\beta_1 x_i + \\beta_2 x_i^2$\n",
    "- This is called a __quadratic regression__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a7ab96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create the square of x\n",
    "df['x2']=df['x']**2\n",
    "# reorder columns\n",
    "df=df[['y','x','x2']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751f6e99",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# iniate a linear regression class\n",
    "quad_regression = LinearRegression()\n",
    "# fit parameters on x and x-squared to best predict y\n",
    "quad_regression.fit(df.iloc[:,df.columns.str.contains('x')],df['y'])\n",
    " \n",
    "print(\"***Estimated parameters:***\")\n",
    "print('Intercept:',round(quad_regression.intercept_,2))\n",
    "print('\\u03B2\\u0302\\u2081:',round(quad_regression.coef_[0],2))\n",
    "print('\\u03B2\\u0302\\u2082:',round(quad_regression.coef_[1],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea0413b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predict\n",
    "- You don't have to create your own function each time!\n",
    "- Scitkit-learn got you covered!\n",
    "- Let's now predict for all values of X between 0 and 1 (note how it differs from the 30 observations we fitted our data on!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2e4c38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# pred y based on the X_test and X_test squared\n",
    "y_pred_x2_test= quad_regression.predict(np.array([X_test,X_test**2]).T) # test set\n",
    "y_hat_x2 = quad_regression.predict(df[['x','x2']]) # use the results from quad_regression to predict y using our sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a76e15b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# initiate plot\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,10))\n",
    "# plot the true function\n",
    "sns.lineplot( x=X_test, y=true_fun(X_test),color='darkorange',label='True Function',ax=ax)\n",
    "#plot the estimated (quadratic) relationship\n",
    "sns.lineplot( x=X_test, y=y_pred_x2_test,color='darkgreen',label='Estimated function',ax=ax)\n",
    "# plot the scatter plot of the actual data\n",
    "mse_degree2=mse_fun(y=y,y_hat=y_hat_x2)\n",
    "sns.scatterplot(x=X, y=y, label='Observations',ax=ax,marker=\"P\")\n",
    "ax.text(.3,1.5,f\"Mean Square Error: {round(mse_degree2,4)}\", fontsize=12)\n",
    "plt.show()\n",
    "md(f\"So adding a squared term improved our MSE from {round(mse_degree1,4)} to {round(mse_degree2,4)}.\\n This is much closer to the best MSE using the true function of {round(mse_true_fun,4)} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0add52ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Polynomial regression\n",
    "- Clearly allowing for non-linearity went a long way in improving the fit\n",
    "- What if we extended this function to higher and higher terms?\n",
    "    - So far we used regressions of degree 1 and 2\n",
    "    - We can extend this to any degree, this is called a polynomial regression\n",
    "- A polynomial of degree n is given by:\n",
    "$$f(x)=\\beta_1 x^1 + \\beta_2 x^2 + \\beta_3 x^3 + ... +\\beta_n x^n $$\n",
    "$$ f(x)= \\sum_1^n \\beta_n x^n$$\n",
    "- This yields the following model(s):\n",
    "$$y_i= \\beta_0 +\\sum_1^n \\beta_n x^n_i + \\varepsilon_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad2a771",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Polynomials and MSE\n",
    "\n",
    "- So what will happen to our MSE if we use higher and higher terms?\n",
    "- Most likely we will allow for ever more __flexible functions__ of X to fit our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0a1490",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Want to try different polynomials?\n",
    "- [Scitkit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) got you covered!\n",
    "- In a loop let's try all polynomials from 1 to 15, save the corresponding MSE and see how it changes with degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ce3f57",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "MSE_list=[] # create empty list to store MSEs for each polynomial regression\n",
    "MSE_list_test = []\n",
    "y_test = true_fun(X_test) + np.random.randn(len(X_test)) * 0.1\n",
    "\n",
    "polynomials=np.arange(1,16) # list of degrees\n",
    "for n in polynomials:\n",
    "    # prepocess the data\n",
    "    polynomial_features =PolynomialFeatures(degree=n,include_bias=False) # initiate a class to get data of degree n without adding the constant term named bias in datascience. If you have read that far I have great feelings regarding your motivation. Case and point you are still reading even though there is no more information to be learned. That's it! In data science the interpect is called bias that's annoying but also now you know it and you can move on with your life. \n",
    "    X_train=polynomial_features.fit_transform(df['x'].values.reshape(-1, 1)) # get data of degree n, we now have n columns from a single column X\n",
    "    # regression\n",
    "    reg = LinearRegression() # initiate the regression class\n",
    "    reg.fit(X_train,y) # fit the data\n",
    "    # predict on the train data\n",
    "    y_hat= reg.predict(X_train)\n",
    "    # get MSE for the fitted data using the train data\n",
    "    mse=mse_fun(y=y,y_hat=y_hat)\n",
    "   \n",
    "    # get MSE for the fitted data using the test data (explained later)\n",
    "    X_test_pol=polynomial_features.fit_transform(X_test.reshape(-1,1))\n",
    "    y_hat_test=reg.predict(X_test_pol)\n",
    "    mse_test = mse_fun(y=y_test, y_hat=y_hat_test)\n",
    "    \n",
    "    # append the result to the loop\n",
    "    MSE_list.append(mse)\n",
    "    MSE_list_test.append(mse_test)\n",
    "\n",
    "# put the results in dataframe\n",
    "mse=pd.DataFrame({'Degree':polynomials, 'MSE':MSE_list, 'MSE_test':MSE_list_test})\n",
    "mse[['Degree', 'MSE']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113a698b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's now plot the relationship between polynomial degrees and our MSE score on our train data\n",
    "- Recall that the best MSE, using the true function, was roughly 0.0125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d95217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(1,1,figsize=(10,10))\n",
    "\n",
    "sns.lineplot(data=mse, x='Degree',y='MSE',ax=ax, color='darkorange')\n",
    "sns.scatterplot(data=mse, x='Degree',y='MSE',ax=ax, color='darkgreen',marker='P')\n",
    "ax.axhline(y=mse_true_fun, color='k',linestyle=\":\")\n",
    "ax.text(12,mse_true_fun+.001,\"MSE of the True Function\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d12e92",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Are higher degrees always better?\n",
    "- It seems  higher and higher polynomials meant that we performed better than... the true function!\n",
    "- Have we improved over the true function?\n",
    "    - We did not\n",
    "    - More importantly we should not\n",
    "- Let's see why using our last estimated model (degree=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263339be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "try: # make sure our last features are indeed of degree 15\n",
    "    polynomial_features.degree==15\n",
    "    X_test_n15=polynomial_features.fit_transform(X_test.reshape(-1,1)) # get data of degree n on a test data\n",
    "    y_pred=reg.predict(X_test_n15) # predict on test data\n",
    "    y_hat=reg.predict(X_train) # predict on train data\n",
    "except:\n",
    "    print(f\"n is not 15 {polynomial_features}\") # warn if try is wrong\n",
    "# initiate plot\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,10))\n",
    "# plot the true function\n",
    "sns.lineplot( x=X_test, y=true_fun(X_test),color='darkorange',label='True Function',ax=ax)\n",
    "#plot the estimated (quadratic) relationship\n",
    "sns.lineplot( x=X_test, y=y_pred,color='darkgreen',label='Estimated function',ax=ax)\n",
    "# plot the scatter plot of the actual data\n",
    "sns.scatterplot(x=X, y=y, label='Observations',ax=ax,marker=\"P\")\n",
    "mse_degree15=round(mse_fun(y=y,y_hat=y_hat),4)\n",
    "ax.text(.3,1.5,f\"Mean Square Error: {mse_degree15}\\n Degree {str(n)}\", fontsize=12)\n",
    "plt.ylim((-2, 2))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4414f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overfitting\n",
    "- What we did here is called __overfitting__:\n",
    "    - Given our dataset we fitted a function to pass very close to each observation $\\{y_i,x_i\\}$\n",
    "- In our graph, the green line is very close to the points\n",
    "- But when there is no points (observations) our prediction is very far from the true function\n",
    "    - See for instance what you would predict for $x\\approx 0.1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad804fa6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In-Sample vs. Out-of-Sample\n",
    "\n",
    "- We are far because we found the function that maximizes __in sample prediction__\n",
    "- Recall that our goal was instead to find the function $\\hat{f}(x)$ that matches the true relationship between X and y \n",
    "- Why? Because we can then feed any x to  $\\hat{f}(x)$, even one __out of sample__ and it should predict y closely\n",
    "    - This is clearly not something our green line (polynomial of degree 15) is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e27fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# initiate plot\n",
    "fig, axes = plt.subplots(1,3, figsize=(15,10),sharey=True, sharex=True)\n",
    "\n",
    "# create test target (out of sample)\n",
    "X_test = np.linspace(0, 1, 100)\n",
    "y_test = true_fun(X_test) + np.random.randn(len(X_test)) * 0.1\n",
    "\n",
    "# Loop of three degrees\n",
    "polynomials=[1,4,15] # list of degrees\n",
    "for i,n in enumerate(polynomials): # i corresponds to the index (0, 1, 2); n takes the values in the polynomial list (1,4,15)\n",
    "    \n",
    "    # prepocess the data\n",
    "    polynomial_features =PolynomialFeatures(degree=n,include_bias=False) # iniate a class to get data of degree n\n",
    "    X_train=polynomial_features.fit_transform(df['x'].values.reshape(-1, 1)) # get data of degree n\n",
    "    \n",
    "    # regression\n",
    "    reg = LinearRegression() # initiate the regression class\n",
    "    reg.fit(X_train,y) # fit the data\n",
    "    \n",
    "    ### predict on the train and test data\n",
    "    # in sample prediction\n",
    "    y_hat_train= reg.predict(X_train) \n",
    "    # out of sample prediction\n",
    "    X_test_pol=polynomial_features.fit_transform(X_test.reshape(-1,1))\n",
    "    y_hat_test=reg.predict(X_test_pol)\n",
    "    \n",
    "    # get MSE for the fitted data using the train data\n",
    "    mse_in_train=mse_fun(y=y,y_hat=y_hat_train)\n",
    "    # get MSE for the fitted data using the test data\n",
    "    mse_in_test=mse_fun(y=y_test,y_hat=y_hat_test)\n",
    "    # get MSE for the true function using the train data\n",
    "    mse_in_train_true_fun=mse_fun(y=y,y_hat=true_fun(X))\n",
    "    # get MSE for the true function using the test data\n",
    "    mse_in_test_true_fun=mse_fun(y=y_test,y_hat=true_fun(X_test))\n",
    "    \n",
    "\n",
    "    # plot the true function\n",
    "    sns.lineplot( x=X_test, y=true_fun(X_test),color='darkorange',label='True Function',ax=axes[i])\n",
    "    # plot the scatter plot of the actual data\n",
    "    sns.scatterplot(x=X, y=y, label='Observations',marker=\"P\",ax=axes[i])\n",
    "    #plot the estimated function\n",
    "    #sns.lineplot( x=X_test, y=y_hat_test,color='darkgreen',label=f'Polynomial of degree {n}',ax=axes[i])\n",
    "    sns.lineplot( x=X_test, y=y_hat_test,color='darkgreen',label=f'Model {i+1}',ax=axes[i])\n",
    "    # Display MSEs\n",
    "    #axes[i].text(.3,1.5,f\"In Sample MSE: {round(mse_in_train,3)}\", fontsize=12)\n",
    "    #axes[i].text(.3,1.4,f\"Out of Sample MSE: {round(mse_in_test,3)}\", fontsize=12)\n",
    "\n",
    "\n",
    "plt.ylim((-1.5, 2))\n",
    "fig.suptitle(f\"MSE for true function: in sample= {round(mse_in_train_true_fun,3)}, out of sample={round(mse_in_test_true_fun,3)}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fe1a7f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Bias-Variance Trade-off\n",
    "\n",
    "- In econometrics, we care about $\\beta$\n",
    "- In machine learning, we care about $\\hat{y} = \\hat{f}(x)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe526af4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "_Bias_: \n",
    "- In econometrics, _bias_ is: $E[\\hat{\\beta}] - \\beta$\n",
    "- In ML, _bias_ refers to cases when the function $\\hat{f}(x)$ is not flexible enough to approximate $f(x)$\n",
    "- ML bias: How close can we approximate f in our given sample? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a873436c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "_Variance_: \n",
    "- In econometrics, we care about $Var(\\hat{\\beta})$ \n",
    "- In ML, we care about $Var(\\hat{f})$\n",
    "- ML variance: How much does our function change when we use a different sample? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc650eee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Bias-Variance Trade-Off\n",
    "\n",
    "If we make the model more complex: \n",
    "- $\\hat{f}(x)$ can better approximate $f(x)$ $\\rightarrow$ Bias goes ___\n",
    "- There is more over-fitting to the current model $\\rightarrow$ Variance goes ___\n",
    "\n",
    "How do we determine the right balance between bias and variance? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b08148",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "## Two Mean Squared Error Estimates: \n",
    "- _In sample MSE_ is the one you are used to and consists in finding the best fit given the available data\n",
    "- _Out of sample MSE_ is the MSE if used on another dataset that was not used to estimate the model\n",
    "\n",
    "In general, we want to choose model complexity to maximize _out of sample MSE_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36283f69",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train vs Test\n",
    "\n",
    "Note: In our current in-class exercise, we know $f(x)$. In general, we don't. \n",
    "\n",
    "How can we measure the out of sample MSE in the real world?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8be7c9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Solution: Split the data in two\n",
    "- Training data: Used to estimate the parameters of the model\n",
    "- Test data: Used to measure out-of-sample Mean-Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6d18cf",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(1,1,figsize=(10,10))\n",
    "\n",
    "mse_small = mse[mse['Degree']<=13]\n",
    "\n",
    "sns.lineplot(data=mse_small, x='Degree',y='MSE',ax=ax, color='darkorange', label=\"Train MSE\")\n",
    "sns.scatterplot(data=mse_small, x='Degree',y='MSE',ax=ax, color='darkgreen',marker='P')\n",
    "sns.lineplot(data=mse_small, x='Degree', y='MSE_test', ax=ax, color='darkblue', label=\"Test MSE\")\n",
    "\n",
    "ax.axhline(y=mse_true_fun, color='k',linestyle=\":\")\n",
    "ax.text(10,mse_true_fun+.001,\"MSE of the True Function\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a280e66",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In-Class Exercise: \n",
    "\n",
    "#5: What degree of polinomial would you choose to give the best predictions $\\hat{y}$ in this example? "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
