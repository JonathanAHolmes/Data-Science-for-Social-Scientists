{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81e03c03",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <div align=\"center\"> SPECIAL TOPICS III </div>\n",
    "## <div align=\"center\"> Data Science for Social Scientists  </div>\n",
    "### <div align=\"center\"> ECO 4199 </div>\n",
    "#### <div align=\"center\">Class 9 Part 2: Tree-based methods</div>\n",
    "<div align=\"center\"> Jonathan Holmes (he/him)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e212ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%conda install python-graphviz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16839f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Road Map\n",
    "$$y=f(X) + \\varepsilon$$\n",
    "- Today we will see a truly different function that relates X and y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2510679",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930dcad5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivating example: Baseball\n",
    "- Say that you want to predict a baseball player's salary\n",
    "- You are given three pieces of information:\n",
    "    - Salary in thousands of dollars (target)\n",
    "    - Number of years of experience in the major leagues and Hits (number of hits made in the previous year)\n",
    "- You start with some data cleaning:\n",
    "    - Drop when you don't observe salary\n",
    "    - Take the __log-transform__ of salary so it's distribution looks more like a bell-shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0d240e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "folder=\"~/Dropbox/_teaching/ECO4199/2023/Data-Science-for-Social-Scientists/Class 09B - Tree Based Methods/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb6b8c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(folder+\"Hitters.csv\")\n",
    "df.pop(df.columns[0])\n",
    "print(\"Before Data Cleaning\")\n",
    "display(df.head())\n",
    "df=df.loc[:,['Salary','Years','Hits']] # keep only the variables needed\n",
    "df=df.loc[~df['Salary'].isna()] # keep only if observations have information on salary (i.e. if Salary is not (~) missing)\n",
    "df['salary']=np.log(df['Salary']) # create log of Salary and assign to salary (small cap)\n",
    "display(df.describe().T)\n",
    "print(\"After Data Cleaning\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df90c8be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes =plt.subplots(1,2, figsize=(12,8))\n",
    "\n",
    "sns.histplot(x=df['Salary'], stat='frequency', kde=True,color='darkorange', fill=False,line_kws={'color':'k'}, ax=axes[0])\n",
    "sns.histplot(x=df['salary'], stat='density', kde=True,color='darkgreen', fill=False, ax=axes[1])\n",
    "\n",
    "axes[0].set_xlabel(\"Salary in Thoushands\",fontsize=12)\n",
    "axes[1].set_xlabel(\"Log-Transform of Salary\",fontsize=12)\n",
    "axes[0].set_ylabel(\"Frequency\",fontsize=12)\n",
    "axes[1].set_ylabel(\"Frequency\",fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35095ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree  import DecisionTreeRegressor\n",
    "# Import train_test_split\n",
    "from sklearn.model_selection  import train_test_split\n",
    "# Import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn import tree\n",
    "from IPython.display import SVG\n",
    "from graphviz import Source\n",
    "from IPython.display import display\n",
    "\n",
    "# Split dataset into 80% train, 20% test\n",
    "#X_train, X_test, y_train, y_test= train_test_split(df[['Hits','Years']], df['salary'],test_size=0.2,random_state=1706)\n",
    "X_train=df.loc[:,['Hits','Years']]\n",
    "y_train=df.loc[:,'salary']\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeRegressor(max_depth=2, random_state=1706)\n",
    "dt.fit(X_train,y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = dt.predict(X_train)\n",
    "# Evaluate test-set accuracy\n",
    "print(f\"In-sample MSE: {mean_squared_error(df['salary'], y_pred)}\")\n",
    "\n",
    "#store label name\n",
    "labels = X_train.columns.tolist()\n",
    "\n",
    "# print decision tree\n",
    "graph = Source(tree.export_graphviz(dt, out_file=None, feature_names=labels , filled = True))\n",
    "display(SVG(graph.pipe(format='svg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd2864b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df['Salary_Predicted']=y_pred\n",
    "# Split on Years\n",
    "df.loc[df.Years<=4.5,'Year_Split']='Years $\\leq$ 4.5'\n",
    "df.loc[df.Years>4.5,'Year_Split']='Years $>$ 4.5'\n",
    "# If Split on Years is True\n",
    "df.loc[((df.Years<=4.5) & (df.Hits<=15.5)),'Hits_Split']=r\"Hits $\\leq$ 15.5\"\n",
    "df.loc[((df.Years<=4.5)& (df.Hits>15.5)),'Hits_Split']=r'Hits $>$ 15.5'\n",
    "# If Split on Years is False\n",
    "df.loc[((df.Years>4.5) & (df.Hits<=117.5)),'Hits_Split']=r\"Hits $\\leq$ 117.5\"\n",
    "df.loc[((df.Years>4.5)& (df.Hits>117.5)),'Hits_Split']='Hits $>$ 117.5'\n",
    "df['Path']= df['Year_Split'] +' and '+df['Hits_Split']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaff24c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_palette(\"Set2\")\n",
    "fig, ax=plt.subplots(1,1, figsize=(12,12))\n",
    "\n",
    "sns.scatterplot(data=df, x='Years', y='Hits',hue='Path', size='Salary')\n",
    "\n",
    "# single split on year at 4.5\n",
    "ax.axvline(x=4.5, color='k', linestyle=':')\n",
    "\n",
    "# Years <4.5 and Hits <=15.5\n",
    "ax.plot((0, 4.5), (15.5, 15.5), 'k:')\n",
    "# Years >4.5 and Hits <=117.5\n",
    "ax.plot((4.5, 25), (117.5, 117.5), 'k:')\n",
    "\n",
    "ax.set_xlabel(\"YEARS\", fontsize=20)\n",
    "ax.set_ylabel(\"HITS\", fontsize=20)\n",
    "\n",
    "ax.set_xlim(xmin=0,xmax=25)\n",
    "plt.text(1.5, 7, f\"R1: {str(round(np.exp(7.243499)))}\", horizontalalignment='left', size='large', color='black', weight='semibold')\n",
    "plt.text(1.5, 190, f\"R2: {str(round(np.exp(5.058228)))}\", horizontalalignment='left', size='large', color='black', weight='semibold')\n",
    "plt.text(15, 7, f\"R3: {str(round(np.exp(5.998380)))}\", horizontalalignment='left', size='large', color='black', weight='semibold')\n",
    "plt.text(15, 190, f\"R4: {str(round(np.exp(6.739687)))}\", horizontalalignment='left', size='large', color='black', weight='semibold')\n",
    "\n",
    "plt.title(\"Splits and Predicted Salaries\",fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de46e38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression tree fit to the data\n",
    "- The 2 figures in the previous slides shows how data is fit to the data depending on __splitting rules__\n",
    "- It consists of a series of splitting rules, __starting at the top of the tree__. \n",
    "- The top split assigns observations having Years<4.5 to the left __branch__.\n",
    "    - The data is then further divided depending on the number of hits:\n",
    "        - For Hits <=15.5 (Region 1): Predicted Salary is $e^{7.243}$ = 1,399 thousands dollars\n",
    "        - For Hits > 15.5 (Region 2): Predicted Salary is \\$157,000\n",
    "- Similar reasoning leads to regions 3 and 4\n",
    "- Note that these predicted wages correspond to the mean of each group!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdef1b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby('Path').agg({'salary':np.mean,'Salary_Predicted':np.mean})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898f7898",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Terminology\n",
    "- R1, R2, R3 and R4 are known as __terminal nodes__ or __leaves__\n",
    "    - Trees are typically drawn upside down, in the sense that the leaves are at the bottom of the tree. \n",
    "- The points along the tree where the predictor space is split are referred to as __internal nodes__. \n",
    "    - This is where the splits occur\n",
    "- We refer to the segments of the trees that connect the nodes as __branches__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8c8ced",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interprating the results\n",
    "You should interpret the results this way:\n",
    "- Years is the most important factor in determining Salary and players with less experience earn lower salaries than more experienced players. \n",
    "- Given that a player is less experienced, the number of hits that he made in the previous year seems to play little role in his salary. \n",
    "    - For a couple of players it's even a plus (outliers?)\n",
    "- But among players who have been in the major leagues for five or more years, the number of hits made in the previous year does affect salary, and players who made more hits last year tend to have higher salaries.\n",
    "- Note that we could probably predict Salary using an OLS regression but this representation is very easy to interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd95036",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision trees: under the hood\n",
    "- What we did here is predict salary using __stratification of the feature space__ which we did in 2 steps:\n",
    "    1. We divide the predictor space—that is, the set of possible values for $X_1,X_2, . . .,X_p$—into J distinct and non-overlapping regions, $R_1,R_2, \\dots , R_J$\n",
    "        - In our example, J=4\n",
    "    2. For every observation that falls into the region $R_j$, we make the same prediction, which is simply the mean of the response values for the training observations in $R_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34babb58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dividing the predictor space\n",
    "How do we construct the regions $R_1, \\dots,R_J$? \n",
    "- We divide the predictor space into high-dimensional rectangles, or __boxes__\n",
    "- The goal is to find boxes R1, . . . , RJ that minimize the RSS!\n",
    "    $$\\Large \\sum_{j=1}^J \\sum_{i \\in R_J} (y_i - \\hat{y}_{R_j})^2$$\n",
    "- Where $\\hat{y}_{R_j}$ is __the average__ of the $j_{th}$ box\n",
    "For J=2:\n",
    "$$  \\sum_{i \\in R_1} (y_i - \\hat{y}_{R_1})^2 +\\sum_{i \\in R_2} (y_i - \\hat{y}_{R_2})^2 $$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faa5abf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dividing the predictor space continued\n",
    "- Note that we are no longer trying to fit data using parameters ($\\beta$) \n",
    "- Our prediction for y instead relies on simply taking the average of the group j\n",
    "- This a method known as __nonparametric__ estimation\n",
    "- As you can imagine, the question is how to best minimize our objective function (RSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ecd12e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dividing the predictor space, still\n",
    "- Consider the _Years_ predictor, 4.5 Years is value that minimizes the RSS\n",
    "- It means we divided the predictor space into two regions $\\left\\{X|X_j \\leq s\\right\\}$ and $\\left\\{X|X_j > s\\right\\}$ such that the cutpoint $s$ leads to the greatest possible reduction in RSS. \n",
    "    - The notation means the region of predictor space in which $X_j$ takes on a value less or equal (or greater than s.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f24dc6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dd=df.copy()\n",
    "cutpoints=np.linspace(df['Years'].min(),df['Years'].max(),((df['Years'].max()-df['Years'].min())*2)+1)\n",
    "RSS=[] ; cutoffs=[]\n",
    "for s in cutpoints:\n",
    "    # split the data along the cutoff\n",
    "    dd_inf=dd.loc[df.Years<=s].copy() ; dd_sup=dd.loc[df.Years>s].copy()\n",
    "    dd_inf['y_hat']=dd_inf['salary'].mean() ; dd_sup['y_hat']=dd_sup['salary'].mean()\n",
    "    # take the square of the residual of each region\n",
    "    dd_inf['residual_sq'] = np.square(dd_inf['salary'] - dd_inf['y_hat'] ) \n",
    "    dd_sup['residual_sq'] = np.square(dd_sup['salary'] - dd_sup['y_hat'] ) \n",
    "    # take the sum of the squared residual\n",
    "    rss_inf=dd_inf['residual_sq'].sum(); rss_sup=dd_sup['residual_sq'].sum()\n",
    "    # add the RSS of each region and append in RSS list\n",
    "    RSS.append(rss_inf+rss_sup)  ; cutoffs.append(s)\n",
    "    \n",
    "dd=pd.DataFrame({'RSS':RSS, 'Cutpoint':cutoffs})  \n",
    "dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6122a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(12,8))\n",
    "sns.lineplot(x='Cutpoint',y='RSS',color=\"darkorange\",data=dd,ax=ax)\n",
    "ax.axvline(4.5,color='darkgreen')\n",
    "ax.set_xlabel(\"Value of Split for Years\", fontsize=20)\n",
    "ax.set_ylabel(\"Residual Sum of Squares from split of predictor space\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c569de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dividing the predictor space, next\n",
    "\n",
    "- So we now know that we should split the data along Years=4.5\n",
    "- Next, we repeat the process, looking for the best predictor and best cutpoint in order to split the data further so as to minimize the RSS within each of the __resulting regions__. \n",
    "    - However, this time, instead of splitting the entire predictor space, we split one of the two previously identified regions.\n",
    "- The process continues until a stopping criterion is reached; \n",
    "    - for instance, we may continue until no region contains more than five observations.\n",
    "- Once the regions $R_1, \\dots , R_J$ have been created, we predict the response for a given test observation using the mean of the training observations in the region to which that test observation belongs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bc2c68",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification Trees\n",
    "- What if you want to predict a qualitative variable instead?\n",
    "- __Classification trees__ are very similar to a regression tree, except that it is classification used to predict a qualitative response rather than a quantitative one. \n",
    "- Instead of predicting the average of the region, we will predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509a1ad7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Error rate\n",
    "- In the classification setting, RSS cannot be used as a criterion for making the binary splits.\n",
    "- Instead we can use the __classification error rate__. \n",
    "    - the fraction of the training observations in that region that do not belong to the most common class:\n",
    "$$ \\text{Error Rate}= 1 - \\max_k(\\hat{p}_{mk}) $$\n",
    "- Here $\\hat{p}_{mk}$ represents the proportion of training observations in the $m_{th}$ region that are from the $k^{th}$ class. \n",
    "    - However, it turns out that classificationerror is not sufficiently sensitive for tree-growing, and in practice two other measures are preferable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5359df59",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gini Index\n",
    "$$\\text{Gini}=\\sum_{k=1}^K \\hat{p}_{mk}(1-\\hat{p}_{mk})$$\n",
    "- Gini is a measure of total variance across the K classes. \n",
    "    - Gini index takes on a small value if all of the $\\hat{p}_{mk}$ are close to zero or one. \n",
    "- Gini index is a measure of __node purity__ \n",
    "    - a small value indicates that a node contains predominantly observations from a single class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fd2950",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Enthropy\n",
    "$$\\text{Enthropy}= - \\sum_{k=1}^K \\hat{p}_{mk}\\log\\hat{p}_{mk}$$\n",
    "- Since $0 \\leq \\hat{p}_{mk} \\leq 1$ , it follows that $0 \\leq −\\hat{p}_{mk}\\log\\hat{p}_{mk}$. \n",
    "- Entropy will take on a value near zero if the $\\hat{p}_{mk}$ are all near zero or near one. \n",
    "- Gini or Enthropy are the preferred ways to measure the quality of a particular split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d59baab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df2=pd.read_csv(folder+'Heart.csv')\n",
    "df2.pop(df2.columns[0])\n",
    "display(df2.head())\n",
    "df2['ChestPain'] = pd.factorize(df2.ChestPain)[0]\n",
    "df2['Thal'] = pd.factorize(df2.Thal)[0]\n",
    "df2['AHD']=df2['AHD'].replace({\"Yes\":1, \"No\":0})\n",
    "df2.dropna(inplace=True)\n",
    "y_train2=df2.pop('AHD') \n",
    "X_train2=df2\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836a2775",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Classification Tree using Giny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818fd696",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree  import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(max_depth=None, max_leaf_nodes=6, max_features=3, random_state=1706)\n",
    "clf.fit(X_train2,y_train2)\n",
    "display(clf.score(X_train2,y_train2))\n",
    "\n",
    "#store label name\n",
    "labels = X_train2.columns.tolist()\n",
    "\n",
    "# print decision tree\n",
    "print(\"Classification Tree using Giny\")\n",
    "graph = Source(tree.export_graphviz(clf, out_file=None, feature_names=labels, class_names=['No', 'Yes'] , filled = True))\n",
    "display(SVG(graph.pipe(format='svg')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130b9d63",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Classification Tree using Enthropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb33500",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=None, max_leaf_nodes=6, max_features=3, criterion=\"entropy\", random_state=1706)\n",
    "clf.fit(X_train2,y_train2)\n",
    "display(clf.score(X_train2,y_train2))\n",
    "\n",
    "#store label name\n",
    "labels = X_train2.columns.tolist()\n",
    "\n",
    "# print decision tree\n",
    "print(\"Classification Tree using Enthropy\")\n",
    "graph = Source(tree.export_graphviz(clf, out_file=None, feature_names=labels, class_names=['No', 'Yes'] , filled = True))\n",
    "display(SVG(graph.pipe(format='svg')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66446b08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear model vs Trees\n",
    "- As you know, the linear regression assumes a model of the form:\n",
    "$$y= f(x)= \\beta_0 + \\sum_{j=1}^p \\beta_j X_j$$\n",
    "- Instead, regression trees assume a model of the form:\n",
    "$$y=f(x)=\\sum_{m=1}^M c_m \\mathbb{1}_{X\\in R_m}$$\n",
    "- If the relationship is indeed linear then an approach such as linear regression will likely work well, and will outperform a method such as a regression tree that does not exploit this linear structure. \n",
    "- If instead there is a highly non-linear and complex relationship between the features and the response, then decision trees may outperform classical approaches.\n",
    "    - Recall also that interpretability is better for trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43928e84",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](figure8_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6973143b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Trees, Pros and Cons\n",
    "\n",
    "| Pros | Cons |\n",
    "| --- | --- |\n",
    "|- Trees are very easy to explain to people including non-expexts | - Trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches seen in this class |\n",
    "|- Close to human decision-making (cons?) | - Trees can be very non-robust. In other words, a small change in the data can cause a large change in the final estimated tree|\n",
    "|- Trees can easily handle qualitative predictors without the need to create dummy variables | |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbea9507",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bagging, Random Forests, Boosting\n",
    "\n",
    "Bagging, random forests, and boosting use trees as building blocks to\n",
    "construct more powerful prediction models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e63c5b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging\n",
    "- The decision trees discussed so far suffer from high variance.\n",
    "    - if we split the training data into two parts at random, and fit a decision tree to both halves, the results that we get could be quite different. \n",
    "    - In contrast, a procedure with low variance will yield similar results if applied repeatedly to distinct data sets\n",
    "    - linear regression tends to have low variance, if the ratio of n to p is moderately large.\n",
    "- __Bootstrap aggregation__, or __bagging__, is a general-purpose procedure for reducing the bagging variance of a statistical learning method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9956c2a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging, continued\n",
    "\n",
    "- Given a set of n independent observations $Z_1, \\dots , Z_n$, each with variance $\\sigma^2$, the variance of the mean $\\bar{Z}$ of the observations is given by $\\sigma^2/n$. \n",
    "    - Averaging a set of observations reduces variance!\n",
    "    \n",
    "- A natural way to reduce the variance and hence increase the prediction _accuracy_ of a statistical learning method is to take many training sets from the population, build a separate prediction model using each training set, and average the resulting predictions. \n",
    "- In other words, we could calculate $\\hat{f}^1(X), \\hat{f}^2(X), \\dots, \\hat{f}^B(X)$ using B separate training sets and average them in order to obtain a single low-variance statistical learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaf58f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bootstrapping in Bagging\n",
    "- In reality we rarely have different training datasets\n",
    "- Bootstrapping consists in taking repeated samples from the (single) training data set. \n",
    "- In this approach we generate B different bootstrapped training data sets. \n",
    "- We then train our method on the $b^{th}$ bootstrapped training set in order to get $\\hat{f}^{\\ast b}(X)$ and average:\n",
    "$$\\hat{f}_{\\text{bag}}(x)= \\frac{1}{B} \\sum_{b=1}^B \\hat{f}^{\\ast b}(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80749492",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging, why it's working\n",
    "- While bagging can improve predictions for many regression methods, it is particularly useful for decision trees. To apply bagging to regression trees \n",
    "    - we simply construct B regression trees using B bootstrapped training sets, and average the resulting predictions. \n",
    "    - These trees are grown deep, and are not pruned. \n",
    "    - Hence each individual tree has high variance, but low bias. \n",
    "- Averaging these B trees reduces the variance. \n",
    "- Bagging has been demonstrated to give impressive improvements in accuracy by combining together hundreds or even thousands of trees into a single procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123f0b18",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Out-of-Bag Error Estimation\n",
    "- There is a very straightforward way to estimate the test error of a bagged model, without the need to perform cross-validation or the validation set approach. \n",
    "- Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations.\n",
    "- On average, each bagged tree makes use of around two-thirds of the observations\n",
    "- The remaining one-third of the observations not used to fit a given bagged tree are referred to as the __out-of-bag (OOB) observations__\n",
    "- with B sufficiently large, OOB error is virtually equivalent to leave-one-out cross-validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f910ee5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(folder+'Boston.csv')\n",
    "display(df.head())\n",
    "y_train = df.pop('medv')\n",
    "X_train = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd59b0ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "regr = BaggingRegressor(base_estimator=DecisionTreeRegressor(),oob_score=True, n_estimators=100, random_state=1706).fit(X_train, y_train)\n",
    "print(regr.oob_score_, regr.score(X_train,y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b289f1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "scores=[] ;  num_B=[]\n",
    "for i,B in enumerate(range(20,300)):\n",
    "    regr = BaggingRegressor(base_estimator=DecisionTreeRegressor(),oob_score=True, n_estimators=B, random_state=1706).fit(X_train, y_train)\n",
    "    # Predict test set labels\n",
    "    scores.append(mean_squared_error(y_train, regr.predict(X_train))) ; num_B.append(i+1)\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,6))  \n",
    "\n",
    "sns.lineplot(x=num_B, y=scores)\n",
    "plt.show()     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba82494",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Variable Importance Measures\n",
    "- Bagging improves prediction accuracy at the expense of interpretability since it is the average of many trees\n",
    "- One can obtain an overall summary of the importance of each predictor using:\n",
    "    - the RSS (for bagging regression trees)\n",
    "    - the Gini index (for bagging classification trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44698781",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "feature_importances = np.mean([tree.feature_importances_ for tree in regr.estimators_], axis=0)\n",
    "\n",
    "Importance = pd.DataFrame({'Importance':feature_importances, 'Features':X_train.columns})\n",
    "Importance.sort_values('Importance', axis=0, ascending=True,inplace=True)\n",
    "\n",
    "fig, ax=plt.subplots(1,1,figsize=(12,8))\n",
    "sns.barplot(y='Features', x='Importance',data=Importance, orient='h', ax=ax)\n",
    "ax.set_xlabel(\"Total Decrease in RSS due to Feature\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c886802b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Forests\n",
    "- __Random forests__ provide an improvement over bagged trees by way of a small tweak that decorrelates the trees\n",
    "- Each time a split in a tree is considered, a random sample of m predictors is chosen as split candidates from the full set of p predictors.\n",
    "- The split is allowed to use only one of those m predictors. \n",
    "- A fresh sample of m predictors is taken at each split, and typically we choose $m\\approx \\sqrt{p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc44ffc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random forests, why it's working\n",
    "- In other words, the algorithm is not allowed to consider a majority of the available predictors. \n",
    "\n",
    "Why?\n",
    "- Suppose that there is one very strong predictor in the data set, along with a number of other moderately strong predictors. \n",
    "- Then in the collection of bagged trees, most or all of the trees will use this strong predictor in the top split.\n",
    "- Consequently, all of the bagged trees will look quite similar to each other.\n",
    "    - Hence the predictions from the bagged trees will be __highly correlated__\n",
    "- Averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quantities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691e55d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random forests, why it's working continued\n",
    "- Because each split is forced to consider only a subset of the predictors, many splits will not consider the strong predictor\n",
    "- This decorrelates the trees: on average trees are less variable and hence more reliable\n",
    "- Note that bagging is a special case of random forests where $m=p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e6d34d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "print(X_train2.shape)\n",
    "m = round(np.sqrt(X_train2.shape[1]))\n",
    "print(m)\n",
    "\n",
    "# Random forests: using m features\n",
    "clf2 = RandomForestClassifier(max_features=m, random_state=1706)\n",
    "clf2.fit(X_train2, y_train2)\n",
    "mean_squared_error(y_train2, clf2.predict(X_train2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d34848",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting\n",
    "- __Boosting__, is another approach for improving the predictions resulting from a decision tree. \n",
    "    - Like bagging, boosting is a general approach that can be applied to many statistical learning methods\n",
    "- Recall that bagging involves creating multiple copies of the original training data set using the bootstrap\n",
    "    - fitting a separate decision tree to each copy\n",
    "    - then combining all of the trees in order to create a single predictive model. \n",
    "    - Each tree is built on a bootstrap data set, independent of the other trees \n",
    "- Boosting works in a similar way, except that the trees are grown sequentially: \n",
    "    - each tree is grown using information from previously grown trees. \n",
    "    - there is no bootstrap sampling; \n",
    "    - instead each tree is fit on a modified version of the original data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286b7125",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting Algorithm\n",
    "1. Set $\\hat{f}(x) = 0$ and $r_i=y_i$ for all $i$ in the training set \n",
    "2. For $b = 1, 2, \\dots,B$, repeat:\n",
    "    - a) Fit a tree $\\hat{f}^b$ with d splits (d+1 terminal nodes) to the training data $(X, r)$.\n",
    "    - b) Update $\\hat{f}$ by adding in a shrunken version of the new tree:\n",
    "    $$\\hat{f}(x) \\leftarrow \\hat{f}(x) + \\lambda \\hat{f}^b(x)$$\n",
    "    - c) Update the residual:\n",
    "    $$r_i=r_i-\\lambda \\hat{f}^b(x)$$\n",
    "3. Output the boosted model:\n",
    "$$\\hat{f}(x) = \\sum_{b=1}^B \\lambda\\hat{f}^b(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b1bc7a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting explained\n",
    "- Unlike fitting a single large decision tree to the data, which amounts to _fitting the data hard_ and potentially __overfitting__, the boosting approach instead _learns slowly_.\n",
    "- we fit a tree using the current residuals, rather than the outcome Y , as the response.\n",
    "-  We then add this new decision tree into the fitted function in order to update the residuals. \n",
    "- Each of these trees can be rather small, with just a few terminal nodes, determined by the parameter d in the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235255ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyperparameters\n",
    "- We started from a single parameter and ended with boosting method\n",
    "- Note that this method is also usually combined with cross validation techniques\n",
    "- As you can tell, our discussion is increasingly moving away from discussing individual predictors\n",
    "- Instead, we are now interested in:\n",
    "    - How many trees should we use (B)\n",
    "    - How many splits our tree should have  (d)\n",
    "    - How fast should the model learn $\\lambda$\n",
    "    - How many folds should our CV use (k)\n",
    "- These learning parameters are known as __hyperparameters__ and are usually the core focus of machine learning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb034ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3022fbfa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# load Boston data\n",
    "boston_df = pd.read_csv('Boston.csv')\n",
    "display(boston_df.info())\n",
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855561f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Separate predictors and target\n",
    "X = boston_df.drop('medv', axis=1)\n",
    "y = boston_df.medv\n",
    "# Split between train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f05f979",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Regression Tree\n",
    "reg = DecisionTreeRegressor(max_depth=3)\n",
    "reg.fit(X_train, y_train)\n",
    "pred = regr.predict(X_test)\n",
    "\n",
    "graph = Source(tree.export_graphviz(reg, out_file=None, feature_names=X.columns , filled = True))\n",
    "display(SVG(graph.pipe(format='svg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7413216",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Regression Tree and Depth of the Tree\n",
    "MSEs=[] ; Depth=[]\n",
    "for d in np.arange(1,30):\n",
    "    reg = DecisionTreeRegressor(max_depth=d, random_state=1706)\n",
    "    reg.fit(X_train, y_train)\n",
    "    pred = reg.predict(X_test)\n",
    "    MSEs.append(mean_squared_error(y_test, pred)) ;  Depth.append(d)\n",
    "fig, ax = plt.subplots(1,1, figsize=(8,8))\n",
    "sns.lineplot(x=Depth, y=MSEs,ax=ax)\n",
    "ax.set_xlabel(\"Depth of Tree\",fontsize=16)\n",
    "ax.set_ylabel(\"Test MSE\",fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06234428",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Regression Tree and Random State\n",
    "MSEs=[] ; States=[]\n",
    "for state in np.arange(1,300):\n",
    "    reg = DecisionTreeRegressor(max_depth=10, random_state=state)\n",
    "    reg.fit(X_train, y_train)\n",
    "    pred = reg.predict(X_test)\n",
    "    MSEs.append(mean_squared_error(y_test, pred)) ;  States.append(state)\n",
    "fig, ax = plt.subplots(1,1, figsize=(8,8))\n",
    "sns.histplot(x=MSEs,stat='probability',ax=ax)\n",
    "ax.set_xlabel('Test MSE score',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cfd674",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Bagging\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "oob_bag=[] ; r2_bag=[] # create list to record predictive performance\n",
    "num_trees=[]\n",
    "m=X_train.shape[1] # set m to the number of features in the data\n",
    "for i,B in enumerate(range(20,300)):\n",
    "    reg_bag = RandomForestRegressor(n_estimators=B, max_features=m, oob_score=True,random_state=1706).fit(X_train, y_train) # recall that bagging is a special case of random forest in which max features is the number of predictors\n",
    "    oob_bag.append(reg_bag.oob_score_) ; r2_bag.append(r2_score(y_test, reg_bag.predict(X_test))) ; num_trees.append(i)\n",
    "res=pd.DataFrame({'OOB Score':oob_bag, r\"Test $R^2$\":r2_bag, \"Trees\": num_trees})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc43a1e",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax= plt.subplots(1,1, figsize=(10,8))\n",
    "sns.lineplot(x='Trees', y='OOB Score', color='darkorange',linestyle=\":\",label=r'OOB $R^2$ Score', data=res, ax=ax)\n",
    "sns.lineplot(x='Trees', y='Test $R^2$', color='darkgreen',linestyle=\":\",label=r\"Out of Sample $R^2$\", data=res,ax=ax)\n",
    "ax.set_ylabel(r\"$R^2$\", fontsize=16)\n",
    "ax.set_xlabel(\"Number of Trees\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c703a84",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Boosting\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "B=100\n",
    "learning_rates=np.linspace(0.001,1,30)\n",
    "depths=np.arange(1,31)\n",
    "MSEs=[] ; lambdas=[] ; depth_values=[]\n",
    "for 𝜆 in learning_rates:\n",
    "    for d in depths:   \n",
    "        boo = GradientBoostingRegressor(n_estimators=B,max_depth=d, learning_rate=𝜆, random_state=1706).fit(X_train, y_train)\n",
    "        MSEs.append(mean_squared_error(y_test,boo.predict(X_test))) ; lambdas.append(𝜆) ; depth_values.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14207b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=pd.DataFrame({'MSE':MSEs, '𝜆':lambdas, 'Depth':depth_values})\n",
    "print(f\" Best learning rate: {float(res.loc[res.MSE==res.MSE.min(),'𝜆'])}\")\n",
    "print(f\" Best Depth: {int(res.loc[res.MSE==res.MSE.min(),'Depth'])}\")\n",
    "\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981478bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2,figsize=(15,8))\n",
    "sns.lineplot(x=r'𝜆', y='MSE', data=res, ax=axes[0])\n",
    "sns.lineplot(x='Depth', y='MSE', data=res, ax=axes[1])\n",
    "\n",
    "axes[0].set_xlabel(r\"$\\lambda$ - Learning Rate\", fontsize=16)\n",
    "axes[1].set_xlabel(\"Depth\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3820d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "best_learning_rate=float(res.loc[res.MSE==res.MSE.min(),'𝜆'])\n",
    "best_depth=int(res.loc[res.MSE==res.MSE.min(),'Depth'])\n",
    "regr = GradientBoostingRegressor(n_estimators=B, learning_rate=best_learning_rate,max_depth=best_depth, random_state=1706).fit(X_train, y_train)\n",
    "\n",
    "feature_importance = regr.feature_importances_*100\n",
    "Importance = pd.DataFrame({'Importance':feature_importance, 'Features':X_train.columns})\n",
    "Importance.sort_values('Importance', axis=0, ascending=True,inplace=True)\n",
    "\n",
    "fig, ax=plt.subplots(1,1,figsize=(12,8))\n",
    "sns.barplot(y='Features', x='Importance',data=Importance, orient='h', ax=ax)\n",
    "ax.set_xlabel(\"Total Decrease in RSS due to Feature\", fontsize=20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
