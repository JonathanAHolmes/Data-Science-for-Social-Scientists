{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "seventh-language",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <div align=\"center\"> SPECIAL TOPICS III </div>\n",
    "## <div align=\"center\"> Data Science for Social Scientists  </div>\n",
    "### <div align=\"center\"> ECO 4199 </div>\n",
    "#### <div align=\"center\">Class 9A - Model Selection</div>\n",
    "<div align=\"center\"> Jonathan Holmes, (he/him)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-simpson",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Subset Selection\n",
    "$$Y = \\beta_0+\\beta_1X_1+\\beta_2X_2+ ... + \\beta_pX_p + \\epsilon$$\n",
    "- The issue it that adding predictors will always weakly improve the in sample prediction\n",
    "    - But at the expense of out of sample prediction\n",
    "- It is therefore important to limit the number of predictors to those actually related to Y\n",
    "- __Subset Selection__ _is the process of identifying the p predictors actually related to Y._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-defense",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Best Subset Selection\n",
    "- Here is the algorithm to select the best subset given a dataset with p predictors\n",
    "- We will apply this algorithm to the Credit dataset\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "1. Let $\\mathscr{M}_0$ denote the null model , which contains no predictors. This\n",
    "model simply predicts the sample mean for each observation.\n",
    "2. For k = 1, 2, . . .p:\n",
    "    - a. Fit all ${P\\choose k}$ containing exactly $k$ predictors\n",
    "    - b. Pick the best among these ${P\\choose k}$ and call it $\\mathscr{M}_k$. Here best is defined as having the smallest RSS, or equivalently largest R2.\n",
    "3. Select a single best model from among $\\mathscr{M}_0$, . . . ,$\\mathscr{M}_p$ using crossvalidated prediction error (MSE), AIC, BIC, or adjusted $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-richardson",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The null model\n",
    "- Let's start with the null model\n",
    "- This a model using no predictors and a single parameter: the intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436c0866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let me set my current directory using the %cd magic\n",
    "%cd \"~/Dropbox/_teaching/ECO4199/2023/Data-Science-for-Social-Scientists/Class 09A ‚Äì Model Selection/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-experience",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import sklearn.linear_model as skl_lm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-superior",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"Credit.csv\")\n",
    "df['Own']=df['Own'].replace({\"No\":0,  \"Yes\":1})\n",
    "df['Student']=df['Student'].replace({\"No\":0,  \"Yes\":1})\n",
    "df['Married']=df['Married'].replace({\"No\":0,  \"Yes\":1})\n",
    "df=df.join(pd.get_dummies(df['Region'], drop_first=True))\n",
    "df.pop(\"Region\")\n",
    "display(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-stuart",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\mathscr{M}_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-bangkok",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# M0\n",
    "results = smf.ols('Balance ~ 1 ', data=df).fit()\n",
    "# Inspect the results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-commonwealth",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# get y and X\n",
    "y=df['Balance'] # y\n",
    "X=df.loc[:,~df.columns.str.contains('Balance')] # X\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-martial",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\mathscr{M}_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-maximum",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# initiate empty list to store R2\n",
    "R2= 0\n",
    "best_predictor=\"none\"\n",
    "X=df.loc[:,~df.columns.str.contains('Balance')]\n",
    "y=df.loc[:,df.columns.str.contains('Balance')]\n",
    "X['intercept']=1\n",
    "\n",
    "\n",
    "# for each predictor in X\n",
    "for k in range(X.shape[1]-1): # minus 1 because last column is intercept\n",
    "    results = sm.OLS(y, X.iloc[:,[k,-1]]).fit() # fit predictor in position p and intercept (in last position, -1)\n",
    "    print(f\"R\\u00b2 for predictor {X.columns[k]} is {round(results.rsquared,6):f}\")\n",
    "    if results.rsquared>R2:\n",
    "        R2=results.rsquared ; best_predictor=X.columns[k]\n",
    "print(\"\\nBest predictor for M1 is: {} with {}.\".format(best_predictor,round(R2,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66523956",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# In-Class Exercise: \n",
    "\n",
    "Q1: Which single variable does the best job at predicting \"Balance\"? \n",
    "\n",
    "Q2: How many possible combinations are there if you have: \n",
    "- 1 variable: \n",
    "- 2 variables: \n",
    "- 3 variables: \n",
    "- 4 variables: \n",
    "- 5 variables: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-sapphire",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "DFs=[]\n",
    "#for p in range(X.shape[1]): # uncomment here if you want to do it for all RUN AT YOUR OWN RISK\n",
    "for p in range(8):\n",
    "    R2=0\n",
    "    best_predictor=\"none\"\n",
    "    R2_list=[]; adj_R2_list=[] ; aic=[] ; bic=[] ; models=[]\n",
    "    bestmodel=[-1]\n",
    "    for k in permutations(list(range(X.shape[1]-1)), p):\n",
    "        model=list(k)\n",
    "        model.append(-1)\n",
    "        results = sm.OLS(y, X.iloc[:,model]).fit() # fit predictors from model\n",
    "        # append statistics to list\n",
    "        R2_list.append(results.rsquared) ; adj_R2_list.append(results.rsquared_adj) \n",
    "        aic.append(results.aic); bic.append(results.bic) ; models.append(','.join(X.columns[model]))\n",
    "\n",
    "        if results.rsquared_adj>R2:\n",
    "            bestmodel=model\n",
    "            #model.pop(-1)\n",
    "            R2=results.rsquared_adj \n",
    "    DFs.append(pd.DataFrame(data={'R2':R2_list,'Adjusted R2':adj_R2_list, \"AIC\":aic, \"BIC\":bic,'Predictors':p,'model':models}))\n",
    "    \n",
    "    print(\"Best predictor for M{} is: {} with {}.\".format(p,', '.join(X.columns[bestmodel]),round(R2,3)))\n",
    "    \n",
    "dd=pd.concat(DFs, ignore_index=True)\n",
    "dd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-equality",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computational challenge\n",
    "- Best subset selection is a simple and conceptually appealing \n",
    "- But the number of possible models that must be considered grows rapidly as p increases. \n",
    "- In general, there are 2p models that involve subsets of p predictors. \n",
    "    - So if p = 10, then there are approximately 1,000 possible models to be considered!\n",
    "    - So if p = 20, then there are more than 1,000,000 possible models to be considered!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-register",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![title](credit_10predictors.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-instruction",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dd_best=dd.groupby(['Predictors']).agg({\"R2\":np.max, \"Adjusted R2\":np.max, \"AIC\":np.min, \"BIC\":np.min}).reset_index()\n",
    "dd_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-madonna",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, figsize=(10,10),sharex=True)\n",
    "axes = axes.ravel() # access axes with a single position instead of 2\n",
    "for i, statistics in enumerate(['R2',\"Adjusted R2\",\"AIC\",\"BIC\"]):\n",
    "    sns.scatterplot(x='Predictors',y=statistics,data=dd,ax=axes[i], color='gray',marker='.',alpha=.3)\n",
    "    sns.lineplot(x='Predictors',y=statistics,data=dd_best,ax=axes[i], color='darkorange')\n",
    "    sns.scatterplot(x='Predictors',y=statistics,data=dd_best,ax=axes[i], color='darkgreen')\n",
    "    axes[i].set_ylabel(statistics)\n",
    "    axes[i].set_xticks(np.arange(p+1))\n",
    "    \n",
    "fig.suptitle(\"In-Sample Statistics\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-conditioning",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 3 of Best Subset Selection\n",
    "- You can think of this stage in terms of last lecture:\n",
    "    - You have many models (last time many polynomial models), one for each p\n",
    "    - But they all maximize the in sample fit\n",
    "- In step 3, you can do this based on the adjusted-$R^2$, AIC, BIC or you can use CV techniques to find the model that gives the __best out of sample prediction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-assets",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dd['best']=dd.groupby(['Predictors'])['Adjusted R2'].transform(np.max)\n",
    "dd_best2=dd.loc[dd['Adjusted R2']==dd['best']].groupby('Predictors').first().reset_index()\n",
    "dd_best2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-range",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "models=[\"intercept\",[\"Rating\",\"intercept\"], \n",
    "        [\"Income\",\"Rating\",\"intercept\"],\n",
    "        [\"Income\",\"Rating\",\"Student\",\"intercept\"],\n",
    "        [\"Income\",\"Limit\",\"Cards\",\"Student\",\"intercept\"],\n",
    "        [\"Income\",\"Limit\",\"Rating\",\"Cards\",\"Student\",\"intercept\"],\n",
    "        [\"Income\",\"Limit\",\"Rating\",\"Cards\",\"Age\",\"Student\",\"intercept\"],\n",
    "        [\"Income\",\"Limit\",\"Rating\",\"Cards\",\"Age\",\"Own\",\"Student\",\"intercept\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-delay",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df['intercept']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-reform",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Best number of predictors using Cross Validation\n",
    "# use best model from precedent exercise to speed up the code\n",
    "kfold=5\n",
    "DFs=[]\n",
    "kf = KFold(n_splits=kfold, random_state=1706, shuffle=True)\n",
    "\n",
    "for i,m in enumerate(models):\n",
    "    MSEs=[] # empty list of MSE scores\n",
    "    X=df[m].values\n",
    "    y=df['Balance'].values\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        if i==0:\n",
    "            X_train, X_test = X[train_index].reshape(-1, 1), X[test_index].reshape(-1, 1)\n",
    "        else:\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # regression\n",
    "        reg = LinearRegression() # initiate the regression class\n",
    "        reg.fit(X_train,y_train) # fit the data\n",
    "        # Out of Sample MSE:\n",
    "        mse=mean_squared_error(y_test, reg.predict(X_test))\n",
    "        MSEs.append(mse)\n",
    "    DFs.append(pd.DataFrame({'Predictors':i,'MSE':MSEs}))\n",
    "    \n",
    "MSE_scores=pd.concat(DFs)\n",
    "mse=MSE_scores.groupby('Predictors').mean().reset_index()    \n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-faculty",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dd_best=dd_best.merge(mse, on='Predictors')\n",
    "dd_best.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-activity",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Best number of predictors for other statistics\n",
    "adj_R2_best=int(dd_best2.loc[dd_best2['Adjusted R2']==dd_best2['Adjusted R2'].max(),'Predictors'])\n",
    "aic_best=int(dd_best2.loc[dd_best2['AIC']==dd_best2['AIC'].min(),'Predictors'])\n",
    "bic_best=int(dd_best2.loc[dd_best2['BIC']==dd_best2['BIC'].min(),'Predictors'])\n",
    "mse_best=int(mse.loc[mse.MSE==mse.MSE.min(),'Predictors'])\n",
    "best_preds=[adj_R2_best,aic_best,bic_best,mse_best]\n",
    "print(f\"Best number of parameters for\\nAdjusted R-square: {adj_R2_best}\\nAIC: {aic_best}\\nBIC: {bic_best}\\n10-fold CV:{mse_best} \")\n",
    "\n",
    "fig, axes = plt.subplots(2,2, figsize=(12,12),sharex=True)\n",
    "axes = axes.ravel() # access axes with a single position instead of 2\n",
    "for i, statistics in enumerate([\"Adjusted R2\",\"AIC\",\"BIC\", \"MSE\"]):\n",
    "    sns.lineplot(x='Predictors',y=statistics,data=dd_best,ax=axes[i], color='darkorange')\n",
    "    sns.scatterplot(x='Predictors',y=statistics,data=dd_best,ax=axes[i], color='darkgreen')\n",
    "    # axes[i].axvline(best_preds[i], color='k')\n",
    "    axes[i].scatter(x=best_preds[i],y=float(dd_best.loc[best_preds[i],statistics]),marker='X',color='red',s=100)\n",
    "    axes[i].set_ylabel(statistics)\n",
    "    axes[i].set_xticks(np.arange(p+1))\n",
    "fig.suptitle(\"Best number of parameters by technique\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-communications",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solutions to computational challenge: Forward Stepwise Selection\n",
    "\n",
    "1. Let $\\mathscr{M}_0$ denote the null model, which contains no predictors.\n",
    "2. For $k = 0, \\dots , p ‚àí 1$:\n",
    "    - (a) Consider all p ‚àí k models that augment the predictors in $\\mathscr{M}_k$ with one additional predictor.\n",
    "    - (b) Choose the best among these p ‚àí k models, and call it $\\mathscr{M}_{k+1}$.\n",
    "        - Here best is defined as having smallest RSS or highest R2.\n",
    "3. Select a single best model from among $\\mathscr{M}_0$, $\\dots$ ,$\\mathscr{M}_p$ using crossvalidated prediction error, AIC, BIC, or adjusted R2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-lounge",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solutions to computational challenge: Backward Stepwise Selection\n",
    "\n",
    "1. Let $\\mathscr{M}_p$ denote the full model, which contains all p predictors.\n",
    "2. For $k = p, p ‚àí 1, \\dots , 1$:\n",
    "    - (a) Consider all k models that contain all but one of the predictors in $\\mathscr{M}_k$, for a total of k ‚àí 1 predictors.\n",
    "    - (b) Choose the best among these k models, and call it $\\mathscr{M}_{k-1}$. \n",
    "        - Here best is defined as having smallest RSS or highest $R^2$.\n",
    "3. Select a single best model from among $\\mathscr{M}_0, \\dots ,\\mathscr{M}_p$ using crossvalidated prediction error, AIC, BIC, or adjusted R2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ba313d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# When to use forward VS backward stepwise selection? \n",
    "\n",
    "Advantage of forward stepwise selection: \n",
    "- If your dataset has many, many variables forward stepwise selection is easier/feasible\n",
    "\n",
    "Advantage of backward stepwise selection: \n",
    "- It works better with collinear variables\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-regard",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Taking Stock\n",
    "- Last week we saw the risks associated with overfitting\n",
    "- This risk increases with the number of parameters\n",
    "- Different techniques yield different types of subsets but all penalize, one way or another, having too many parameters\n",
    "- The trade-off therefore is to find the best out of sample prediction using the least number of predictors possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-china",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Shrinkage Methods\n",
    "- Think back on the credit dataset\n",
    "- The data set has a number of predictors, which all seem reasonable\n",
    "    - All seem to be legitimate predictors of Balance\n",
    "    - There no variable irrelevant variable\n",
    "- Instead of our iterative, and long, process it would be nicer to fit all p predictors using a technique that __constrains__ or __regularizes the coefficient estimates__, or equivalently, that shrinks the coefficient estimates towards zero. \n",
    "    - Instead of cherry picking parameters we, instead, force the parameters of redundant predictors to be small or zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-founder",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ridge Regression\n",
    "- OLS regression for a model with p parameters finds that $\\beta_0, \\beta_1, ... \\beta_p$ that minimize (as you know):\n",
    "$$\\Large \\text{RSS} = \\sum_{i=1}^n \\Big(y_i - \\beta_0 - \\sum_{j=1}^p \\beta_jx_{ij}\\Big)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-exhaust",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ridge Regression, continued\n",
    "- Ridge regression is very similar to least squares, except that the coefficients ridge are by minimizing a slightly different quantity. \n",
    "- In particular, the ridge regression coefficient estimates $\\hat{\\beta^R}$ are the values that minimize:\n",
    "\\begin{gather}\n",
    "\\Large \\sum_{i=1}^n \\Big(y_i - \\beta_0 - \\sum_{j=1}^p \\beta_jx_{ij}\\Big)^2 + \\lambda \\sum_{j=1}^p\\beta_j^2 \\\\\n",
    "= \\Large\\text{RSS} + \\lambda \\sum_{j=1}^p\\beta_j^2\n",
    "\\end{gather}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-merit",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ridge Regression, shrinkage\n",
    "- $\\lambda$ is a __tuning parameter__ that is determined outside of the minimization problem\n",
    "- As with OLS, Ridge regression seeks coefficient estimates that fit the data well (small RSS)\n",
    "- Unlike OLS, the second term $\\lambda \\sum_{j=1}^p\\beta_j^2$ is small if $\\beta_1, ..., \\beta_j$ are small ($\\beta_0$ not included!)\n",
    "    - This second term is known as a __shrinkage penalty__\n",
    "- The tuning parameter $\\lambda$ serves to control the relative impact of these two terms on the regression coefficient estimates.\n",
    "    - When $\\lambda$ = 0, the penalty term has no effect, and ridge regression = OLS\n",
    "    - $\\lambda \\to \\infty$, the impact of the shrinkage penalty grows, and the ridge regression coefficient estimates will approach zero.\n",
    "- As you will see, the optimal $\\lambda$ is given using cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-spare",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Standardization of your dataset\n",
    "- Standardization of datasets is a common requirement for many machine learning estimators implemented see this tutorial in [scikit-learn](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "\n",
    "- You can transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.\n",
    "\n",
    "- Models such as the Ridge regression assume that all features are centered around zero and have variance in the same order. \n",
    "- If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n",
    "- In other words, in the OLS regression, multiplying X by a constant c will change $\\beta$ to $\\frac{\\beta}{c}$\n",
    "- In the Ridge regression, the $\\beta^R$ will depend not only on the value of Œª, but also on the scaling of the $j^{th}$ predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cf1e9b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# In-Class Exercise\n",
    "\n",
    "Q3: Why do we call \"Ridge\" regression a \"Shrinkage\" estimator? \n",
    "\n",
    "Q4: Suppose that we are predicting a farm harvest using the following regression\n",
    "\n",
    "$$\\text{Total_Harvest} = \\beta_0 + \\beta_1\\text{Average_Temperature} + \\beta_2\\text{Average_Rainfall} + u $$\n",
    "    \n",
    "a) If I run OLS two times with temperature measured in degrees celcius and degrees kelvin, what will change? \n",
    "\n",
    "b) If I ran Ridge regression two times measured in degrees celcius and degrees kelvin, what will change? \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-editor",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-malaysia",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# import the preprocessing module from sklearn\n",
    "from sklearn import preprocessing\n",
    "X_train =df[['Income','Limit','Rating','Student','Cards','Age','Education','Married','Own', 'West', 'South']]\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_train.mean(axis=0) , X_train.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-purchase",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's now see what happens to our estimates as the value for $\\lambda$ changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-addiction",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "lambdas = 10**np.linspace(10,-2,100)*0.5\n",
    "ridge = Ridge()\n",
    "coefs = []\n",
    "\n",
    "for ùúÜ in lambdas:\n",
    "    ridge.set_params(alpha=ùúÜ)\n",
    "    ridge.fit(X_train, y)\n",
    "    coefs.append(ridge.coef_)\n",
    "ridge_results=pd.DataFrame(coefs,columns=['Income','Limit','Rating','Student','Cards','Age','Education','Married','Own', 'West', 'South'])\n",
    "ridge_results['Lambda']=lambdas  \n",
    "ridge_results=pd.melt(ridge_results,id_vars=['Lambda'], var_name='Beta', value_name='Estimate')\n",
    "ridge_results.head()                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-corruption",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(1,1, figsize=(7,7))\n",
    "sns.lineplot(x='Lambda', y='Estimate', hue='Beta',data=ridge_results)\n",
    "ax.set_xscale('log')\n",
    "ax.axhline(0,color='k',linestyle=\":\")\n",
    "plt.axis('tight')\n",
    "plt.xlabel(r'$\\lambda$', fontsize=20)\n",
    "plt.ylabel('Standardized Coefficients',fontsize=20)\n",
    "plt.title('Ridge coefficients as a function of the regularization');\n",
    "\n",
    "plt.title('Plot B', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-consortium",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ridge and Model Selection\n",
    "- The ridge regression is clearly faster than our best subset methodology\n",
    "- Note though that we never truly select a model in the sense that we never use a subset of predictors\n",
    "- Instead, we are shrinking how much they matter in our prediction but use all p predictors (unless Œª = ‚àû).\n",
    "- This may not be a problem for prediction accuracy, but it can create a challenge in model interpretation in settings in which the number of variables p is quite large. \n",
    "- For example, in the Credit data set, it appears that the most important variables are income, limit, rating, and student. \n",
    "- So we might wish to build a model including just these predictors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-kitty",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lasso Regression\n",
    "- The Lasso regression is an alternative to the Ridge regression as it allows to shrink parmeters to zero\n",
    "- The lasso regression coefficient estimates $\\hat{\\beta^L_\\lambda}$ are the values that minimize:\n",
    "\\begin{gather}\n",
    "\\Large\\text{RSS} + \\lambda \\sum_{j=1}^p\\left|\\beta_j\\right|\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-davis",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lasso and Model Selection\n",
    "- As with ridge regression, the lasso shrinks the coefficient estimates towards zero. \n",
    "- However, in the case of the lasso, the $\\ell_1$ penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter Œª is sufficiently large. \n",
    "- Hence, much like best subset selection, the lasso performs variable selection.\n",
    "- We say that the lasso yields __sparse models__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-paper",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lambdas = 10**np.linspace(10,-1.5,100)*0.5\n",
    "lasso = Lasso()\n",
    "coefs = []\n",
    "\n",
    "\n",
    "for ùúÜ in lambdas:\n",
    "    lasso.set_params(alpha=ùúÜ)\n",
    "    lasso.fit(X_train, y)\n",
    "    coefs.append(lasso.coef_)\n",
    "lasso_results=pd.DataFrame(coefs,columns=['Income','Limit','Rating','Student','Cards','Age','Education','Gender','Married','Asian','Caucasian'])\n",
    "lasso_results['Lambda']=lambdas  \n",
    "lasso_results=pd.melt(lasso_results,id_vars=['Lambda'], var_name='Beta', value_name='Estimate')\n",
    "lasso_results.head()                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-slovakia",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(1,1, figsize=(7,7))\n",
    "ax = plt.gca()\n",
    "sns.lineplot(x='Lambda', y='Estimate', hue='Beta',data=lasso_results)\n",
    "ax.set_xscale('log')\n",
    "ax.axhline(0,color='k',linestyle=\":\")\n",
    "plt.axis('tight')\n",
    "plt.xlabel(r'$\\lambda$', fontsize=20)\n",
    "plt.ylabel('Standardized Coefficients',fontsize=20)\n",
    "#plt.title('Lasso coefficients as a function of the regularization');\n",
    "\n",
    "plt.title('Plot A', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-writer",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another Formulation for Ridge Regression and the Lasso\n",
    "- You may have recognized something you are already familiar with as economists\n",
    "- The Lasso and Ridge regressions can be written in terms of objective function (to minimize) and a constraint\n",
    " \n",
    " __Ridge__:\n",
    " \\begin{gather}\n",
    " \\large \\min_{\\mathbf{\\beta}} \\left\\{ \\sum_{i=1}^n \\Big(y_i - \\beta_0 - \\sum_{j=1}^p \\beta_jx_{ij}\\Big)^2 \\right\\} \\\\ \\large \\text{subject to } \\ \\sum_{j=1}^p\\beta_j^2 \\leq s\n",
    " \\end{gather}\n",
    " \n",
    " __Lasso__:\n",
    "  \\begin{gather}\n",
    " \\large \\min_{\\mathbf{\\beta}} \\left\\{ \\sum_{i=1}^n \\Big(y_i - \\beta_0 - \\sum_{j=1}^p \\beta_jx_{ij}\\Big)^2 \\right\\} \\\\ \\large \\text{subject to } \\ \\sum_{j=1}^p\\left|\\beta_j\\right| \\leq s\n",
    " \\end{gather}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-breakdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another Formulation for Ridge Regression and the Lasso, continued\n",
    "\n",
    "- The formulas in the previous slide mean that for every value of Œª, there is some s such that the constraint minization yields the same result as our first definition of Lasso and Ridge\n",
    "- When p=2 (2 predictors): \n",
    "    - the lasso coefficient estimates have the smallest RSS such that $\\left|\\beta_1\\right|+ \\left|\\beta_2\\right| \\leq s $\n",
    "    - the ridge coefficient estimates have the smallest RSS such that $\\beta_1^2 + \\beta_2^2 \\leq s$\n",
    "\n",
    "- We can think of it as follows.\n",
    "    - When we perform the Lasso or Ridge we are trying to find the set of coefficient estimates that lead to the smallest RSS, subject to the constraint that there is a budget s for how large $\\sum_{j=1}^p \\left|\\beta_j\\right|$ or \n",
    "$\\sum_{j=1}^p \\beta_j^2$ can be.\n",
    "\n",
    "- If s is large the restriction is not binding (not restrictive)\n",
    "- For s large enough you get the OLS estimates (which are unconstrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-amount",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![title](lasso_ridge.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-recovery",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## OLS vs Ridge and Lasso\n",
    "- In the previous slide, the OLS solution is marked as $\\hat{\\beta}$ and lies outside the constraint\n",
    "- If s was sufficiently large, Ridge and Lasso estimates would be the same as OLS (case where $\\lambda=0$)\n",
    "- The ellipses that are centered around $\\hat{\\beta}$ represent regions of constant RSS. \n",
    "- As the ellipses expand away from the least squares coefficient estimates, the RSS increases. \n",
    "- The lasso and ridge regression coefficient estimates are given by the first point at which an ellipse contacts the constraint region. \n",
    "- Since ridge regression has a circular constraint with no sharp points, this intersection will not generally occur on an axis, and so the ridge regression coefficient estimates will be exclusively non-zero. \n",
    "- However, the lasso constraint has corners at each of the axes, and so the ellipse will often intersect the constraint region at an axis. When this occurs, one of the coefficients will equal\n",
    "     - Here, the intersection occurs at Œ≤1 = 0, and so theresulting model will only include Œ≤2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-probability",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Selecting the tuning parameter\n",
    "- Since a lot seems to depend on the value of $\\lambda$ which value should you choose?\n",
    "- As usual, we need to remember that our end goal is to maximize out of sample prediction\n",
    "- As such the right model and/or the right tuning parameter will be given by cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-conclusion",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![image.png](lasso_lambda.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088b7ca8",
   "metadata": {},
   "source": [
    "# In-Class Exercise\n",
    "\n",
    "Q5: What is the difference between Lasso and Ridge regression? \n",
    "\n",
    "Q6: How could we decide which model to use (Lasso, Ridge, or OLS)? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d171285",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Here is an example of using built-in Python commands to do cross-validated Lasso and Ridge\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "Xdata = X_train\n",
    "#Xdata = X\n",
    "\n",
    "\n",
    "results_lasso = LassoCV(cv=10, random_state=31415).fit(Xdata, y)\n",
    "\n",
    "print(\"Lasso CV selected lambda = {} with an $R^2$ of {}\".format(\n",
    "    round(results_lasso.alpha_,2), \n",
    "    round(results_lasso.score(Xdata,y), 2)))\n",
    "\n",
    "#print(results_lasso.coef_)\n",
    "                  \n",
    "\n",
    "results_ridge = RidgeCV(cv=10).fit(Xdata, y)\n",
    "\n",
    "\n",
    "print(\"Ridge CV selected lambda = {} with an $R^2$ of {}\".format(\n",
    "    round(results_ridge.alpha_,2), \n",
    "    round(results_ridge.score(Xdata,y), 2)))\n",
    "\n",
    "#print(results_ridge.coef_)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
