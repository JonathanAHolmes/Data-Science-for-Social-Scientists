{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "seventh-language",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <div align=\"center\"> SPECIAL TOPICS III </div>\n",
    "## <div align=\"center\"> Data Science for Social Scientists  </div>\n",
    "### <div align=\"center\"> ECO 4199 </div>\n",
    "#### <div align=\"center\">Class 7 - Model Selection</div>\n",
    "<div align=\"center\"> Fabien Forge, (he/him)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-mustang",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Roadmap\n",
    "\n",
    "- In the last lecture we saw the risk associated with overfitting. \n",
    "- Although we discussed the case of polynomial regressions, the risk of overfitting is common to all learning models\n",
    "- This risk usually increases with the number of parameters\n",
    "$$ Y = f(\\mathbf{X}) + \\varepsilon$$\n",
    "- Regression and classification used tools that you were already familiar with (at least to some extent)\n",
    "- But we only scratched the surface of what these functions may look like\n",
    "- Today, we will keep using these tools with a different goal in mind\n",
    "- This goal will require new tools..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-mongolia",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dealing with overfitting\n",
    "- Last lecture, we saw the issues associated with overfitting\n",
    "- You may think that the issue came from allowing for (undue) higher degrees in our model\n",
    "- But the issue is broader than this.\n",
    "- We used a model in which Y was related to $X$ and $X^2$ not $X^n$ for n>2\n",
    "- In other words we were using too many predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-appearance",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RSS and MSE\n",
    "- the training set MSE is generally an underestimate of the test MSE. \n",
    "    - Recall that MSE = RSS/n. \n",
    "- When we fit a model to the training data using least squares, we specifically estimate the regression coefficients such that the training RSS (but not the test RSS) is as small as possible. \n",
    "- In particular, the training error will decrease as more variables are included in the model, but the test error may not.\n",
    "- Therefore, training set RSS and training set R2 cannot be used to select from among a set of models with different numbers of variables.\n",
    "- However, a number of techniques for adjusting the training error for the model size are available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-simpson",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Subset Selection\n",
    "$$Y = \\beta_0+\\beta_1X_1+\\beta_2X_2+ ... + \\beta_pX_p + \\epsilon$$\n",
    "- The issue it that adding predictors will always weakly improve the in sample prediction\n",
    "    - But at the expense of out of sample prediction\n",
    "- It is therefore important to limit the number of predictors to those actually related to Y\n",
    "- __Subset Selection__ _is the process of identifying the p predictors actually related to Y._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-defense",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Best Subset Selection\n",
    "- Here is the algorithm to select the best subset given a dataset with p predictors\n",
    "- We will apply this algorithm to the Credit dataset\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "1. Let $\\mathscr{M}_0$ denote the null model , which contains no predictors. This\n",
    "model simply predicts the sample mean for each observation.\n",
    "2. For k = 1, 2, . . .p:\n",
    "    - a. Fit all ${P\\choose k}$ containing exactly $k$ predictors\n",
    "    - b. Pick the best among these ${P\\choose k}$ and call it $\\mathscr{M}_k$. Here best is defined as having the smallest RSS, or equivalently largest R2.\n",
    "3. Select a single best model from among $\\mathscr{M}_0$, . . . ,$\\mathscr{M}_p$ using crossvalidated prediction error (MSE), AIC, BIC, or adjusted $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-richardson",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The null model\n",
    "- Let's start with the null model\n",
    "- This a model using no predictors and a single parameter: the intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "secret-experience",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import sklearn.linear_model as skl_lm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "from itertools import permutations\n",
    "\n",
    "\n",
    "folderPath=\"~/Dropbox/Data Science for Social Scientists/Classes/Class 7 – Model Selection/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "colonial-superior",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 12)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Income</th>\n",
       "      <th>Limit</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Cards</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Student</th>\n",
       "      <th>Married</th>\n",
       "      <th>Balance</th>\n",
       "      <th>Asian</th>\n",
       "      <th>Caucasian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.891</td>\n",
       "      <td>3606</td>\n",
       "      <td>283</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>333</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106.025</td>\n",
       "      <td>6645</td>\n",
       "      <td>483</td>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>903</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104.593</td>\n",
       "      <td>7075</td>\n",
       "      <td>514</td>\n",
       "      <td>4</td>\n",
       "      <td>71</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>580</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148.924</td>\n",
       "      <td>9504</td>\n",
       "      <td>681</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>964</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55.882</td>\n",
       "      <td>4897</td>\n",
       "      <td>357</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>331</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Income  Limit  Rating  Cards  Age  Education  Gender  Student  Married  \\\n",
       "0   14.891   3606     283      2   34         11       0        0        1   \n",
       "1  106.025   6645     483      3   82         15       1        1        1   \n",
       "2  104.593   7075     514      4   71         11       0        0        0   \n",
       "3  148.924   9504     681      3   36         11       1        0        0   \n",
       "4   55.882   4897     357      2   68         16       0        0        1   \n",
       "\n",
       "   Balance  Asian  Caucasian  \n",
       "0      333      0          1  \n",
       "1      903      1          0  \n",
       "2      580      1          0  \n",
       "3      964      1          0  \n",
       "4      331      0          1  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(folderPath+\"Credit.csv\" ,usecols=list(range(1,12)))\n",
    "df['Gender']=df['Gender'].replace({\" Male\":0,  \"Female\":1})\n",
    "df['Student']=df['Student'].replace({\"No\":0,  \"Yes\":1})\n",
    "df['Married']=df['Married'].replace({\"No\":0,  \"Yes\":1})\n",
    "df=df.join(pd.get_dummies(df['Ethnicity'], drop_first=True))\n",
    "df.pop(\"Ethnicity\")\n",
    "display(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-stuart",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\mathscr{M}_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "interpreted-bangkok",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                Balance   R-squared:                      -0.000\n",
      "Model:                            OLS   Adj. R-squared:                 -0.000\n",
      "Method:                 Least Squares   F-statistic:                      -inf\n",
      "Date:                Tue, 09 Mar 2021   Prob (F-statistic):                nan\n",
      "Time:                        11:54:08   Log-Likelihood:                -3019.4\n",
      "No. Observations:                 400   AIC:                             6041.\n",
      "Df Residuals:                     399   BIC:                             6045.\n",
      "Df Model:                           0                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    520.0150     22.988     22.621      0.000     474.822     565.208\n",
      "==============================================================================\n",
      "Omnibus:                       28.709   Durbin-Watson:                   1.945\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               27.405\n",
      "Skew:                           0.582   Prob(JB):                     1.12e-06\n",
      "Kurtosis:                       2.464   Cond. No.                         1.00\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/statsmodels/regression/linear_model.py:1657: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return self.ess/self.df_model\n"
     ]
    }
   ],
   "source": [
    "# M0\n",
    "results = smf.ols('Balance ~ 1 ', data=df).fit()\n",
    "# Inspect the results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "irish-commonwealth",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Income</th>\n",
       "      <th>Limit</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Cards</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Student</th>\n",
       "      <th>Married</th>\n",
       "      <th>Asian</th>\n",
       "      <th>Caucasian</th>\n",
       "      <th>intercept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.891</td>\n",
       "      <td>3606</td>\n",
       "      <td>283</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106.025</td>\n",
       "      <td>6645</td>\n",
       "      <td>483</td>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104.593</td>\n",
       "      <td>7075</td>\n",
       "      <td>514</td>\n",
       "      <td>4</td>\n",
       "      <td>71</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148.924</td>\n",
       "      <td>9504</td>\n",
       "      <td>681</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55.882</td>\n",
       "      <td>4897</td>\n",
       "      <td>357</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Income  Limit  Rating  Cards  Age  Education  Gender  Student  Married  \\\n",
       "0   14.891   3606     283      2   34         11       0        0        1   \n",
       "1  106.025   6645     483      3   82         15       1        1        1   \n",
       "2  104.593   7075     514      4   71         11       0        0        0   \n",
       "3  148.924   9504     681      3   36         11       1        0        0   \n",
       "4   55.882   4897     357      2   68         16       0        0        1   \n",
       "\n",
       "   Asian  Caucasian  intercept  \n",
       "0      0          1          1  \n",
       "1      1          0          1  \n",
       "2      1          0          1  \n",
       "3      1          0          1  \n",
       "4      0          1          1  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get y and X\n",
    "y=df['Balance'] # y\n",
    "X=df.loc[:,~df.columns.str.contains('Balance')] # X\n",
    "X['intercept']=1\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-martial",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\mathscr{M}_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "republican-maximum",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² for predictor Income is 0.622637\n",
      "R² for predictor Limit is 0.858626\n",
      "R² for predictor Rating is 0.845239\n",
      "R² for predictor Cards is 0.506125\n",
      "R² for predictor Age is 0.516919\n",
      "R² for predictor Education is 0.533170\n",
      "R² for predictor Gender is 0.385951\n",
      "R² for predictor Student is 0.370826\n",
      "R² for predictor Married is 0.400392\n",
      "R² for predictor Asian is 0.416977\n",
      "\n",
      "Best predictor for M1 is: Limit with 0.859.\n"
     ]
    }
   ],
   "source": [
    "# initiate empty list to store R2\n",
    "R2= 0\n",
    "best_predictor=\"none\"\n",
    "X=df.loc[:,~df.columns.str.contains('Balance')]\n",
    "y=df.loc[:,df.columns.str.contains('Balance')]\n",
    "\n",
    "# for each predictor in X\n",
    "for k in range(X.shape[1]-1): # minus 1 because last column is intercept\n",
    "    results = sm.OLS(y, X.iloc[:,[k,-1]]).fit() # fit predictor in position p and intercept (in last position, -1)\n",
    "    print(f\"R\\u00b2 for predictor {X.columns[k]} is {round(results.rsquared,6):f}\")\n",
    "    if results.rsquared>R2:\n",
    "        R2=results.rsquared ; best_predictor=X.columns[k]\n",
    "print(\"\\nBest predictor for M1 is: {} with {}.\".format(best_predictor,round(R2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "regulation-sapphire",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best predictor for M0 is:  with 0.276.\n",
      "Best predictor for M1 is: Asian, Caucasian with 0.858.\n",
      "Best predictor for M2 is: Asian, Married, Caucasian with 0.897.\n",
      "Best predictor for M3 is: Asian, Married, Student, Caucasian with 0.93.\n",
      "Best predictor for M4 is: Asian, Married, Student, Gender, Caucasian with 0.964.\n",
      "Best predictor for M5 is: Asian, Married, Student, Gender, Education, Caucasian with 0.969.\n",
      "Best predictor for M6 is: Asian, Married, Student, Gender, Education, Age, Caucasian with 0.969.\n",
      "Best predictor for M7 is: Asian, Married, Student, Gender, Education, Age, Cards, Caucasian with 0.97.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-a1e216b2abc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fit predictors from model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;31m# append statistics to list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mR2_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsquared\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m;\u001b[0m \u001b[0madj_R2_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsquared_adj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1759\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1760\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1761\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1762\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1763\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   2077\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2079\u001b[0;31m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2081\u001b[0m             \u001b[0;31m# if the dim was reduced, then pass a lower-dim the next time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2126\u001b[0m         \u001b[0;31m# a list of integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2127\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_list_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2130\u001b[0m         \u001b[0;31m# a single integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2108\u001b[0m         \"\"\"\n\u001b[1;32m   2109\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2111\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2112\u001b[0m             \u001b[0;31m# re-raise with different error message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[0;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[1;32m   3407\u001b[0m         \u001b[0mSee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocstring\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mexplanation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3408\u001b[0m         \"\"\"\n\u001b[0;32m-> 3409\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3410\u001b[0m         \u001b[0;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3411\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m   3393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3394\u001b[0m         new_data = self._data.take(\n\u001b[0;32m-> 3395\u001b[0;31m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3396\u001b[0m         )\n\u001b[1;32m   3397\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[1;32m   1384\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1386\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_convert_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/indexers.py\u001b[0m in \u001b[0;36mmaybe_convert_indices\u001b[0;34m(indices, n)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"indices are out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DFs=[]\n",
    "#for p in range(X.shape[1]): # uncomment here if you want to do it for all RUN AT YOUR OWN RISK\n",
    "for p in range(8):\n",
    "    R2=0\n",
    "    best_predictor=\"none\"\n",
    "    R2_list=[]; adj_R2_list=[] ; aic=[] ; bic=[] ; models=[]\n",
    "    for k in permutations(list(range(X.shape[1]-1)), p):\n",
    "        model=list(k)\n",
    "        model.append(-1)\n",
    "        results = sm.OLS(y, X.iloc[:,model]).fit() # fit predictors from model\n",
    "        # append statistics to list\n",
    "        R2_list.append(results.rsquared) ; adj_R2_list.append(results.rsquared_adj) \n",
    "        aic.append(results.aic); bic.append(results.bic) ; models.append(','.join(X.columns[model]))\n",
    "\n",
    "        if results.rsquared_adj>R2:\n",
    "            model.pop(-1)\n",
    "            R2=results.rsquared_adj \n",
    "    DFs.append(pd.DataFrame(data={'R2':R2_list,'Adjusted R2':adj_R2_list, \"AIC\":aic, \"BIC\":bic,'Predictors':p,'model':models}))\n",
    "    \n",
    "    print(\"Best predictor for M{} is: {} with {}.\".format(p,', '.join(X.columns[model]),round(R2,3)))\n",
    "    \n",
    "dd=pd.concat(DFs)\n",
    "dd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-equality",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computational challenge\n",
    "- Best subset selection is a simple and conceptually appealing \n",
    "- But the number of possible models that must be considered grows rapidly as p increases. \n",
    "- In general, there are 2p models that involve subsets of p predictors. \n",
    "    - So if p = 10, then there are approximately 1,000 possible models to be considered!\n",
    "    - So if p = 20, then there are more than 1,000,000 possible models to be considered!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-register",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![title](credit_10predictors.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-instruction",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dd_best=dd.groupby(['Predictors']).agg({\"R2\":np.max, \"Adjusted R2\":np.max, \"AIC\":np.min, \"BIC\":np.min}).reset_index()\n",
    "dd_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-identifier",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Akaike information criterion (AIC)\n",
    "\n",
    "- The AIC criterion is defined for a large class of models fit by maximum likelihood. \n",
    "    - Recall that OLS regressions are a subset of maximum likelihood\n",
    "- For a fitted least squares model containing p predictors\n",
    "$$\\large \\text{AIC}=\\frac{1}{n\\hat{\\sigma}^2}(\\text{RSS} + 2p\\hat{\\sigma}^2)$$\n",
    "- Also, recall that MSE is defined as $\\frac{1}{n} \\text{RSS}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-movie",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Akaike information criterion (AIC), continued\n",
    "- $\\hat{\\sigma}^2$ is an estimate of the variance of the error $\\varepsilon$ associated with each response measurement \n",
    "- $2p\\hat{\\sigma}^2$  is a statistical penalty on the training RSS\n",
    "    - In order to account for the fact that training error tends to underestimate the test error\n",
    "    - The penalty increases with the number of predictors $p$ to adjust for the corresponding decrease in training RSS\n",
    "- Smaller values of AIC indicates better fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-palace",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian information criterion (BIC)\n",
    "$$\\large \\text{BIC}=\\frac{1}{n\\hat{\\sigma}^2}(\\text{RSS} + \\log(n)p\\hat{\\sigma}^2)$$\n",
    "- $n$ represents the number of observations\n",
    "- Notice that BIC replaces the $2p\\hat{\\sigma}^2$ used by AIC with a $\\log(n)p\\hat{\\sigma}^2$\n",
    "term, where n is the number of observations\n",
    "- Since $\\log(n) > 2$ for any n > 7, the BIC statistic generally places a heavier penalty on models with many variables compared with AIC \n",
    "- Here too, smaller values of AIC indicates better fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-workplace",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Adjusted $R^2$\n",
    "$$\\large \\text{Adjusted R}^2 = 1- \\frac{\\frac{\\text{RSS}}{n-p-1}}{\\frac{TSS}{n-1}}$$\n",
    "- Recall that the usual $R^2= 1- \\frac{\\text{RSS}}{\\text{TSS}}$\n",
    "- Also $n > p$ always (at least for now)\n",
    "- Since RSS always decreases as more variables are added to the model, the $R^2$ always increases as more variables are added. \n",
    "- For a least squares model with p variables, the adjusted R2 statistic is calculated as\n",
    "- Large values of adjusted $R^2$ indicate a model with small error. \n",
    "- Maximizing the adjusted $R^2$ is equivalent to minimizing $\\frac{\\text{RSS}}{n-p-1}$\n",
    "    - While RSS always decreases as the number of variables in the model increases\n",
    "    - $\\frac{\\text{RSS}}{n-p-1}$ may increase or decrease, due to the presence of p in the denominator.\n",
    "- The intuition behind the adjusted $R^2$ is that once all of the correct variables have been included in the model, adding additional noise variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-madonna",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, figsize=(10,10),sharex=True)\n",
    "axes = axes.ravel() # access axes with a single position instead of 2\n",
    "for i, statistics in enumerate(['R2',\"Adjusted R2\",\"AIC\",\"BIC\"]):\n",
    "    sns.scatterplot(x='Predictors',y=statistics,data=dd,ax=axes[i], color='gray',marker='.',alpha=.3)\n",
    "    sns.lineplot(x='Predictors',y=statistics,data=dd_best,ax=axes[i], color='darkorange')\n",
    "    sns.scatterplot(x='Predictors',y=statistics,data=dd_best,ax=axes[i], color='darkgreen')\n",
    "    axes[i].set_ylabel(statistics)\n",
    "    axes[i].set_xticks(np.arange(p+1))\n",
    "    \n",
    "fig.suptitle(\"In-Sample Statistics\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-conditioning",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 3 of Best Subset Selection\n",
    "- You can think of this stage in terms of last lecture:\n",
    "    - You have many models (last time many polynomial models), one for each p\n",
    "    - But they all maximize the in sample fit\n",
    "- In step 3, you can do this based on the adjusted-$R^2$, AIC, BIC or you can use CV techniques to find the model that gives the __best out of sample prediction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-assets",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dd['best']=dd.groupby(['Predictors'])['Adjusted R2'].transform(np.max)\n",
    "dd_best2=dd.loc[dd['Adjusted R2']==dd['best']].groupby('Predictors').first().reset_index()\n",
    "dd_best2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-range",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "models=[\"intercept\",[\"Rating\",\"intercept\"], \n",
    "        [\"Income\",\"Rating\",\"intercept\"],\n",
    "        [\"Income\",\"Rating\",\"Student\",\"intercept\"],\n",
    "        [\"Income\",\"Limit\",\"Cards\",\"Student\",\"intercept\"],\n",
    "        [\"Income\",\"Limit\",\"Rating\",\"Cards\",\"Student\",\"intercept\"],\n",
    "        [\"Income\",\"Limit\",\"Rating\",\"Cards\",\"Age\",\"Student\",\"intercept\"],\n",
    "        [\"Income\",\"Limit\",\"Rating\",\"Cards\",\"Age\",\"Gender\",\"Student\",\"intercept\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['intercept']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-reform",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Best number of predictors using Cross Validation\n",
    "# use best model from precedent exercise to speed up the code\n",
    "kfold=5\n",
    "DFs=[]\n",
    "kf = KFold(n_splits=kfold, random_state=1706, shuffle=True)\n",
    "\n",
    "for i,m in enumerate(models):\n",
    "    MSEs=[] # empty list of MSE scores\n",
    "    X=df[m].values\n",
    "    y=df['Balance'].values\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        if i==0:\n",
    "            X_train, X_test = X[train_index].reshape(-1, 1), X[test_index].reshape(-1, 1)\n",
    "        else:\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # regression\n",
    "        reg = LinearRegression() # initiate the regression class\n",
    "        reg.fit(X_train,y_train) # fit the data\n",
    "        # Out of Sample MSE:\n",
    "        mse=mean_squared_error(y_test, reg.predict(X_test))\n",
    "        MSEs.append(mse)\n",
    "    DFs.append(pd.DataFrame({'Predictors':i,'MSE':MSEs}))\n",
    "    \n",
    "MSE_scores=pd.concat(DFs)\n",
    "mse=MSE_scores.groupby('Predictors').mean().reset_index()    \n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-faculty",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dd_best=dd_best.merge(mse, on='Predictors')\n",
    "dd_best.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-activity",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Best number of predictors for other statistics\n",
    "adj_R2_best=int(dd_best2.loc[dd_best2['Adjusted R2']==dd_best2['Adjusted R2'].max(),'Predictors'])\n",
    "aic_best=int(dd_best2.loc[dd_best2['AIC']==dd_best2['AIC'].min(),'Predictors'])\n",
    "bic_best=int(dd_best2.loc[dd_best2['BIC']==dd_best2['BIC'].min(),'Predictors'])\n",
    "mse_best=int(mse.loc[mse.MSE==mse.MSE.min(),'Predictors'])\n",
    "best_preds=[adj_R2_best,aic_best,bic_best,mse_best]\n",
    "print(f\"Best number of parameters for\\nAdjusted R-square: {adj_R2_best}\\nAIC: {aic_best}\\nBIC: {bic_best}\\n10-fold CV:{mse_best} \")\n",
    "\n",
    "fig, axes = plt.subplots(2,2, figsize=(12,12),sharex=True)\n",
    "axes = axes.ravel() # access axes with a single position instead of 2\n",
    "for i, statistics in enumerate([\"Adjusted R2\",\"AIC\",\"BIC\", \"MSE\"]):\n",
    "    sns.lineplot(x='Predictors',y=statistics,data=dd_best,ax=axes[i], color='darkorange')\n",
    "    sns.scatterplot(x='Predictors',y=statistics,data=dd_best,ax=axes[i], color='darkgreen')\n",
    "    # axes[i].axvline(best_preds[i], color='k')\n",
    "    axes[i].scatter(x=best_preds[i],y=float(dd_best.loc[best_preds[i],statistics]),marker='X',color='red',s=100)\n",
    "    axes[i].set_ylabel(statistics)\n",
    "    axes[i].set_xticks(np.arange(p+1))\n",
    "fig.suptitle(\"Best number of parameters by technique\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-communications",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solutions to computational challenge: Forward Stepwise Selection\n",
    "\n",
    "1. Let $\\mathscr{M}_0$ denote the null model, which contains no predictors.\n",
    "2. For $k = 0, \\dots , p − 1$:\n",
    "    - (a) Consider all p − k models that augment the predictors in $\\mathscr{M}_k$ with one additional predictor.\n",
    "    - (b) Choose the best among these p − k models, and call it $\\mathscr{M}_{k+1}$.\n",
    "        - Here best is defined as having smallest RSS or highest R2.\n",
    "3. Select a single best model from among $\\mathscr{M}_0$, $\\dots$ ,$\\mathscr{M}_p$ using crossvalidated prediction error, AIC, BIC, or adjusted R2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-lounge",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solutions to computational challenge: Backward Stepwise Selection\n",
    "\n",
    "1. Let $\\mathscr{M}_p$ denote the full model, which contains all p predictors.\n",
    "2. For $k = p, p − 1, \\dots , 1$:\n",
    "    - (a) Consider all k models that contain all but one of the predictors in $\\mathscr{M}_k$, for a total of k − 1 predictors.\n",
    "    - (b) Choose the best among these k models, and call it $\\mathscr{M}_{k-1}$. \n",
    "        - Here best is defined as having smallest RSS or highest $R^2$.\n",
    "3. Select a single best model from among $\\mathscr{M}_0, \\dots ,\\mathscr{M}_p$ using crossvalidated prediction error, AIC, BIC, or adjusted R2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-regard",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Taking Stock\n",
    "- Last week we saw the risks associated with overfitting\n",
    "- This risk increases with the number of parameters\n",
    "- Different techniques yield different types of subsets but all penalize, one way or another, having too many parameters\n",
    "- The trade-off therefore is to find the best out of sample prediction using the least number of predictors possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-china",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Shrinkage Methods\n",
    "- Think back on the credit dataset\n",
    "- The data set has a number of predictors, which all seem reasonable\n",
    "    - All seem to be legitimate predictors of Balance\n",
    "    - There no variable irrelevant variable\n",
    "- Instead of our iterative, and long, process it would be nicer to fit all p predictors using a technique that __constrains__ or __regularizes the coefficient estimates__, or equivalently, that shrinks the coefficient estimates towards zero. \n",
    "    - Instead of cherry picking parameters we, instead, force the parameters of redundant predictors to be small or zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-founder",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ridge Regression\n",
    "- OLS regression for a model with p parameters finds that $\\beta_0, \\beta_1, ... \\beta_p$ that minimize (as you know):\n",
    "$$\\Large \\text{RSS} = \\sum_{i=1}^n \\Big(y_i - \\beta_0 - \\sum_{j=1}^p \\beta_jx_{ij}\\Big)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-exhaust",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ridge Regression, continued\n",
    "- Ridge regression is very similar to least squares, except that the coefficients ridge are by minimizing a slightly different quantity. \n",
    "- In particular, the ridge regression coefficient estimates $\\hat{\\beta^R}$ are the values that minimize:\n",
    "\\begin{gather}\n",
    "\\Large \\sum_{i=1}^n \\Big(y_i - \\beta_0 - \\sum_{j=1}^p \\beta_jx_{ij}\\Big)^2 + \\lambda \\sum_{j=1}^p\\beta_j^2 \\\\\n",
    "= \\Large\\text{RSS} + \\lambda \\sum_{j=1}^p\\beta_j^2\n",
    "\\end{gather}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-merit",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ridge Regression, shrinkage\n",
    "- $\\lambda$ is known as a __tuning parameter__ that is determined outside of the minimization problem\n",
    "- As with OLS, Ridge regression seeks coefficient estimates that fit the data well (small RSS)\n",
    "- Unlike OLS, the second term $\\lambda \\sum_{j=1}^p\\beta_j^2$ is small if $\\beta_1, ..., \\beta_j$ are small ($\\beta_0$ not included!)\n",
    "    - This second term is known as a __shrinkage penalty__\n",
    "- The tuning parameter $\\lambda$ serves to control the relative impact of these two terms on the regression coefficient estimates.\n",
    "    - When $\\lambda$ = 0, the penalty term has no effect, and ridge regression = OLS\n",
    "    - $\\lambda \\to \\infty$, the impact of the shrinkage penalty grows, and the ridge regression coefficient estimates will approach zero.\n",
    "- As you will see, the optimal $\\lambda$ is given using cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-spare",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Standardization of your dataset\n",
    "- Standardization of datasets is a common requirement for many machine learning estimators implemented see this tutorial in [scikit-learn](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "\n",
    "- You can transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.\n",
    "\n",
    "- Models such as the Ridge regression assume that all features are centered around zero and have variance in the same order. \n",
    "- If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n",
    "- In other words, in the OLS regression, multiplying X by a constant c will change $\\beta$ to $\\frac{\\beta}{c}$\n",
    "- In the Ridge regression, the $\\beta^R$ will depend not only on the value of λ, but also on the scaling of the $j^{th}$ predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-editor",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-malaysia",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# import the preprocessing module from sklearn\n",
    "from sklearn import preprocessing\n",
    "X_train =df[['Income','Limit','Rating','Student','Cards','Age','Education','Gender','Married','Asian','Caucasian']]\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_train.mean(axis=0) , X_train.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-purchase",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's now see what happens to our estimates as the value for $\\lambda$ changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-addiction",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "lambdas = 10**np.linspace(10,-2,100)*0.5\n",
    "ridge = Ridge()\n",
    "coefs = []\n",
    "\n",
    "for 𝜆 in lambdas:\n",
    "    ridge.set_params(alpha=𝜆)\n",
    "    ridge.fit(X_train, y)\n",
    "    coefs.append(ridge.coef_)\n",
    "ridge_results=pd.DataFrame(coefs,columns=['Income','Limit','Rating','Student','Cards','Age','Education','Gender','Married','Asian','Caucasian'])\n",
    "ridge_results['Lambda']=lambdas  \n",
    "ridge_results=pd.melt(ridge_results,id_vars=['Lambda'], var_name='Beta', value_name='Estimate')\n",
    "ridge_results.head()                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-corruption",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(1,1, figsize=(10,10))\n",
    "sns.lineplot(x='Lambda', y='Estimate', hue='Beta',data=ridge_results)\n",
    "ax.set_xscale('log')\n",
    "ax.axhline(0,color='k',linestyle=\":\")\n",
    "plt.axis('tight')\n",
    "plt.xlabel(r'$\\lambda$', fontsize=20)\n",
    "plt.ylabel('Standardized Coefficients',fontsize=20)\n",
    "plt.title('Ridge coefficients as a function of the regularization');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-consortium",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ridge and Model Selection\n",
    "- The ridge regression is clearly faster than our best subset methodology\n",
    "- Note though that we never truly select a model in the sense that we never use a subset of predictors\n",
    "- Instead, we are shrinking how much they matter in our prediction but use all p predictors (unless λ = ∞).\n",
    "- This may not be a problem for prediction accuracy, but it can create a challenge in model interpretation in settings in which the number of variables p is quite large. \n",
    "- For example, in the Credit data set, it appears that the most important variables are income, limit, rating, and student. \n",
    "- So we might wish to build a model including just these predictors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-kitty",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lasso Regression\n",
    "- The Lasso regression is an alternative to the Ridge regression as it allows to shrink parmeters to zero\n",
    "- The lasso regression coefficient estimates $\\hat{\\beta^L_\\lambda}$ are the values that minimize:\n",
    "\\begin{gather}\n",
    "\\Large\\text{RSS} + \\lambda \\sum_{j=1}^p\\left|\\beta_j\\right|\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-davis",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lasso and Model Selection\n",
    "- As with ridge regression, the lasso shrinks the coefficient estimates towards zero. \n",
    "- However, in the case of the lasso, the $\\ell_1$ penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter λ is sufficiently large. \n",
    "- Hence, much like best subset selection, the lasso performs variable selection.\n",
    "- We say that the lasso yields __sparse models__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-paper",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lambdas = 10**np.linspace(10,-2,100)*0.5\n",
    "lasso = Lasso()\n",
    "coefs = []\n",
    "\n",
    "for 𝜆 in lambdas:\n",
    "    lasso.set_params(alpha=𝜆)\n",
    "    lasso.fit(X_train, y)\n",
    "    coefs.append(lasso.coef_)\n",
    "lasso_results=pd.DataFrame(coefs,columns=['Income','Limit','Rating','Student','Cards','Age','Education','Gender','Married','Asian','Caucasian'])\n",
    "lasso_results['Lambda']=lambdas  \n",
    "lasso_results=pd.melt(lasso_results,id_vars=['Lambda'], var_name='Beta', value_name='Estimate')\n",
    "lasso_results.head()                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-slovakia",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(1,1, figsize=(10,10))\n",
    "ax = plt.gca()\n",
    "sns.lineplot(x='Lambda', y='Estimate', hue='Beta',data=lasso_results)\n",
    "ax.set_xscale('log')\n",
    "ax.axhline(0,color='k',linestyle=\":\")\n",
    "plt.axis('tight')\n",
    "plt.xlabel(r'$\\lambda$', fontsize=20)\n",
    "plt.ylabel('Standardized Coefficients',fontsize=20)\n",
    "plt.title('Lasso coefficients as a function of the regularization');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-writer",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another Formulation for Ridge Regression and the Lasso\n",
    "- You may have recognized something you are already familiar with as economists\n",
    "- The Lasso and Ridge regressions can be written in terms of objective function (to minimize) and a constraint\n",
    " \n",
    " __Ridge__:\n",
    " \\begin{gather}\n",
    " \\large \\min_{\\mathbf{\\beta}} \\left\\{ \\sum_{i=1}^n \\Big(y_i - \\beta_0 - \\sum_{j=1}^p \\beta_jx_{ij}\\Big)^2 \\right\\} \\\\ \\large \\text{subject to } \\ \\sum_{j=1}^p\\beta_j^2 \\leq s\n",
    " \\end{gather}\n",
    " \n",
    " __Lasso__:\n",
    "  \\begin{gather}\n",
    " \\large \\min_{\\mathbf{\\beta}} \\left\\{ \\sum_{i=1}^n \\Big(y_i - \\beta_0 - \\sum_{j=1}^p \\beta_jx_{ij}\\Big)^2 \\right\\} \\\\ \\large \\text{subject to } \\ \\sum_{j=1}^p\\left|\\beta_j\\right| \\leq s\n",
    " \\end{gather}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-breakdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another Formulation for Ridge Regression and the Lasso, continued\n",
    "\n",
    "- The formulas in the previous slide mean that for every value of λ, there is some s such that the constraint minization yields the same result as our first definition of Lasso and Ridge\n",
    "- When p=2 (2 predictors): \n",
    "    - the lasso coefficient estimates have the smallest RSS such that $\\left|\\beta_1\\right|+ \\left|\\beta_2\\right| \\leq s $\n",
    "    - the ridge coefficient estimates have the smallest RSS such that $\\beta_1^2 + \\beta_2^2 \\leq s$\n",
    "\n",
    "- We can think of it as follows.\n",
    "    - When we perform the Lasso or Ridge we are trying to find the set of coefficient estimates that lead to the smallest RSS, subject to the constraint that there is a budget s for how large $\\sum_{j=1}^p \\left|\\beta_j\\right|$ or \n",
    "$\\sum_{j=1}^p \\beta_j^2$ can be.\n",
    "\n",
    "- If s is large the restriction is not binding (not restrictive)\n",
    "- For s large enough you get the OLS estimates (which are unconstrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-amount",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![title](lasso_ridge.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-recovery",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## OLS vs Ridge and Lasso\n",
    "- In the previous slide, the OLS solution is marked as $\\hat{\\beta}$ and lies outside the constraint\n",
    "- If s was sufficiently large, Ridge and Lasso estimates would be the same as OLS (case where $\\lambda=0$)\n",
    "- The ellipses that are centered around $\\hat{\\beta}$ represent regions of constant RSS. \n",
    "- As the ellipses expand away from the least squares coefficient estimates, the RSS increases. \n",
    "- The lasso and ridge regression coefficient estimates are given by the first point at which an ellipse contacts the constraint region. \n",
    "- Since ridge regression has a circular constraint with no sharp points, this intersection will not generally occur on an axis, and so the ridge regression coefficient estimates will be exclusively non-zero. \n",
    "- However, the lasso constraint has corners at each of the axes, and so the ellipse will often intersect the constraint region at an axis. When this occurs, one of the coefficients will equal\n",
    "     - Here, the intersection occurs at β1 = 0, and so theresulting model will only include β2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-probability",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Selecting the tuning parameter\n",
    "- Since a lot seems to depend on the value of $\\lambda$ which value should you choose?\n",
    "- As usual, we need to remember that our end goal is to maximize out of sample prediction\n",
    "- As such the right model and/or the right tuning parameter will be given by cross validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "frequent-conclusion",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![image.png](lasso_lambda.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
