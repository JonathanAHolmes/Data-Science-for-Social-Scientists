{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b64ae1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <div align=\"center\"> SPECIAL TOPICS III </div>\n",
    "## <div align=\"center\"> Data Science for Social Scientists  </div>\n",
    "### <div align=\"center\"> ECO 4199 </div>\n",
    "#### <div align=\"center\">Class 6 - Linear Regressions</div>\n",
    "<div align=\"center\"> Jonathan Holmes, (he/him)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068ba1ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's make sure you have all the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eff5e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's make sure everything is running\n",
    "\n",
    "# data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-white') # set option\n",
    "\n",
    "# Statistics libraries\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats\n",
    "\n",
    "import itertools\n",
    "\n",
    "# Jupyter library to show video\n",
    "from IPython.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5ff3de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A motivating example\n",
    "\n",
    "- Today we will use the Advertising dataset.\n",
    "- This dataset is available from the R package ISLR or on the [book's webpage](https://www.statlearning.com/resources-first-edition)\n",
    "\n",
    "The data set consists of the sales of that product in 200 different markets, along with advertising budgets for the product in each of those markets for three different media: TV, radio, and newspaper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d255552e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let me set my current directory using the %cd magic\n",
    "%cd \"~/Dropbox/_teaching/ECO4199/2023/Data-Science-for-Social-Scientists/Class 06 - Linear Regressions/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead3aec8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ads = pd.read_csv('Advertising.csv', usecols=[1,2,3,4])\n",
    "display(ads.info())\n",
    "ads.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dde1b73",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prediction\n",
    "- More generally, suppose that we observe a quantitative response Y \n",
    "- We also have p different predictors, X1,X2, . . .,Xp. \n",
    "- We assume that there is some relationship between Y and X = (X1,X2, . . .,Xp)\n",
    "- This can be written in the very general form:\n",
    "$$ Y = f(\\mathbf{X}) + \\varepsilon $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b7304c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Making sense of the formula\n",
    "$$ Y = f(\\mathbf{X}) + \\varepsilon $$\n",
    "- Here f is some fixed but unknown __function__ of X1, . . . , Xp, in the __population__\n",
    "- $\\varepsilon$  is a random __error term__, which is independent of X and has mean zero. \n",
    "- In this formulation, $f$ represents the _systematic_ information that X provides about Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7c1c3f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is this function?\n",
    "\n",
    "- You may be wondering what is this function about?\n",
    "- Turns out you already kow at least one such function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b22c452",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\\begin{equation*}\n",
    "y= f(x) + \\varepsilon = \\beta_0 + \\beta_1 x + \\varepsilon\n",
    "\\end{equation*}\n",
    "\n",
    "- $\\beta_0$ and $\\beta_1$ are two unknown __parameters__ that represent\n",
    "the __intercept__ and __slope__ terms in the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbaf909",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simple linear regression\n",
    "- The __simple linear regression__ is the simplest way to predict a __quantitative response__ of Y\n",
    "- Here a single __predictor variable__ X influences Y. \n",
    "- It assumes that there is approximately a linear relationship between X and Y. \n",
    "- Before predicting Y, we first need to estimate these parameters.\n",
    "- Let's estimate three distinct simple linear regressions, one for each independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7793c2bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ads.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c212187",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "fig, axes = plt.subplots(1,3, figsize=(20,10), sharey=True)\n",
    "for i, v in enumerate(np.sort(ads.columns[ads.columns!='Sales'].tolist())):\n",
    "    axes[i].scatter(ads[v],ads['Sales'])\n",
    "    axes[i].set_xlabel(v.capitalize(), fontsize=20)\n",
    "    \n",
    "axes[0].set_ylabel(\"Sales\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41ced7c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Econometrics Review\n",
    "\n",
    "What is the ordinary least square method minimizing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9496e13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "fig, axes = plt.subplots(1,3, figsize=(20,10), sharey=True)\n",
    "for i, v in enumerate(np.sort(ads.columns[ads.columns!='Sales'].tolist())):\n",
    "    sns.regplot(data=ads, x=v, y='Sales', order=1, ci=None, ax=axes[i],scatter_kws={\"color\": \"black\", \"alpha\":.5})\n",
    "    axes[i].set_xlabel(v.capitalize(), fontsize=20)\n",
    "    \n",
    "axes[0].set_ylabel(\"Sales\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5cdf10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "results = smf.ols('Sales ~ TV ', data=ads).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4665b6a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In-Class Exercise #1 - Test Your Econometrics Knowledge! \n",
    "\n",
    "Question #1: Find the following information in the table: \n",
    "- What are the independent and dependent variable(s) are in this table? \n",
    "- How many observations are there in this regression? \n",
    "\n",
    "\n",
    "Question #2: Identify each of the following coefficients in this table: \n",
    "\n",
    "|   | coef | std err | t | P>abs(t) | [.025 | .975] |  \n",
    "| --- | :-: | :-: | :-: | :-: | :-: | :-: |\n",
    "| Intercept |     7.0326  |    0.458  |  15.360  |    0.000  |     6.130  |     7.935 |\n",
    "| TV        |     0.0475  |    0.003  |   17.668 |    0.000  |     0.042  |     0.053 |\n",
    "\n",
    "- $\\hat{\\beta_0}$: \n",
    "- $\\hat{\\beta_1}$: \n",
    "- The standard error of $\\hat{\\beta_1}$: \n",
    "- The 95% confidence interval of $\\hat{\\beta_1}$: \n",
    "- The 95\\% confidence interval of $\\hat{\\beta_0}$:\n",
    "\n",
    "Question #3: Is more TV spending associated with higher sales? What do you look at in the table to figure this out? \n",
    "\n",
    "\n",
    "Question #4: What does the column $P> |t|$ mean? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4dddcd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The OLS Estimator of $\\beta_1$\n",
    "\n",
    "\n",
    "- The formula for $\\hat{\\beta}_1$, which can be written:\n",
    "\\begin{equation*}\n",
    "\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\n",
    "\\end{equation*}\n",
    "- Again equivalently (after dividing top and bottom by $n - 1$):\n",
    "\\begin{equation*}\n",
    "\\hat{\\beta}_1 = \\frac{s_{XY}}{s_X^2}\n",
    "\\end{equation*}\n",
    "- So in the linear regression model with one regressor, the OLS estimator of $\\beta_1$ is the sample covariance of $X$ and $Y$ divided by the sample variance of $X$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb9bea7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# beta_1 = covariance of X and Y over the variance of X:\n",
    "cov_xy=ads[[\"Sales\",\"TV\"]].cov().iloc[0,1] # covariance of X and Y\n",
    "var_x=ads.TV.var() # Variance of X\n",
    "beta=cov_xy/var_x # beta_1, the slope, is the ratio of the 2\n",
    "print(\"The covariance of X and Y is {0}.\\nThe variance of X is {1}. \\n\\u03B2 is {2} as expected!\".format(cov_xy.round(2),round(var_x,2),beta.round(4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699c6fce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Interpretation of $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$\n",
    "\n",
    "-  $\\hat{\\beta}_0$ is the intercept: it tells you where to position the line along the y-axis\n",
    "- $\\hat{\\beta}_1$ is the slope: it tells you how steep is the line\n",
    "- Remember how to interpret these in our example?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c961c2c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualizing Ordinary Least Squares\n",
    "\n",
    "OLS = Ordinary _Least Squares_ \n",
    "\n",
    "What does \"Least Squares\" refer to? \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c0dafd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "ANSWER: It refers to _minimizing_ the sum of _squared residuals_ \n",
    "\n",
    "Equivalent: \n",
    "- Minimize RSS (residual sum of squares)\n",
    "- Minimize the  MSE (mean squared error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e300731d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "fig, ax = plt.subplots(1,1, figsize=(15,10))\n",
    "\n",
    "# plot {y_i, x_i}\n",
    "sns.scatterplot(data=ads,x='TV',y='Sales',color='darkorange', ax=ax )\n",
    "ax.set_xlabel('TV Commercials', fontsize=20)\n",
    "ax.set_ylabel(\"Sales\", fontsize=20)\n",
    "ax.set_xlim(xmin=0, xmax=ads['TV'].max())\n",
    "\n",
    "# line of best fit\n",
    "results = smf.ols('Sales ~ TV ', data=ads).fit() # regress sales on Tv (see below)\n",
    "alpha=results.params['Intercept']\n",
    "beta=results.params['TV']\n",
    "ads['line of best fit']=alpha+beta*ads['TV']\n",
    "sns.lineplot(data=ads, x='TV', y='line of best fit',color='darkblue', ax=ax)\n",
    "\n",
    "# plot the residuals\n",
    "ads['residuals'] = ads['line of best fit'] - ads['Sales']\n",
    "for _, row in ads.iterrows():    \n",
    "    ax.vlines(row['TV'],row['Sales'],row['Sales']+row['residuals'], color='grey',alpha=.5,linestyle=\":\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9eaa58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# OLS Regression\n",
    "\n",
    "- As you probably remember the OLS regression find the $\\beta_0$ and $\\beta_1$ that minimize the residual sum of squares\n",
    "- Let $\\hat{y_i} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_i$ be the prediction for Y based on the ith value of X.\n",
    "- Then $e_i = y_i−\\hat{y_i}$ represents the $i_{th}$ residual\n",
    "    - This is the difference between the $i_{th}$ observed value (actual data) and the $i_{th}$ predicted value\n",
    "\n",
    "- So the residual sum of squares (RSS) corresponds to \n",
    "\n",
    "$RSS = e^2_1 + e^2_2 + · · · + e^2_n$\n",
    "\n",
    "or\n",
    "\n",
    "$RSS = (y_1− \\hat{\\beta}_0− \\hat{\\beta}_1x_1)^2+(y_2− \\hat{\\beta}_0− \\hat{\\beta}_1x_2)^2+. . .+(y_n− \\hat{\\beta}_0− \\hat{\\beta}_1x_n)^2.$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8111f11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#%matplotlib widget\n",
    "\n",
    "ads.sort_values('TV',inplace=True)\n",
    "ads.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# Scatter plot\n",
    "fig, ax = plt.subplots(1,1, figsize=(15,10))\n",
    "\n",
    "# plot {y_i, x_i}\n",
    "sns.scatterplot(data=ads,x='TV',y='Sales',color='darkorange', ax=ax )\n",
    "ax.set_xlabel('TV Commercials', fontsize=20)\n",
    "ax.set_ylabel(\"Sales\", fontsize=20)\n",
    "ax.set_xlim(xmin=0, xmax=ads['TV'].max())\n",
    "\n",
    "# line of best fit\n",
    "results = smf.ols('Sales ~ TV ', data=ads).fit() # regress sales on Tv (see below)\n",
    "alpha=results.params['Intercept']\n",
    "beta=results.params['TV']\n",
    "ads['line of best fit']=alpha+beta*ads['TV']\n",
    "sns.lineplot(data=ads, x='TV', y='line of best fit',color='darkblue', ax=ax)\n",
    "\n",
    "# plot the residuals\n",
    "ads['residuals'] = ads['line of best fit'] - ads['Sales']\n",
    "for i, row in ads.iterrows():    \n",
    "    ax.vlines(row['TV'],row['Sales'],row['Sales']+row['residuals'], color='grey',alpha=.5,linestyle=\":\")\n",
    "    if i%10==0:\n",
    "        ax.text(x=row['TV']+1, y=row['line of best fit']-(row['residuals']/2), s=f'e$_{{{str(i)}}}$' )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f8ec0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Minimized?\n",
    "\n",
    "- Let's prove that the results from an ols regression indeed minize the RSS\n",
    "- To do so we will use the [Statsmodels API](\n",
    "https://www.statsmodels.org/stable/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40adfad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Regress Sales on a constant term and TV\n",
    "results = smf.ols('Sales ~ TV ', data=ads).fit()\n",
    "# Inspect the results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead40daa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ads['alpha_hat']=results.params['Intercept']\n",
    "ads['beta_hat']=results.params['TV']\n",
    "ads['Sales_hat']=ads['alpha_hat']+ ads['beta_hat']* ads['TV']\n",
    "ads['Residual']=ads['Sales']- ads['Sales_hat']\n",
    "ads['RSS']=np.square(ads['Sales']- ads['Sales_hat'])\n",
    "ads.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486e8fbb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"The RSS from this OLS regression is {ads['RSS'].sum().round(1)}\")\n",
    "print(\"The residual sums up to: {}! By construction! (see the first order condition below)\".format(round(np.abs(ads['Residual'].sum()),1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfdcf8d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- What happends to the RSS if we keep the intercept, $\\alpha$, constant and change $\\beta$?\n",
    "- In other words, can we find a $\\beta$ that improves over the current $\\hat{\\beta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94b1bec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# prepare list for a new dataframe\n",
    "betas=[beta for beta in np.arange(0.03, 0.065, 0.00025)] # all values of beta we will try\n",
    "#betas=[beta for beta in np.arange(0.03, 0.065, 0.005)] # all values of beta we will try\n",
    "RSSs=[] # Create empty list to store RSS\n",
    "# Change beta_hat by some small amounts\n",
    "\n",
    "for beta in betas: # loop over all the\n",
    "    ads['Sales_hat_2']=ads['alpha_hat']+ beta* ads['TV'] # predict sales given new beta\n",
    "    ads['RSS_2']=np.square(ads['Sales']- ads['Sales_hat_2']) # square the residual\n",
    "    RSSs.append(ads.RSS_2.sum()) # take the sum of the squared residuals and append to the RSSs list\n",
    "\n",
    "data={'beta':betas, 'RSS': RSSs} # Put in a dictionary\n",
    "df = pd.DataFrame(data=data) # Turn dictionary to dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccc83bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "beta=results.params['TV']\n",
    "fig, axes=plt.subplots(1,2, sharey=True, figsize=(15,9))\n",
    "sns.scatterplot(data=df, x=\"beta\", y=\"RSS\", ax=axes[0], color='darkblue')\n",
    "axes[0].set_xlabel(r\"$\\hat{\\beta}$\", fontsize=20, color='indigo')\n",
    "axes[0].set_ylabel(\"Residual Sum of Squares\", fontsize=16)\n",
    "axes[0].axvline(beta,color='darkorange')\n",
    "axes[0].text(x=0.036, y=3000, s=r\"y=RSS($\\hat{\\beta}}$)\", color='darkblue',fontsize=16)\n",
    "\n",
    "\n",
    "\n",
    "sns.scatterplot(data=df, x=\"beta\", y=\"RSS\", ax=axes[1], color='darkblue')\n",
    "axes[1].set_xlabel(r\"$\\hat{\\beta}$\", fontsize=20, color='indigo')\n",
    "axes[1].axhline(y=ads['RSS'].sum(),color='darkgreen', xmin=0.35, xmax=0.65)\n",
    "axes[1].text(x=beta, y=ads['RSS'].sum()-50, s=r\"RSS reaches a minimum for $\\hat{\\beta}}$\", ha=\"center\",color='darkgreen',fontsize=12)\n",
    "axes[1].text(x=0.036, y=3000, s=r\"y=RSS($\\hat{\\beta}}$)\", color='darkblue',fontsize=16)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91586822",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also visualize the way the RSS changes as we change both the intercept and the slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a24c72",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Regression coefficients (Ordinary Least Squares)\n",
    "regr = LinearRegression()\n",
    "\n",
    "X = scale(ads.TV, with_mean=True, with_std=False).reshape(-1,1) # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "y = ads.Sales\n",
    "\n",
    "regr.fit(X,y)\n",
    "print(\"\\nCompare with the results above:\")\n",
    "print(f\"Intercept for the centered data is: {regr.intercept_}.\")\n",
    "print(f\"The slope coefficient is: {regr.coef_[0]}.\")\n",
    "\n",
    "# Create grid coordinates for plotting\n",
    "B0 = np.linspace(regr.intercept_-2, regr.intercept_+2, 50)\n",
    "B1 = np.linspace(regr.coef_[0]-0.02, regr.coef_[0]+0.02, 50)\n",
    "\n",
    "xx, yy = np.meshgrid(B0, B1, indexing='xy')\n",
    "Z = np.zeros((B0.size,B1.size))\n",
    "\n",
    "# Calculate Z-values (RSS) based on grid of coefficients\n",
    "for (i,j),v in np.ndenumerate(Z):\n",
    "    Z[i,j] =((y - (xx[i,j]+X.ravel()*yy[i,j]))**2).sum()/1000\n",
    "\n",
    "# Minimized RSS\n",
    "min_RSS = r'$\\beta_0$, $\\beta_1$ for minimized RSS'\n",
    "min_rss = np.sum((regr.intercept_+regr.coef_*X - y.values.reshape(-1,1))**2)/100\n",
    "print(f\"The minimum residual sum of squares is {min_rss}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1393cf8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes =plt.subplots(1,2,figsize=(12,6), sharex=True, sharey=True)\n",
    "\n",
    "# contour plot\n",
    "CS=axes[0].contour(xx, yy, Z, cmap=plt.cm.Set1, levels=[2.15, 2.2, 2.3, 2.5, 3, 3.5])\n",
    "axes[0].plot(regr.intercept_, regr.coef_[0],  color='darkorange', marker='+', label=min_RSS)\n",
    "axes[0].clabel(CS, CS.levels, inline=True, fmt='%1.1f', fontsize=10)\n",
    "\n",
    "#heatmap\n",
    "CSS=axes[1].pcolormesh(xx, yy, Z, cmap='gnuplot2', vmin=np.abs(Z).min(), vmax=np.abs(Z).max(), shading='auto')\n",
    "\n",
    "\n",
    "\n",
    "# settings common to both plots\n",
    "axes[0].set_xlabel(r'$\\beta_0$', fontsize=17)\n",
    "axes[1].set_xlabel(r'$\\beta_0$', fontsize=17)\n",
    "axes[0].set_ylabel(r'$\\beta_1$', fontsize=17)\n",
    "axes[0].set_yticks([0.03,0.04,0.05,0.06])\n",
    "#     ax.legend()\n",
    "\n",
    "cbar=fig.colorbar(CSS, ax=axes[1])\n",
    "cbar.set_label('Residual Sum of Squares', rotation=270, labelpad=+15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74f6ac0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline  \n",
    "# %matplotlib notebook\n",
    "from matplotlib import cm\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,10), subplot_kw={'projection': '3d'})\n",
    "\n",
    "# Plot the 3D surface\n",
    "ax.plot_surface(xx, yy, Z, rstride=3, cstride=3, alpha=0.3)\n",
    "\n",
    "# Plot projections of the contours for each dimension.  By choosing offsets\n",
    "# that match the appropriate axes limits, the projected contours will sit on\n",
    "# the 'walls' of the graph\n",
    "cset = ax.contour(xx, yy, Z, zdir='z', offset=np.abs(Z).min(), cmap=cm.coolwarm)\n",
    "cset = ax.contour(xx, yy, Z, zdir='x', offset=np.abs(xx).min(), cmap=cm.coolwarm)\n",
    "cset = ax.contour(xx, yy, Z, zdir='y', offset=np.abs(yy).min(), cmap=cm.coolwarm)\n",
    "\n",
    "ax.set_xlabel(r'$\\beta_0$')\n",
    "ax.set_ylabel(r'$\\beta_1$')\n",
    "ax.set_zlabel(\"RSS\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646292f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aside: Optimization Methods\n",
    "\n",
    "- We want to choose $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ to minimize the following function: \n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{MSE} = \\large \\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\beta_0 - \\beta_1 x_i)^2\n",
    "\\end{equation*}\n",
    "\n",
    "We just used a method called a _grid search_\n",
    "1. Propose a large number of values for $\\beta_0$ and $\\beta_1$ that form a _grid_\n",
    "2. Calculate the value of MSE for each pair of $\\beta_0$ and $\\beta_1$\n",
    "3. Choose the point on the grid that is the lowest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01272c6b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notes: \n",
    "- Grid search will not find the _best_ solution in general\n",
    "- The more points in the grid, the closer you get to the true solution\n",
    "- Grid searches take a LONG time for computers to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef097ed1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aside: Optimization Methods\n",
    "\n",
    "_Grid Search_ is an example of a __Numerical Method__\n",
    "\n",
    "In Econometrics, we use calculus to derive the following experession: $\\hat{\\beta}_1 = \\frac{s_{XY}}{s_X^2}$\n",
    "\n",
    "In general, this will _not_ be possible in Machine Learning models: \n",
    "- They are too complicated! \n",
    "- No amount of calculus will allow us to derive a simple equation with coefficients on the left-hand side, and data on the right-hand side. \n",
    "\n",
    "SOLUTION: Numerical methods: \n",
    "- These are algorithms which allow the computer to \"guess\" and \"check\" the answer to a question, to eventually converge to the correct answer\n",
    "- In Machine Learning, the dominant numerical method is called _Gradiant Descent_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d50b14",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assessing the Accuracy of the Coefficient Estimates\n",
    "\n",
    "- Recall that we assume that the __true__ relationship between X and Y takes the form:\n",
    "$Y = f(\\mathbf{X}) + \\varepsilon$  \n",
    "    - for some unknown function $f$, where $ \\mathbb{E}[\\varepsilon ]=0$\n",
    "    - If $f$ is to be approximated by a linear function, then we can write this relationship as\n",
    "    \n",
    "$$Y = β_0 + β_1X + \\varepsilon$$\n",
    "\n",
    "- This is the __population regression line__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b37ba1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# From population to sample\n",
    "\n",
    "- Of course you rarely observe the entire population\n",
    "- Instead you get a __random sample__ to estimate your parameters\n",
    "- Say that the parameters for the population ression line corresponds to:\n",
    "$$ Y = \\beta_0 + \\beta_1 X + \\varepsilon = 2 + 3 X + \\varepsilon $$\n",
    "    - How close, $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ will be to the true parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac9e1e2",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Set a seed so our numbers are the same\n",
    "np.random.seed(1706)\n",
    "# Create random array of 1,000 observations\n",
    "obs=1000\n",
    "# Create data set\n",
    "data={'X_variable':np.random.randint(100, size=obs) , # X takes random integer values between 0 and 100\n",
    "      'normal_error':np.random.normal(loc=0.0,scale=300.0, size=obs) # Normal erro with mean zero and SD 300 \n",
    "     }\n",
    "df=pd.DataFrame(data=data)\n",
    "df['𝛽_0']=2 # true intercept value\n",
    "df['𝛽_1']=3 # true slope value\n",
    "df['Y_variable']= df['𝛽_0']+ df['𝛽_1']*df['X_variable'] + df['normal_error'] # y=f(x) for the entire population\n",
    "df=df[['Y_variable','𝛽_0','𝛽_1', 'X_variable','normal_error']] # reorder columns\n",
    "\n",
    "print(f\"The error term expectation is: {df['normal_error'].mean()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cb1e14",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "fig, axes = plt.subplots(1,2, figsize=(20,15), sharey=True)\n",
    "\n",
    "# Fit the population line\n",
    "sns.regplot(data=df, x='X_variable', y='Y_variable', order=1, ci=None, ax=axes[0],scatter_kws={\"color\": \"black\", \"alpha\":.1},line_kws={\"color\": \"red\"})\n",
    "axes[0].set_ylabel(\"Y Variable\", fontsize=20)\n",
    "axes[0].set_xlabel(\"X Variable\", fontsize=20)\n",
    "axes[0].set_title(\"Population Regression Line\", fontsize=20)\n",
    "\n",
    "\n",
    "for i, v in enumerate(range(5)):\n",
    "    colors=['darkgreen','orange','indigo','lime','rosybrown']\n",
    "    sns.regplot(data=df.sample(n=300, random_state=(v+17)**2), x='X_variable', y='Y_variable',order=1, ci=None, ax=axes[1],scatter_kws={\"color\": colors[i], \"alpha\":.05},line_kws={\"color\": colors[i], 'alpha':.5})\n",
    "\n",
    "axes[1].set_ylabel(\"\", fontsize=20)\n",
    "axes[1].set_xlabel(\"X Variable\", fontsize=20)    \n",
    "axes[1].set_title(\"Sample Estimates\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f56ff9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Error term and Uncertainty\n",
    "\n",
    "- You can see from the population regression line that there is a lot of variation around the line of best fit\n",
    "- With random sampling we can seen that the intercepts and slopes are close to the values of the population regression but not always the same\n",
    "- This depends on the sampling distribution of $\\beta$-OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6164c4ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# About unbiasnessness and consistency\n",
    "- What does the distribution of $\\hat{\\beta}_1$ looks like?\n",
    "- Let's take 500 samples of 150 observations, regress y on x each time and plot the distribution of $\\hat{\\beta}_1$\n",
    "\n",
    "- Let's take 500 samples of 600 observations, regress y on x each time and plot the distribution of $\\hat{\\beta}_1$\n",
    "\n",
    "- If $\\hat{\\beta}_1$ is __unbiased__ then the distribution should be centered around the true $\\beta_1$\n",
    "- If $\\hat{\\beta}_1$ is __consistent__ then the distribution narrow around the true $\\beta_1$ as the number of observations increases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1704d711",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "num_samples=500 # Number of samples we will take\n",
    "small_sample_obs=150 # Number of observations when taking small samples\n",
    "large_sample_obs=600 # Number of observations when taking large samples\n",
    "# Create numpy arrays of zeros with the num_samples dimension, we will then store estimates separately in them\n",
    "small_sample=np.zeros(num_samples)\n",
    "large_sample=np.zeros(num_samples)\n",
    "# Run regressions for each sample\n",
    "for i, v in enumerate(range(num_samples)):\n",
    "    # small sample\n",
    "    df_small=df.sample(n=small_sample_obs, random_state=v).copy() # small sample from the population\n",
    "    results = smf.ols('Y_variable ~ X_variable ', data=df_small).fit() # Regress Sales on a constant term and TV\n",
    "    small_sample[i]=results.params['X_variable'] # store the result in the numpy array at position i\n",
    "\n",
    "    # large sample\n",
    "    df_large=df.sample(n=large_sample_obs, random_state=v).copy() # large sample from population\n",
    "    results = smf.ols('Y_variable ~ X_variable ', data=df_large).fit() # Regress Sales on a constant term and TV\n",
    "    large_sample[i]=results.params['X_variable'] # store the result in the numpy array at position i\n",
    "    \n",
    "# save in a dataframe    \n",
    "par=pd.DataFrame({'Small Sample':small_sample, 'Large Sample':large_sample})\n",
    "par.head()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e78d99",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot kernel density of the estimates around the true population parameter\n",
    "fig, axes=plt.subplots(1,2, sharey=True, sharex=True, figsize=(20,15))\n",
    "\n",
    "sns.kdeplot(data=par, x=\"Small Sample\", ax=axes[0], color=\"darkgreen\") #kdensity of small sample\n",
    "sns.kdeplot(data=par, x=\"Large Sample\", ax=axes[1], color=\"darkgreen\") #kdensity of large sample\n",
    "\n",
    "axes[0].axvline(3,color='darkorange') # Line for true Beta\n",
    "axes[1].axvline(3,color='darkorange') # Line for true Beta\n",
    "axes[0].text(4,1,r'Population $\\beta$',fontsize=20, color='darkorange')\n",
    "axes[1].text(4,1,r'Population $\\beta$',fontsize=20, color='darkorange')\n",
    "\n",
    "# Label axes\n",
    "axes[0].set_ylabel(r\"Kernel Density of $\\hat{\\beta}$\", fontsize=20, color=\"darkgreen\")\n",
    "axes[0].set_xlabel(r\"$\\hat{\\beta}$ - Small Sample\", fontsize=20)\n",
    "axes[1].set_xlabel(r\"$\\hat{\\beta}$ - Large Sample\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287839b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# OLS estimator continued\n",
    "- So the OLS estimator is an unbiased and consistent estimator\n",
    "- It follows from these properties that one can test for parameters significance\n",
    "- Using in particular the fact that $\\hat{\\beta}_1$ is normally distributed\n",
    "- This is the __hypothesis testing__ process:\n",
    "    - H0: There is no effect and we picked $\\hat{\\beta}_1$ from a normal distribution centered at zero\n",
    "    - H1: It is unlikely we picked a value $\\hat{\\beta}_1$ from a normal distribution centered at zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c28e50e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Standard Errors\n",
    "\n",
    "- You may remember from econometrics that you are not only interested in the magnitude of the parameter estimate\n",
    "\n",
    "- You also care about the distribution of $\\hat{\\beta}_1$, $\\sigma_{\\hat{\\beta}_1}^2$\n",
    "\n",
    "- You can't observe $\\sigma_{\\hat{\\beta}_1}^2$ (a population parameter) but you can estimate __Standard Errors__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73580921",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at our last estimation from our last exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776f75c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e31be1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "According to the Central Limit Theorem, $\\hat{\\beta_1}$ follows a normal distribution with mean $\\beta_1$ and with some standard error\n",
    "\n",
    "$H_0$: $\\beta_1 = 0$\n",
    "- Let's also pretend that standard error we got is correct for $\\hat{\\beta_1}$\n",
    "- Let's pretend that $H_0$ is true. \n",
    "\n",
    "Based on these assumptions: \n",
    "- We know that $\\hat{\\beta_1}$ is going to have the properties of a normal distribution (CENTRAL LIMIT THEOREM, LAW OF LARGE NUMBERS)\n",
    "\n",
    "\n",
    "Under these assumptions, $\\hat{\\beta_1}$ is going to follow a _normal distribution_ with mean 0 and standard error of .425\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let's create a normal distribution, centered at zero and with standard deviation from this estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632c46a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mu = 0 # center at zer0\n",
    "sigma = results.bse['X_variable'] # Standard Deviation from SEs\n",
    "print(sigma)\n",
    "x = np.linspace(mu - 5*sigma, mu + 5*sigma, 100) # range is 5 standard deviations away from 0, to the left and to the right\n",
    "plt.plot(x, stats.norm.pdf(x, mu, sigma))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de98bbcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How far is our $\\hat{\\beta}_1$ from zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25781dab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,10))\n",
    "\n",
    "ax.plot(x, stats.norm.pdf(x, mu, sigma)) # Plot the normal distribution centered at zero and sigma=SE\n",
    "ax.axvline(round(results.params['X_variable'],2),color='darkorange') # Line for true Beta\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e749eb59",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# t-statistic\n",
    "\n",
    "- How likely are we to observe a value for $\\hat{\\beta}_1=2.4517$ if the true value is zero and the standard deviation is equal to the standard errors?\n",
    "    - Very unlikely\n",
    "- This is also what you get from the t-statistic\n",
    "$$t = \\frac{\\hat{\\beta}_1 - \\beta_{1,0}}{SE(\\hat{\\beta}_1)} = 5.766 $$\n",
    "\n",
    "- The result from this estimation suggests we are 5.8 standard deviations away from zero... this is far!\n",
    "- We can conclude that $\\hat{\\beta}_1 \\neq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5400cd7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assessing the Accuracy of the Model\n",
    "- Hypothesis testing tells us whether we should trust the parameters are significant\n",
    "- This is different from __model accuracy__\n",
    "    - Model accuracy tells us the extent to which the entire model fits the data\n",
    "    - This is linked to parameter accuracy using $R^2$\n",
    "    - Of course because we are looking at the simple linear regression model this is a small difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcc4791",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $R^2$ Statistic\n",
    "\n",
    "- $R^2$ statistic provides an alternative measure of fit. \n",
    "- It takes the form of a proportion\n",
    "    - the proportion of variance explained\n",
    "    - and so it always takes on a value between 0 and 1 \n",
    "    - and is independent of the scale of Y.\n",
    "- The formula is given by:\n",
    "\\begin{equation*}\n",
    "R^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS}\n",
    "\\end{equation*}\n",
    "\n",
    "with TSS = $\\sum (y_i − \\bar{y})^2$, the __total sum of squares__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0e7440",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $R^2$ Statistic continued\n",
    "\n",
    "- TSS measures the total variance in the response Y , \n",
    "    - it can be  thought of as the amount of variability inherent in the response before the regression is performed \n",
    "\n",
    "- In contrast, RSS measures the amount of variability that is left unexplained after performing the regression. \n",
    "    - TSS−RSS measures the amount of variability in the response that is explained by performing the regression\n",
    "    \n",
    "- __R2 measures the proportion of variability in Y that can be explained using X__. \n",
    "    - close to 1 indicates that a large proportion of the variability in the response has been explained by the regression. \n",
    "    - close to 0 indicates that the regression did not explain much of the variability in the response\n",
    "        - this might occur because the linear model is wrong, or the inherent error is high, or both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0afbb7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multiple Linear Regression\n",
    "- Of course there is no reason to limit the number of inputs to a single variable\n",
    "- When adding variables we allow for each predictor to have a separate slope coefficient in a single model.\n",
    "\n",
    "$$Y = β_0 + β_1X_1 + β_2X_2 + · · · + β_pX_p + \\varepsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67883e88",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "display(ads.head())\n",
    "\n",
    "results = smf.ols('Sales ~ TV + Radio + Newspaper ', data=ads).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0299578a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Get estimates\n",
    "regr = LinearRegression()\n",
    "X = ads[['Radio', 'TV']]\n",
    "y = ads.Sales\n",
    "regr.fit(X,y)\n",
    "print(regr.coef_)\n",
    "print(regr.intercept_)\n",
    "\n",
    "# What are the min/max values of Radio & TV?\n",
    "# Use these values to set up the grid for plotting.\n",
    "display(ads[['Radio', 'TV']].describe())\n",
    "\n",
    "# Create a coordinate grid\n",
    "Radio = np.arange(0,50)\n",
    "TV = np.arange(0,300)\n",
    "\n",
    "B1, B2 = np.meshgrid(Radio, TV, indexing='xy')\n",
    "Z = np.zeros((TV.size, Radio.size))\n",
    "\n",
    "for (i,j),v in np.ndenumerate(Z):\n",
    "        Z[i,j] =(regr.intercept_ + B1[i,j]*regr.coef_[0] + B2[i,j]*regr.coef_[1])\n",
    "\n",
    "# Create plot\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "fig.suptitle('Regression: Sales ~ Radio + TV Advertising', fontsize=20)\n",
    "\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.plot_surface(B1, B2, Z, rstride=10, cstride=5, alpha=0.4)\n",
    "ax.scatter3D(ads.Radio, ads.TV, ads.Sales, c='r')\n",
    "\n",
    "ax.set_xlabel('Radio')\n",
    "ax.set_xlim(0,50)\n",
    "ax.set_ylabel('TV')\n",
    "ax.set_ylim(ymin=0)\n",
    "ax.set_zlabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107ad26e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Is There a Relationship Between the Response and Predictors?\n",
    "\n",
    "- In the multiple regression setting with __p-predictors__, we need to ask whether __all of the regression coefficients are zero__\n",
    "    - i.e. whether $β_1 = β_2 = · · · = β_p = 0$. \n",
    "    \n",
    "Formula $$ F = \\frac{(TSS − RSS)/p}{RSS/(n − p − 1)} $$\n",
    "\n",
    "- F-statistic: 570.3, reject the null: at least one parameter is significant (we already knew that)\n",
    "    - Rule of thumbs: f-stat should be greater than 1\n",
    "    - Much greater than 1 if n is large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4ae6d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to choose what variables to put in your model? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5942277d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Economists: \n",
    "\n",
    "<div align=\"center\"> <img src=\"thinking-man-statue.JPG\", width=750 /> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504928fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to choose what variables to put in your model? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026f8bb4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Machine learning: \n",
    "    \n",
    "<div align=\"center\"> <img src=\"computer-algorithm.JPG\", width=750 /> </div>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ff4dca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Different strategies to identify variables that should go in the model:\n",
    "\n",
    "1. (Economist way): Add variables that make sense based on your economic model\n",
    "2. (Machine learning): Add variables if they reduce the \"test\" sample mean squared error. \n",
    "\n",
    "Some other strategies that some people use:\n",
    "1. Include variables in the model if they are _statistically significant_ (they are different from zero)\n",
    "2. Add variables if they increase the $r^2$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275af42e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Question #5: In the Advertising exercise, what variables would you put in a model if you want to predict sales? Note: There is no \"best\" answer to this question, but please write 1-2 sentences to explain your answer. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
