{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXuirKXSbi4Q",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <div align=\"center\"> SPECIAL TOPICS III </div>\n",
    "## <div align=\"center\"> Data Science for Social Scientists  </div>\n",
    "### <div align=\"center\"> ECO 4199 </div>\n",
    "#### <div align=\"center\">Class 11 - Deep Learning</div>\n",
    "<div align=\"center\"> Jonathan Holmes, (he/him)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today's class: \n",
    "\n",
    "### Part #1: Lecture on Deep Learning\n",
    "\n",
    "### Part #2: Hands-on deep learning using tensorflow\n",
    "\n",
    "Using the following guide: https://www.tensorflow.org/tutorials/keras/regression\n",
    "- You can run directly in Google Colab (see link)\n",
    "- To use on a local machine: %conda install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tfm1GRSRbi4R",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning\n",
    "- Deep learning (DL) is the technique behind most of artificial intelligence innovation in recent years\n",
    "    - self-driving cars\n",
    "    - image recognition\n",
    "    - deep fakes etc.\n",
    "\n",
    "- Deep Learning and Artificial Neural Networks are synonyms in this class\n",
    "\n",
    "\n",
    "Additional resources: \n",
    "- The fairly technical book [_Deep Learning_](https://www.deeplearningbook.org/)  by [Ian Goodfellow](https://en.wikipedia.org/wiki/Ian_Goodfellow) and the terrific [Kaggle tutorial](https://www.kaggle.com/learn/intro-to-deep-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How does a human brain understand numbers? \n",
    "\n",
    "<div align=\"center\"> <img src=\"numbers.png\" width=600 height=600 /> </div>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Eyes detect light, dark, and colors (Eyes)\n",
    "2. Visual processing sensory input into shapes (Visual Cortex)\n",
    "3. Brain decodes shapes into numbers (Visual Association Cortex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## From human brains to computers\n",
    "\n",
    "Why don't we let computers do that? \n",
    "\n",
    "1. Start with data inputs\n",
    "2. Combine the data into concepts\n",
    "3. Combine the concepts into a predictive answer\n",
    "\n",
    "#### Notes: \n",
    "- The computer gets to define its own concepts\n",
    "- Just like humans, it's very hard to determine why a deep learning model got the answer it did\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjzGOOYobi4S",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What's deep in DL?\n",
    "$$y=f(X) + \\varepsilon$$\n",
    "\n",
    "\n",
    "- Deep learning includes multiple nested functions:\n",
    "$$f^\\ast(x)= f^{(3)}(f^{(2)}(f^{(1)}(x)))$$\n",
    "\n",
    "- These chain structures are the most commonly used structures of __neural networks__. \n",
    "    - f(1) is called the first __layer__ of the network\n",
    "    - f (2) is called the second layer, and so on\n",
    "    \n",
    "- Layers are like levels of abstraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A single layer neural network\n",
    "\n",
    "<div align=\"center\"> <img src=\"neural_network1.png\" width=500 height=500 /> </div>\n",
    "\n",
    "1. Input layer is data\n",
    "2. Hidden layer is functions of the data\n",
    "3. Output layer is the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "# What's a neuron in neural network?\n",
    "\n",
    "<div align=\"center\"> <img src=\"fig1_unit.png\" width=500 height=500 /> </div>\n",
    "\n",
    "- This is an artificial __neuron__\n",
    "    - Synonyms: __node__, __unit__\n",
    "\n",
    "- Neurons represent functions: \n",
    "    - Inputs X and a constant\n",
    "    - Output Y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uu7JietRbi4S",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear regression and Linear unit\n",
    "- Example neuron funtion: The Linear Unit/Linear Regression\n",
    "    -  y = w x + b \n",
    "- This is the equation of a line:\n",
    "     - w is a __weight__ (Econometrics: the slope $\\beta$)\n",
    "     - b is a __bias__ (Econometrics: the intercept $\\alpha$)\n",
    "- Too many definitions for __bias__!\n",
    "    - Econometrics: When $E[\\hat{\\beta}] \\neq \\beta$ \n",
    "    - Statistical learning: When the model $\\hat{f}(x)$ is not flexible enough to fit the real $f(x)$\n",
    "    - Neural networks: Bias = Coefficient on constant term (Intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpZNz1wPtQvo",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple Inputs \n",
    "\n",
    "- You can simply add more input __connections__ to the neuron, one for each additional feature. \n",
    "  - $y = w_0 x_0 + w_1 x_1 + w_2 x_2 + b$\n",
    "  \n",
    "- This equation has _3_ weights and _1_ bias term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other functions\n",
    "\n",
    "Generally, neurons are non-linear functions: \n",
    "$$ y = h(x) = g(w_0x_0 + w_1x_1 + ... + w_Nx_N + b) $$ \n",
    "\n",
    "Examples of $g$: \n",
    "- Sigmoid function: $g(z) = \\frac{e^z}{1+e^z}$\n",
    "- Activation function $g(z) = \\cases{0 \\text{ if } z < 0 \\\\ z \\text{ otherwise}}$\n",
    "    - Sometimes called a **rectified linear unit** or **ReLU**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What's going on? \n",
    "\n",
    "\n",
    "<div align=\"center\"> <img src=\"neural_network1.png\" width=600 height=600 /> </div>\n",
    "\n",
    "- Each neuron in an intermediate layer is an equation\n",
    "- $A1 = h_1(X)$, ..., $A5 = h_5(X)$\n",
    "- Output $Y = f(A1, A2, ..., A5)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In-Class Exercise\n",
    "\n",
    "Q1: In the above neural network, how many features are there? How many neurons? \n",
    "There are 4 features (X1 ... X4) and 5 neurons (A1...A5)\n",
    "\n",
    "Q2: In the above neural network: \n",
    "- How many parameters do you estimate for each neuron? How many are weights, and how many bias terms?\n",
    "A1: $h_1(X) = g(w^1_1X_1 + w^1_2X_2 + w^1_3X_3 + w^1_4X_4 + b)$ 4 weights w, 1 bias term. \n",
    "\n",
    "- How many parameters do you estimate for the output layer?\n",
    "$$f(X) = \\beta_0 + \\beta_1 A_1 + \\beta_2 A_2 + \\beta_3 A_3 + \\beta_4 A_4 + \\beta_5 A_5$$\n",
    "$$f(X) = \\beta_0 + \\beta_1 h_1(x) + \\beta_2  h_2(x) + \\beta_3  h_3(x) + \\beta_4  h_4(x) + \\beta_5  h_5(x)$$\n",
    "6 more parameters in the output layer\n",
    "\n",
    "- How many total parameters do you estimate in this model? \n",
    "    - 5 parameters per neuron * 5 neurons = 25\n",
    "    - plus 6 betas in the output layer \n",
    "    - Total: 31\n",
    "\n",
    "\n",
    "Q3: Why do you think we usually use non-linear functions for the neurons instead of linear functions?  \n",
    "- Allows for more complex relationships\n",
    "- Linear functions don't actually work! (Linear algebra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural networks and human brains\n",
    "\n",
    "Similar terminology about the human brain: \n",
    "- If a neuron $A_k$ is close to 0, it is __silent__\n",
    "- If a neuron $A_k$ is close to 1 (or is large), it is __firing__ \n",
    "\n",
    "Also: \n",
    "- Intermediate neurons are part of __hidden layers__\n",
    "- We will NOT be able to understand what the weights (w), bias terms (b), or even the neurons (A1,...) mean\n",
    "- Evaluate neural networks based on whether they generate good predictions, $\\hat{y}$. If you want to understand the mechanisms ($\\hat{\\beta}$), use econometrics!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training a deep neural network\n",
    "\n",
    "Coder decides: \n",
    "- Number of hidden layers\n",
    "- Number of neurons\n",
    "- What function (sigmoid, activation, etc) to use for each node. \n",
    "\n",
    "Then: \n",
    "1. Computer assigns random values of all weights and biases ($w$, $b$)\n",
    "2. Repeat many times: \n",
    "    1. Calculate model error based on the current $w$, $b$ \n",
    "    2. For each $w$ and $b$, use calculus to identify the \"next guess\"\n",
    "    3. Update $w$ and $b$ to the new guess "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Terminology\n",
    "\n",
    "- _Backpropogation_: The algorithm based on calculus, which computes the best next step (the _gradiant_, aka derivative)\n",
    "- _Gradiant descent_: An algorithm which progressivly gets better guesses using the gradiant as a guide\n",
    "- _Epoch_: Each time you run the loop, it's a new epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advantages of neural networks\n",
    "\n",
    "In previous classes, we made linear models more flexible by adding: \n",
    "- Polynomial terms (eg: $X_1^2$, $X_1^3$)\n",
    "- Interaction terms (eg: $X_1X^2$, $X_1^2X_2$)\n",
    "\n",
    "Neural networks can be _far_ more flexible\n",
    "- The algorithm decides how to combine/interact variables\n",
    "- It's possible to have discontinuous, non-linear, interacting, complex relationships\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In-class exercise\n",
    "\n",
    "\"unbiased\" => Econometrics term\n",
    "ML: Reducing bias \n",
    "Q4: I have a dataset and a prediction problem, and I'm deciding whether to use OLS, Lasso, or a neural network. How can I decide which method is best? \n",
    "- OLS/lasso: Higher bias, lower variance \n",
    "- Complex interactions: Neural network\n",
    "- Just try them, use our tests! (Cross validation, Validation set, AIC, BIC, etc). \n",
    "\n",
    "Q5: Lets say I wanted to answer the same question as Q4, but with a dataset that is 100 times larger than before. Which method is likely to be better? \n",
    "- Neural network gets better\n",
    "- As your data gets bigger, concerns about \"variance\" and \"overfitting\" are less important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple Layered Neural Networks\n",
    "\n",
    "<div align=\"center\"> <img src=\"neural_network2.png\" width=550 height=550 /> </div>\n",
    "\n",
    "- Input layer is input to hidden L1\n",
    "- Hidden L1 is input to hidden L2\n",
    "- Output is calculated as a function of L2\n",
    "- In this case, there are many outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Layers #\n",
    "\n",
    "- Neurons are organized in **layers** that are inter-connected\n",
    "  - We talk about  __dense__ layers when linear units have in common the same set of inputs\n",
    "\n",
    "- It turns out that combining dense layers is not enough to obtain the _flexibility_ that makes neural networks so powerful\n",
    "  - You could think of each layer in a neural network as performing some kind of relatively simple transformation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decisions to make to design a neural network\n",
    "\n",
    "1. How many layers? \n",
    "2. How many neurons per layer? \n",
    "3. What function $h(X)$ do you use in each layer? \n",
    "\n",
    "In-class exercise: \n",
    "\n",
    "Q6. If you increase the number of layers, this will most likely __ bias and __ variance of the model \n",
    "DECREASE BIAS, INCREASE VARIANCE \n",
    "\n",
    "Q7. If you increase the number of neurons, this will most likely __ bias and __ variance of the model \n",
    "DECREASE BIAS, INCREASE VARIANCE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Advances in Neural Networks\n",
    "\n",
    "### Part 1: Better, faster, more efficient estimation algorithms \n",
    "- Example: Stochastic gradiant descent\n",
    "- Goal: Faster convergence, less memory usage, ability to use \"distributed\" computing, using GPUs, etc. \n",
    "\n",
    "\n",
    "### Part 2: Creative intermediate layers (using different functions h(X))\n",
    "- ChatGPT is a _transformer_ model, known for its _attention_ layers\n",
    "- Image recognition uses _convolutional_ layers\n",
    "\n",
    "\n",
    "### Part 3: Throwing a lot more data at the problem\n",
    "- GPT3 has 175 billion parameters, 96 \"blocks\" of layers layers (each containing multiple layers)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "class 10 - Deep Learning .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
