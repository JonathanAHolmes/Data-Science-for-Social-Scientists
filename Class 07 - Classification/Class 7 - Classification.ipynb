{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19faebfe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <div align=\"center\"> SPECIAL TOPICS III </div>\n",
    "## <div align=\"center\"> Data Science for Social Scientists  </div>\n",
    "### <div align=\"center\"> ECO 4199 </div>\n",
    "#### <div align=\"center\">Class 7 - Classification</div>\n",
    "<div align=\"center\"> Jonathan Holmes, (he/him)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9492808b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mixing quantitative and qualitative\n",
    "\n",
    "$$ Y = f(\\mathbf{X}) + \\varepsilon$$\n",
    "\n",
    "- Last class we saw only instances of quantitative variables, both for our __predictors__ X and the __target__ Y\n",
    "- In the multiple regression model, we assumed that TV and Radio ads have an __additive effect__\n",
    "    - Remember from econometrics that this gives you the effect of TV on Sales, __holding Radio constant__\n",
    "    - In other words, the effect of TV ads is assumed to be the same, regarless of how much you spend on Radio ads\n",
    "- Today, we will explore new models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de18742",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# New data set XXCHANGE MEXX \n",
    "- Remember that we are still using the book [Introduction to Statistical Learning](http://faculty.marshall.usc.edu/gareth-james/)\n",
    "- I will sometimes use code is coming from this [set of scripts](https://github.com/JWarmenhoven/ISLR-python)\n",
    "- We will also use the <span style=\"color:orange;\">Credit dataset</span> from the R package ISLR or on the [book's webpage](https://www.statlearning.com/resources-first-edition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0ac613",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import sklearn.linear_model as skl_lm\n",
    "\n",
    "import math\n",
    "\n",
    "folderPath=\"~/Dropbox/_teaching/ECO4199/2023/Data-Science-for-Social-Scientists/Class 07 - Classification/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39abc2af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "credit=pd.read_csv('/'.join([folderPath, 'Credit.csv']))\n",
    "display()\n",
    "display(credit.info())\n",
    "display(credit.describe())\n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aef4323",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's inspect this dataset visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25157c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(credit[['Education','Limit','Rating']],diag_kind=\"kde\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b7b832",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(credit[['Age', 'Cards','Education','Income','Limit','Rating','Balance']],diag_kind=\"kde\")\n",
    "txt=\"\\nEach panel of the figure above is a scatterplot for a pair of variables whose identities are given by the corresponding row and column labels.\\nFor example, the scatterplot directly to the right of the $\\it{Balance}$ histogram depicts balance versus age, \\nwhile the plot directly to the right of $\\it{Age}$ corresponds to age versus cards.\"\n",
    "plt.figtext(0.5, -0.03, txt, wrap=True, horizontalalignment='center', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649d974c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The statistical task at hand\n",
    "Our credit dataset records:\n",
    "- _balance_ (average credit card debt for a number of individuals) \n",
    "- __quantitative predictors__: \n",
    "    - age, cards (number of credit cards), education(years of education), income (in thousands of dollars), limit(credit limit), and rating (credit rating). \n",
    "-  __qualitative variables__: \n",
    "    - gender, student (student status), status (marital status), and ethnicity (Caucasian, African American or Asian)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efea0abd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dummy variables\n",
    "\n",
    "- Suppose we are interested in knowing how credit card balance differ between males and females, ignoring the other variables for the moment. \n",
    "- A qualitative predictor (__factor__) with only 2 levels can be easily integrated to a regression\n",
    "    - Create an indicator or __dummy variable__ that takes on two possible numerical values. For example, based on the student variable, we can create variable\n",
    "$$\n",
    "x_i=\n",
    "\\begin{cases}\n",
    "1,\\ \\  \\text{if the ith person is a student}\\\\\n",
    "0,\\ \\  \\text{if the ith person is not a student}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Which yields the following population model:\n",
    "$$\n",
    "y_i= \\beta_0 + \\beta_1 x_i + \\varepsilon_i =\n",
    "\\begin{cases}\n",
    "\\beta_0 + \\beta_1 + \\varepsilon_i ,\\ \\  \\text{if the ith person is a student}\\\\\n",
    "\\beta_0 + \\varepsilon_i ,\\ \\  \\text{if the ith person is not a student}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a431409d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Replace Student by a dummy variable\n",
    "print(\"Before\")\n",
    "display(credit.head())\n",
    "credit['Student']=pd.to_numeric(credit.Student.replace({\"No\":0, \"Yes\":1})).astype(np.int8) # encode\n",
    "print(\"After\")\n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ed4b3a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Regress Balance on Student Status\n",
    "results = smf.ols('Balance ~ Student ', data=credit).fit()\n",
    "# Inspect the results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c9d330",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpreting dummy variables\n",
    "\n",
    "- $\\hat{β}_0$ (\\$480) is average credit card balance among males\n",
    "- $\\hat{β}_0 + \\hat{β}_1$ (\\$876) is the average credit card balance among students, and \n",
    "- $\\hat{β}_1$ (\\$396) is the difference, on average, in credit card balance between students and non-students."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1458ee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Qualitative Predictors with More than Two Levels\n",
    "\n",
    "- If you have more than 2 categories\n",
    "- For example, for the ethnicity variable we can create two dummy variables:\n",
    "\n",
    "$$\n",
    "x^S_{i}=\n",
    "\\begin{cases}\n",
    "1,\\ \\  \\text{if the ith person is from the South region}\\\\\n",
    "0,\\ \\  \\text{if the ith person is not from the South region}\n",
    "\\end{cases}\n",
    "$$\n",
    "and define $x^E_i$ (East), $x^W_i$ (West) similarly (note: There is no North)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0489fe8f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$y_i = β_0+β_1x^S_{i}+β_2x^W_{i}+\\varepsilon_i =\n",
    "\\begin{cases}\n",
    "β_0+β_1+\\varepsilon_i\\ \\ \\ , \\text{if the ith person is from the South}\\\\\n",
    "β_0+β_2+\\varepsilon_i\\ \\ \\ , \\text{if the ith person is from the West} \\\\\n",
    "β_0+\\varepsilon_i\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ , \\text{if the ith person is from the East} \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- $β_0$ is the average credit card balance for East \n",
    "- $β_1$ is the difference in the average balance between South and East\n",
    "- $β_2$ is the difference in the average balance between West and East\n",
    "\n",
    "\n",
    "- To avoid __multicolinearity__ there should be an omitted category. \n",
    "- The level with no dummy variable in this example is also known as the __leave-out group__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b26e2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Regress Sales on a constant term and Ethnicity -- THE EASY WAY\n",
    "results = smf.ols('Balance ~ Region ', data=credit).fit() # smf understands to use Ethnicity as a categorical variable and drops the first category as baseline\n",
    "# Inspect the results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e291cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Regress Sales on a constant term and Region -- THE EXPLICIT WAY\n",
    "dummies=pd.get_dummies(credit['Region'], drop_first=True) # create dummies and drop the omitted category\n",
    "display(dummies.head())\n",
    "df=credit.join(dummies)\n",
    "\n",
    "results = smf.ols('Balance ~ South + West ', data=df).fit() # pass the dummies explicitly\n",
    "# Inspect the results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549da094",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Regress Sales on a constant term and Ethnicity -- THE MULTICOLINEARITY AWARE WAY\n",
    "dummies=pd.get_dummies(credit['Region'], drop_first=False) # create dummies with no omitted category\n",
    "dummies.columns=dummies.columns.str.replace(\" \",\"\")\n",
    "display(dummies.head())\n",
    "df=credit.join(dummies)\n",
    "\n",
    "results = smf.ols('Balance ~ South + West + East -1', data=df).fit() # -1 indicates to drop the intercept\n",
    "# Inspect the results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a73a353",
   "metadata": {},
   "source": [
    "# In-Class Exercise: \n",
    "\n",
    "Suppose I am a business owner, and I want to predict seasonal sales. Let: \n",
    "- $Y$ = total sales\n",
    "- $X_1$ = A dummy variable for _fall_\n",
    "- $X_2$ = A dummy variable for _winter_\n",
    "- $X_3$ = A dummy variable for _spring_\n",
    "- $X_4$ = A dummy variable for _summer_\n",
    "\n",
    "Using data on my historical sales, I estimate the following model: \n",
    "\\begin{equation}\n",
    "    Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + u_i\n",
    "\\end{equation}\n",
    "\n",
    "I estimate the following coefficients: $\\hat{\\beta}_0 = 100$, $\\hat{\\beta}_1 = 10$, $\\hat{\\beta}_2 = -10$, and $\\hat{\\beta}_3 = 50$. \n",
    "\n",
    "Question #1: What were my average sales in fall, winter, spring, and summer? \n",
    "\n",
    "Question #2: Suppose I estimated a different model: \n",
    "\\begin{equation}\n",
    "    Y = \\alpha_0 + \\alpha_2 X_2 + \\alpha_3 X_3 + \\alpha_4 X_4 + u_i\n",
    "\\end{equation}\n",
    "\n",
    "Is it possible to predict what estimates I would get for $\\hat{\\alpha}_0$, $\\hat{\\alpha}_2$, $\\hat{\\alpha}_3$, and $\\hat{\\alpha}_4$? If yes, what are they?   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bd7f73",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Removing the Additive Assumption\n",
    "\n",
    "- Last week we used the <span style=\"color:orange;\">Advertising</span> dataset\n",
    "- In the multiple regression model we saw that both TV and radio were associated with sales. \n",
    "- We also assumed that TV and Radio effects were indepent from one another\n",
    "    - Using the old terminology, we captured the effect of TV ads on Sales keeping radio constant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e186928",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interaction effect\n",
    "\n",
    "- Suppose that spending money on radio ads actually increases the effectiveness of TV ads\n",
    "    - This means that slope for TV should increase as radio spending increases.\n",
    "- If true, for a given budget, spending half on radio and half on TV may increase sales more than allocating the entire amount to either TV or to radio. \n",
    "- This is an interaction effect "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02553e90",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interaction term\n",
    "- Instead of running:\n",
    "$$Y_i = β_0 + β_1 \\text{TV}_i + β_2 \\text{Radio}_i+ \\varepsilon_i$$\n",
    "- We could interact TV and radio using an __interaction term__:\n",
    "$$Y_i = β_0 + β_1 \\text{TV}_i + β_2 \\text{Radio}_i+ β_3 \\text{Radio}_i\\times\\text{TV}_i+ \\varepsilon_i$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766e2009",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interaction term - continued\n",
    "- Note that we can rewrite the last equation as:\n",
    "$$Y_i = β_0 + \\underbrace{(β_1 + β_3 \\text{Radio}_i)}_{\\tilde{β_1}} \\text{TV}_i + β_2 \\text{Radio}_i+ \\varepsilon_i$$\n",
    "\n",
    "\n",
    "$$Y_i = β_0 + \\tilde{β_1} \\text{TV}_i + β_2 \\text{Radio}_i+ \\varepsilon_i$$\n",
    "\n",
    "- Since $\\tilde{β_1}$ changes with Radio, the effect of TV on Y is\n",
    "no longer constant \n",
    "    - adjusting Radio will change the impact of TV on Y\n",
    "    - $β_3$ is the change in the effect of TV ads on Sales when Radio increases by one\n",
    "    - $β_1$ is now the effect of TV ads on Sales when Radio ads equal zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddbde0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ads=pd.read_csv('/'.join([folderPath, 'Advertising.csv']), usecols=[1,2,3,4])\n",
    "\n",
    "print(\"***Additive Model***\")\n",
    "results = smf.ols('Sales ~ TV + Radio ', data=ads).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16517be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"***Model with Interaction Term***\")\n",
    "results = smf.ols('Sales ~ TV + Radio + TV*Radio', data=ads).fit() \n",
    "# Inspect the results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef154f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretation of the results\n",
    "-  $\\hat{β_1} \\text{ and } \\hat{β_2}$ give what's known as the __main effect__\n",
    "- $\\hat{β_3}$ is significant and implies that:\n",
    "    - an increase in TV advertising of 1,000 dollars is associated with increased sales of ($\\hat{β_1}+ \\hat{β_3}\\times $ radio) ×1,000 =19+1.1×radio units. \n",
    "    - an increase in radio advertising of \\\\$1,000 will be associated with an increase in sales of ($\\hat{β_2} + \\hat{β_3} \\times $ TV) × 1,000 = 29 + 1.1 × TV units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a94ce2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Models' performance\n",
    "- We now have two models that include the same number of predictor variables (TV and Radio)\n",
    "- But the second model allows for synergies between Radio and TV\n",
    "- The $R^2$ for our model is 96.8%, compared to only 89.7% for the additive model\n",
    "    -  This means that (96.8 − 89.7)/(100 −89.7) = 69% of the variability in sales that remains after fitting the additive model has been explained by the interaction term.\n",
    "    - This interaction term has greatly improved our predictive power!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316cb979",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification\n",
    "- Qualitative variables need not be used only as predictors\n",
    "- In many interesting predictive tasks, the response variable is qualitative\n",
    "- Predicting a __categorical variable__ (two or more levels), is a process known as __classification__ \n",
    "\n",
    "- In this section, we will attempt to predict whether a person defaults on a loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f87a4c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Load New Dataset: Loan Defaults\n",
    "\n",
    "df = pd.read_excel('/'.join([folderPath, 'Default.xlsx']),index_col=0)\n",
    "\n",
    "#Data cleaning: The dataset has a column that is listed as \"Yes\" and \"No.\" We want it to read 0/1.\n",
    "display(df.head())\n",
    "\n",
    "# Note: factorize() returns two objects: a label array and an array with the unique values.#\n",
    "display(df.default.factorize())\n",
    "# We are only interested in the first object. \n",
    "df.rename(columns={\"default\":\"default_lab\",\"student\":\"student_lab\" },inplace=True)\n",
    "df.loc[:,'default'] = df.default_lab.factorize()[0] \n",
    "df['student'] = df.student_lab.factorize()[0]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d909dffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[['balance','income','default', 'student']],diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6571b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Binned Scatterplot of Balance \n",
    "df['bin'] = pd.cut(df.balance, bins=np.linspace(0, 3000, 10), include_lowest=True)\n",
    "\n",
    "balbins = df.groupby(['bin']).mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "\n",
    "ax.plot(balbins['balance'], balbins['default'], marker='o', linestyle=\"\")\n",
    "\n",
    "ax.set_xlabel(\"Bank Account Balance\")\n",
    "ax.set_ylabel(\"Average Probability of Default\")\n",
    "\n",
    "plt.title(\"Binned Scatterplot, Default vs. Balance\", {'fontsize':30})\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10620f88",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear probability model\n",
    "- The simplest way to perform classification with a categorical variable (2 levels) is to use the __linear probability model__ (LPM)\n",
    "- In the LPM, you regress a dummy variable on your regressors:\n",
    "\\begin{equation*}\n",
    "Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + ... + \\beta_k X_{ki} + \\varepsilon_i\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "- The population regression function is defined by the expected value of the dependent variable given each set of possible values of the regressors:\n",
    "\\begin{equation*}\n",
    "E(Y | X_1, X_2, ..., X_k) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_k X_k\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4991f9d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Question: why was $\\varepsilon_i$ in the population regression but not in the expectation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85efc30a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## From expectation to mean to probability\n",
    "\n",
    "- The expectation in the population corresponds to the sample mean\n",
    "- If a random variable can take only two values (0, 1) then its mean will also be between 0 and 1\n",
    "    - Mean of [0,0,0] is $\\frac{1}{3} \\times (0 + 0 + 0) = \\frac{0}{3} = 0$\n",
    "    - Mean of [1,1,1] is $\\frac{1}{3} \\times (1 + 1 + 1) = \\frac{3}{3} = 1$\n",
    "- If $Y$ is a dummy variable, then its expected value is the same as the probability that Y is equal to 1\n",
    "- So in this case, the population regression function is:\n",
    "\\begin{equation*}\n",
    "P(Y = 1 | X_1, X_2, ..., X_k) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_k X_k\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4163fffe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "est_ols = smf.ols('default ~ balance', data=df).fit() \n",
    "# Inspect the results\n",
    "print(est_ols.summary())\n",
    "balance=1000\n",
    "print(\"\\n\\n******\")\n",
    "print(f\"An increase in the credit card by ${balance:,} is associated with, on average, \\\n",
    "an increase in the probability of default by {round(est_ols.params['balance']*balance*100,3)} percentage points.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e661c92f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax= plt.subplots(1,1,figsize=(10,5))\n",
    "\n",
    "ax = sns.regplot(x=\"balance\", y=\"default\", data=df, ci=None,scatter_kws={'alpha':.05, 'color':'darkorange','marker':'*'},line_kws={ 'color':'darkgreen'})\n",
    "ax.get_xaxis().set_major_formatter( matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "ax.axhline(0, color='k', ls=\":\",alpha=0.5)\n",
    "ax.axhline(1, color='k', ls=\":\",alpha=0.5)\n",
    "\n",
    "ax.set_xlabel(\"Balance in $\", fontsize=16)\n",
    "ax.set_ylabel(\"Default (Dummy Variable)\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8366f41e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What's wrong with the linear probability model?\n",
    "- The relationship we estimated seems intuitive:\n",
    "    - As balance increases the probability of defaulting increases (linearly).\n",
    "- But remember that we now care more about our ability to predict the probability of defaulting\n",
    "    - Obviously our $R^2$ is not fantastic but both our parameters are significant at the 1\\% level \n",
    "    - More importantly, think about the values $\\hat{Y}_i$ can take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b5212",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "balance=[bal for bal in np.arange(0,10000, 200)]\n",
    "df_hat=pd.DataFrame(data={'intercept':est_ols.params['Intercept'], 'slope':est_ols.params['balance'], 'balance':balance})\n",
    "df_hat['Predicted Probability of Default']=df_hat['intercept'] + df_hat['slope']*df_hat['balance']\n",
    "fig, ax=plt.subplots(1,1,figsize=(10,10))\n",
    "\n",
    "sns.lineplot(data=df_hat, x=\"balance\", y=\"Predicted Probability of Default\", ax=ax, color='darkgreen')\n",
    "\n",
    "ax.set_xlim(left=0)\n",
    "ax.set_ylim(bottom=df_hat['Predicted Probability of Default'].min() , top=df_hat['Predicted Probability of Default'].max())\n",
    "\n",
    "ax.get_xaxis().set_major_formatter( matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "\n",
    "ax.set_xlabel(\"Balance in $\", fontsize=16)\n",
    "ax.set_ylabel(\"Probability of Default\", fontsize=16)\n",
    "\n",
    "ax.axhline(0, color='darkorange', ls=\":\")\n",
    "ax.axhline(1, color='darkorange', ls=\":\")\n",
    "\n",
    "ax.set_title(\"Predicted Probability of Default -- Linear Probability Model\")\n",
    "\n",
    "plt.axhspan(0, df_hat['Predicted Probability of Default'].min(), facecolor='0.5', alpha=0.3)\n",
    "plt.axhspan(1, df_hat['Predicted Probability of Default'].max(), facecolor='0.5', alpha=0.3)\n",
    "\n",
    "plt.text(0.2, 0.5, 'Probability',fontsize=16, horizontalalignment='center',verticalalignment='center', transform=ax.transAxes)\n",
    "plt.text(0.2, .9, 'Not a Probability',fontsize=16, horizontalalignment='center',verticalalignment='center', transform=ax.transAxes)\n",
    "plt.text(0.2, .015, 'Not a Probability',fontsize=16, horizontalalignment='center',verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c3e598",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Finding the right function\n",
    "- Remember that our goal is to first find the correct function linking X to Y in the population \n",
    "$$Y = f(\\mathbf{X}) + \\varepsilon$$\n",
    "- It seems that a linear function is bound to fail if the response variable is qualitative\n",
    "- In our example, even reasonable support yields predicted probability below 0 and above 1\n",
    "    - a support is the __range of values__ over which we use our function\n",
    "- Ideally, we would want to find a function which would predict a probability between 0 and 1 even if we use extravagant values as input (e.g -50,000 or 2 million balances)\n",
    "- It turns out, cumulative distribution functions fit this description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2066b8a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic Function\n",
    "- The most used cummulative distribution function is the standard logistic function:\n",
    "$$f(X) = \\frac{e^x}{1+e^x}$$\n",
    "- This function has a useful property for us:\n",
    "    - As $x \\to -\\infty$, $\\frac{e^x}{1+e^x} \\to 0$\n",
    "    - As $x \\to +\\infty$, $\\frac{e^x}{1+e^x} \\to 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b121d3f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2,figsize=(14,8))\n",
    "\n",
    "X=[x for x in np.arange(-20,20,1)]\n",
    "Y1=[math.exp(x) for x in X ]\n",
    "Y2=[math.exp(x)/(1+math.exp(x)) for x in X ]\n",
    "\n",
    "sns.lineplot( x=X, y=Y1 ,ax=axes[0], color='darkgreen')\n",
    "sns.lineplot( x=X, y=Y2 ,ax=axes[1], color='darkgreen')\n",
    "\n",
    "axes[0].set_xlabel(r\"$X$\", fontsize=20)\n",
    "axes[0].set_ylabel(r\"$f(X) = e^x$\", fontsize=16)\n",
    "axes[0].set_title(\"Exponential function\")\n",
    "axes[0].axhline(0, color='darkorange', ls=\":\")\n",
    "\n",
    "\n",
    "axes[1].set_xlabel(r\"$X$\", fontsize=20)\n",
    "axes[1].set_ylabel(r\"$f(X) = \\frac{e^x}{1+e^x} $\", fontsize=16)\n",
    "axes[1].set_title(\"standard logistic function\".capitalize())\n",
    "axes[1].axhline(0, color='darkorange', ls=\":\")\n",
    "axes[1].axhline(1, color='darkorange', ls=\":\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c452c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## From Logistic Function to Logistic Regression\n",
    "\n",
    "- We need to transform our logistic function to a __logistic regression__:\n",
    "    - Note that probability of default given balance can be written as\n",
    "        - Pr(default = Yes|balance)\n",
    "        - You read it as the probability (Pr) of defaulting (\"default=Yes\") given that (\"|\") balance is a given value\n",
    "    - Unlike the linear probability model, Pr(default = Yes|balance) will range between 0 and 1. \n",
    "    - Then for any given value of balance, a prediction can be made for default. \n",
    "$$Pr(Y=1 | X) = p(X) = \\Large\\frac{e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0 + \\beta_1x}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0275335",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logit (tl;dr)\n",
    "After a bit of algebra (that you don't need to know about) we can rewrite the logistic regression:\n",
    "$$ p(X) = \\Large\\frac{e^{(\\beta_0 + \\beta_1X)}}{1+e^{(\\beta_0 + \\beta_1X)}} $$\n",
    "\n",
    "as\n",
    "\n",
    "$$ \\underbrace{\\frac{p(X)}{1-p(X) }}_{odds} = \\Large e^{(\\beta_0 + \\beta_1X)} $$\n",
    "- e.g. $p(X) = 0.2$ implies odds of $\\frac{0.2}{1-0.2} = \\frac{1}{4}$\n",
    "    \n",
    "taking log of both sides yields:\n",
    "$$ \\underbrace{\\log (\\frac{p(X)}{1-p(X) })}_{\\text{log-odds}} = \\beta_0 + \\beta_1X $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1f7add",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression - Estimation\n",
    "\n",
    "- As per the OLS regression, the goal is to find the $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ that provide the best fit of the data.\n",
    "- This is done using __maximum likelihood estimation__ (MLE)\n",
    "    - We won't explain MLE method now, the solution is more technical than OLS\n",
    "    - But MLE follows the same logic:\n",
    "        - Try to find $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ such that plugging these estimates into the model for p(X),  yields a number close to one for all individuals who defaulted, and a number close to zero for all individuals who did not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6d02b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example using [Scikit Learn's Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f78849e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout \n",
    "# hide warnings from sklearn\n",
    "\n",
    "# Logistic regression using sklearn\n",
    "print(\"Shape {} and type {} before.\".format(df.balance.shape, type(df.balance)))\n",
    "X_train = df.balance.values.reshape(-1,1) # store the balance variable (our predictor) to a numpy array\n",
    "print(\"Shape {} and type {} after.\".format(X_train.shape, type(X_train)))\n",
    "y = df.default # Store the target variable in y\n",
    "\n",
    "# Calculate the classification probability and predicted classification.\n",
    "clf = skl_lm.LogisticRegression(solver='newton-cg') # instantiate a logistic regression class\n",
    "clf.fit(X_train,y) # fit the data using this class\n",
    "\n",
    "# Summary of the results\n",
    "beta_0= clf.intercept_[0] ; beta_1=clf.coef_[0][0]\n",
    "print('\\n\\nsklearn function used: ',clf)\n",
    "print('classes (unique values for Default): ',clf.classes_)\n",
    "print('\\u03B2\\u0302\\u2080:',beta_0)\n",
    "print('\\u03B2\\u0302\\u2081:',beta_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06134d4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## New prediction\n",
    "\n",
    "- We now have parameter values that can be used to predict the probability of default given balance.\n",
    "- Our predicted probability will thus follow:\n",
    "\n",
    "$\\hat{Pr}(Y=1 | X) = \\hat{p}(X) = \\Huge\\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1x}}{1+e^{\\hat{\\beta}_0 + \\hat{\\beta}_1x}} = \\Huge\\frac{e^{-10.7 + 0.005\\times x}}{1+e^{-10.7  + 0.005\\times x}} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc07fc5c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the predicted default against the actual data\n",
    "\n",
    "balance=[bal for bal in np.arange(df.balance.min(), df.balance.max())]\n",
    "df_hat=pd.DataFrame(data={'intercept':beta_0, 'slope':beta_1, 'balance':balance})\n",
    "# Predict using the logistic function formula\n",
    "df_hat['Predicted Probability of Default']=np.exp(df_hat['intercept'] + df_hat['slope']*df_hat['balance']) /(1+  np.exp(df_hat['intercept'] + df_hat['slope']*df_hat['balance']))\n",
    "\n",
    "df_hat.head()\n",
    "# initiate plot\n",
    "fig, ax=plt.subplots(1,1,figsize=(10,10))\n",
    "# plot actual datapoints\n",
    "sns.scatterplot(x=\"balance\", y=\"default\", data=df, ax=ax,alpha=.3, color='darkorange', marker='*')\n",
    "# plot y hat, the predicted probability of default from our estimated model\n",
    "sns.lineplot(data=df_hat, x=\"balance\", y=\"Predicted Probability of Default\", ax=ax, color='darkgreen')\n",
    "\n",
    "ax.get_xaxis().set_major_formatter( matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "ax.axhline(0, color='k',alpha=.3, linestyle=\":\")\n",
    "ax.axhline(1, color='k',alpha=.3, linestyle=\":\")\n",
    "\n",
    "ax.set_xlabel(\"Balance in $\", fontsize=16)\n",
    "ax.set_ylabel(\"Probability of Default\", fontsize=16)\n",
    "ax.set_title(\"Predicted Probability of Default -- Logistic Regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8d6302",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "est_logit = smf.logit('default ~ balance' , data=df).fit()\n",
    "print(est_logit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ce5555",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assessing the Accuracy of the Coefficient Estimates\n",
    "- Many aspects of the logistic regression output shown above are similar to the linear regression output from last week. \n",
    "- Estimated parameters also have standard errors. \n",
    "- The z-statistic in table plays the same role as the t-statistic in the linear regression output, for example in \n",
    "    - z-statistic associated with β1 is equal to $\\frac{\\hat{\\beta}_1}{SE(\\hat{\\beta}_1)}$\n",
    "    -  a large (absolute) value of the z-statistic indicates evidence against the null hypothesis that probability of default does not depend on balance \n",
    "        - H0 : $β_1$ = 0. \n",
    "        - This null hypothesis implies that $p(X) = \\frac{e^{\\beta_0}}{1 + e^{\\beta_0}}$\n",
    "- The p-value associated with balance is small, we can reject H0. \n",
    "- The estimated intercept is typically not of interest; its main purpose is to adjust the average fitted probabilities to the proportion of ones in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f76e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification\n",
    "\n",
    "balance=[bal for bal in np.arange(0,10000, 200)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76766640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing the two models \n",
    "est_probit = smf.probit('default ~ balance' , data=df).fit()\n",
    "\n",
    "\n",
    "df['logit_predict'] = est_logit.predict()\n",
    "df['ols_predict'] = est_ols.predict()\n",
    "df['probit_predict'] = est_probit.predict()\n",
    "df['bin'] = pd.cut(df.balance, bins=np.linspace(0, 3000, 100), include_lowest=True)\n",
    "\n",
    "\n",
    "balbins = df.groupby(['bin']).mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "\n",
    "ax.plot(balbins['balance'], balbins['default'], marker='o', linestyle=\"\", label='Binned Scatterplot')\n",
    "ax.plot(balbins['balance'], balbins['logit_predict'], marker='+', label=\"Logit\")\n",
    "ax.plot(balbins['balance'], balbins['probit_predict'], marker='+', label=\"Probit\")\n",
    "ax.plot(balbins['balance'], balbins['ols_predict'], marker='.', label=\"Linear Probability Model\")\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Bank Account Balance\")\n",
    "ax.set_ylabel(\"Average Probability of Default\")\n",
    "\n",
    "plt.title(\"Binned Scatterplot, Default vs. Balance\", {'fontsize':30})\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37f0ff5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Which of these models is a better classifier? \n",
    "\n",
    "__TRICK QUESTION__: All the models are going to set a single threshold above which all observations are classified as _default_ \n",
    "\n",
    "Last step for classifier: We need to set a threshold to determine assignment. What cutoff should we use? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0503333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = pd.DataFrame(data={'cutoffs': [.2, .5, .75, .9]})\n",
    "errors = pd.DataFrame(data={'cutoffs': np.linspace(0,1,10).tolist()})\n",
    "\n",
    "errors['Defaults'] = df[df['default']== 1].shape[0]\n",
    "errors['Total Observations'] = df.shape[0]\n",
    "errors['Predicted Defaults'] = [df[df['logit_predict']>x].shape[0] for x in errors['cutoffs']]\n",
    "errors['Correct Predictions'] = [df[(df['logit_predict']>x) & (df['default']==1)].shape[0] for x in errors['cutoffs']]\n",
    "errors['Incorrect Predictions'] = [df[(df['logit_predict']>x) & (df['default']==0)].shape[0] for x in errors['cutoffs']]\n",
    "errors['False Positive Rate'] = errors['Incorrect Predictions']/errors['Predicted Defaults']\n",
    "errors['True Positive Rate'] = errors['Correct Predictions']/errors['Defaults']\n",
    "\n",
    "errors.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df6b1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "\n",
    "ax.plot(errors['False Positive Rate'], errors['True Positive Rate'], marker='o', linestyle=\"-\")\n",
    "\n",
    "ax.set_xlabel(\"False Positive Rate\")\n",
    "ax.set_ylabel(\"True Positive Rate\")\n",
    "\n",
    "plt.title(\"ROC Curve\", {'fontsize':30})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186bd084",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multiple Logistic Regression\n",
    "\n",
    "- By analogy with the extension from simple to multiple linear regression from last class \n",
    "- We can generalize our logit to p, predictors:\n",
    "$$ \\log (\\frac{p(X)}{1-p(X) }) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bac51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['income_thd']=df['income']/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c46f585",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "est_mult_logistic = smf.logit('default ~ balance + income_thd + student' , data=df).fit()\n",
    "print(est_mult_logistic.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5171ae52",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multiple Logistic Regression - Interpretation\n",
    "\n",
    "- p-values associated with balance and student are small\n",
    "- the coefficient for the student dummy variable is negative:\n",
    "    - students are less likely to default than nonstudents\n",
    "    - for a fixed value of balance and income, a student is less likely to default than a non-student\n",
    "- The p-value associated with income suggests no effect of income\n",
    "    - conditional on balance level and student status "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab34f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "est_mult_linear = smf.ols('default ~ balance + income_thd + student' , data=df).fit()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec55e13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "errors_mult = pd.DataFrame(data={'cutoffs': np.linspace(0,1,10).tolist()})\n",
    "\n",
    "df['logit_predict'] = est_mult_logistic.predict()\n",
    "df['ols_predict'] = est_mult_linear.predict()\n",
    "\n",
    "errors_mult['Defaults'] = df[df['default']== 1].shape[0]\n",
    "errors_mult['Total Observations'] = df.shape[0]\n",
    "errors_mult['Predicted Defaults'] = [df[df['logit_predict']>x].shape[0] for x in errors_mult['cutoffs']]\n",
    "errors_mult['Correct Predictions'] = [df[(df['logit_predict']>x) & (df['default']==1)].shape[0] for x in errors_mult['cutoffs']]\n",
    "errors_mult['Incorrect Predictions'] = [df[(df['logit_predict']>x) & (df['default']==0)].shape[0] for x in errors_mult['cutoffs']]\n",
    "errors_mult['False Positive Rate'] = errors_mult['Incorrect Predictions']/errors_mult['Predicted Defaults']\n",
    "errors_mult['True Positive Rate'] = errors_mult['Correct Predictions']/errors_mult['Defaults']\n",
    "\n",
    "errors_mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f290e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ols_predict'] = est_mult_linear.predict()\n",
    "\n",
    "errors_lin = pd.DataFrame(data={'cutoffs': np.linspace(0,df['ols_predict'].max(),10).tolist()})\n",
    "\n",
    "\n",
    "errors_lin['Defaults'] = df[df['default']== 1].shape[0]\n",
    "errors_lin['Total Observations'] = df.shape[0]\n",
    "errors_lin['Predicted Defaults'] = [df[df['ols_predict']>x].shape[0] for x in errors_lin['cutoffs']]\n",
    "errors_lin['Correct Predictions'] = [df[(df['ols_predict']>x) & (df['default']==1)].shape[0] for x in errors_lin['cutoffs']]\n",
    "errors_lin['Incorrect Predictions'] = [df[(df['ols_predict']>x) & (df['default']==0)].shape[0] for x in errors_lin['cutoffs']]\n",
    "errors_lin['False Positive Rate'] = errors_lin['Incorrect Predictions']/errors_lin['Predicted Defaults']\n",
    "errors_lin['True Positive Rate'] = errors_lin['Correct Predictions']/errors_lin['Defaults']\n",
    "\n",
    "errors_lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d156eb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "\n",
    "ax.plot(errors_mult['False Positive Rate'], errors_mult['True Positive Rate'], marker='o', linestyle=\"-\", label='Multiple Logistic')\n",
    "ax.plot(errors_lin['False Positive Rate'], errors_lin['True Positive Rate'], marker='+', linestyle=\"--\", label='Multiple Linear')\n",
    "ax.plot(errors['False Positive Rate'], errors['True Positive Rate'], marker='+', linestyle=\"--\", label='Single Logistic')\n",
    "\n",
    "ax.set_xlabel(\"False Positive Rate\")\n",
    "ax.set_ylabel(\"True Positive Rate\")\n",
    "\n",
    "plt.title(\"ROC Curve\", {'fontsize':30})\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6467c6c9",
   "metadata": {},
   "source": [
    "# In-Class Exercise\n",
    "\n",
    "3. Let's say I want to target a 0.4 false-positive rate. What model gives the best predictions in this sample given a 0.4 false-positive rate? \n",
    "\n",
    "4. Is one of these three models the \"best?\" Why or why not? "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
