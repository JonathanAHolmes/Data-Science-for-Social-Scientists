{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d5b024",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <div align=\"center\"> SPECIAL TOPICS III </div>\n",
    "## <div align=\"center\"> Data Science for Social Scientists  </div>\n",
    "### <div align=\"center\"> ECO 4199 </div>\n",
    "#### <div align=\"center\">Class 8 - Re-Sampling Methods and Model Selection </div>\n",
    "<div align=\"center\"> Jonathan Holmes, (he/him)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c37a3a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review: Machine Learning (in a nutshell)\n",
    "\n",
    "<div align=\"center\">  $x \\, \\,$ --> $\\, \\, \\hat{f}(x) \\, \\,$ --> $\\, \\, \\hat{y}$ </div>\n",
    "\n",
    "#### Our Goal: \n",
    "- Find the $\\hat{f}$ that gives us the best predictions $\\hat{y}$ of $y$. \n",
    "\n",
    "#### Content of the course: \n",
    "- Different ways you can design $\\hat{f}$\n",
    "- Different ways of figuring out if you have chosen the best $\\hat{f}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3659ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review: Different ways to design $\\hat{f}$\n",
    "\n",
    "So far: \n",
    "1. Linear models (OLS)\n",
    "    - Linear models with dummy variables\n",
    "    - Linear models with interaction terms ($x*z$)\n",
    "    - Linear models with polynomial terms ($x^2$, $x^3$)\n",
    "\n",
    "2. Classification algorithms\n",
    "    - Linear probability model\n",
    "    - Logistic model\n",
    "    \n",
    "Still coming: \n",
    "1. Lasso\n",
    "2. Ridge Regression\n",
    "3. (Maybe) Tree models\n",
    "4. Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58266b8f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review: Different ways to figuring out if we have chosen the \"best\" $f$\n",
    "\n",
    "So far: \n",
    "1. Statistics based on mean-squared errors $r^2$, $RSS$, $MSE$\n",
    "\n",
    "2. For classification algorithms: True Postive rate and False Positive rate, ROC Curve\n",
    "\n",
    "Still Coming \n",
    "1. Adjusted $r^2$, AIC, BIC\n",
    "\n",
    "2. Cross-Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25c06fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review: Bias-Variance Trade-Off\n",
    "\n",
    "\n",
    "$$f(x) = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + ... + \\beta_N X^P $$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660c5a17",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How do we determine how many polynomial terms $N$ to include in the model? \n",
    "\n",
    "Bias-Variance Trade-Off:\n",
    "- Higher N -> Lower Bias\n",
    "- Higher N -> Higher Variance\n",
    "\n",
    "GOAL: Minimize out-of-sample criterion (usually mean-squared error)\n",
    "- Balance bias + variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7f388a",
   "metadata": {},
   "source": [
    "## New terminology\n",
    "\n",
    "$$f(x) = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + ... + \\beta_P X^P $$\n",
    "\n",
    "Parameter vs. Hyperparameter: \n",
    "- $\\beta_0$ ... $\\beta_P$ are __parameters__ \n",
    "- The number of polynomial terms $P$ is called a __tuning parameter__ or a __hyperparameter__\n",
    "\n",
    "Parameters: Variables that are used to fit a model $f$ to the data\n",
    "\n",
    "Tuning parameter: Any parameter that is used to control the learning process\n",
    "- They refer to a __model selection__ task\n",
    "- You define tuning parameters separately from training the model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200ecc6c",
   "metadata": {},
   "source": [
    "## Auto.csv\n",
    "\n",
    "- Let's use the <span style=\"color:orange;\">Auto dataset</span> from [ISLR](https://www.statlearning.com/)\n",
    "- Using this data we want to predict fuel consumption (miles per gallon _mpg_) based on _horsepower_\n",
    "- Let's try a few polynomials in the relationship between these two variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9519b15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let me set my current directory using the %cd magic\n",
    "%cd \"~/Dropbox/_teaching/ECO4199/2023/Data-Science-for-Social-Scientists/Class 08 - Resampling Methods/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177b54f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681c3cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"Auto.csv\") # load data\n",
    "df['horsepower']=pd.to_numeric(df['horsepower'].replace(\"?\",np.nan)) # data cleaning\n",
    "df.dropna(subset=['horsepower','mpg'],inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "display(df.info()) # display info\n",
    "df.head().append(df.tail()) # show head and tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9b521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regress Sales on a constant term and TV\n",
    "results = smf.ols('mpg ~ horsepower', data=df).fit() # degree 1\n",
    "# Inspect the results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d5b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models=['mpg ~ horsepower', 'mpg ~ horsepower + np.square(horsepower)',\n",
    "        'mpg ~ horsepower + np.square(horsepower)+np.power(horsepower,3)',\n",
    "        'mpg ~ horsepower + np.square(horsepower)+np.power(horsepower,3)+np.power(horsepower,4)']\n",
    "n=len(models)\n",
    "\n",
    "R2=np.full(n, np.nan); degrees=np.full(n, np.nan)\n",
    "aic=[];bic=[]; adj_R2_list=[]\n",
    "\n",
    "beta_1=np.full(n, np.nan) ; beta_2=np.full(n, np.nan) ; beta_3=np.full(n, np.nan) ; beta_4=np.full(n,np.nan)\n",
    "p_1=np.full(n, np.nan) ; p_2=np.full(n, np.nan) ; p_3=np.full(n, np.nan); p_4 = np.full(n, np.nan)\n",
    "for i,m in enumerate(models):\n",
    "    results = smf.ols(m, data=df).fit()\n",
    "    #print(results.summary()) # uncomment if you want details\n",
    "    \n",
    "    R2[i]=results.rsquared ; degrees[i]=i+1\n",
    "    adj_R2_list.append(results.rsquared_adj); aic.append(results.aic); bic.append(results.bic)\n",
    "    \n",
    "    beta_1[i]=results.params['horsepower'] ; p_1[i]=results.pvalues['horsepower']\n",
    "    if i>0:\n",
    "        beta_2[i]=results.params['np.square(horsepower)'] ; p_2[i]=results.pvalues['np.square(horsepower)']\n",
    "    if i>1:\n",
    "        beta_3[i]=results.params['np.power(horsepower, 3)'] ; p_3[i]=results.pvalues['np.power(horsepower, 3)']\n",
    "    if i>2:\n",
    "        beta_4[i]=results.params['np.power(horsepower, 4)'] ; p_4[i] = results.pvalues['np.power(horsepower, 4)']\n",
    "\n",
    "res=pd.DataFrame({'Degree':degrees, \n",
    "                  r'$R^2$': R2,\n",
    "                  r'$\\hat{\\beta}_1$':beta_1, r'p-value $\\hat{\\beta}_1$':p_1,\n",
    "                  r'$\\hat{\\beta}_2$':beta_2, r'p-value $\\hat{\\beta}_2$':p_2,\n",
    "                  r'$\\hat{\\beta}_3$':beta_3, r'p-value $\\hat{\\beta}_3$':p_3,\n",
    "                  r'$\\hat{\\beta}_4$':beta_4, r'p-value $\\hat{\\beta}_3$':p_4})\n",
    "\n",
    "res['Degree']=res['Degree'].astype(np.int8)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46149027",
   "metadata": {},
   "source": [
    "## How to determine the tuning parameter $p$? \n",
    "\n",
    "Recall: In-sample $R^2$ only increases if \n",
    "\n",
    "\n",
    "### Different approaches: \n",
    "1. Economist approach: Use p-values and economic models/intuitions\n",
    "2. Corrected in-sample statistics \n",
    "3. The validation set approach\n",
    "4. Cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472676d9",
   "metadata": {},
   "source": [
    "## In-class exercise: \n",
    "\n",
    "Q1: Using just the estimates and p-values, what level of polynomial is best? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396720d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2: Corrected Statistics\n",
    "\n",
    "Problem: In-sample RSS and $R^2$ never go down when you add variables to the model\n",
    "- These statistics do NOT balance variance and bias\n",
    "\n",
    "Potential Solution: Corrected statistics\n",
    "- These statistics \"penalize\" in-sample estimates when you add more variables to the model\n",
    "- Higher penalty -> Less variance \n",
    "- Lower penalty -> Less bias\n",
    "\n",
    "Three examples: \n",
    "- Adjusted $R^2$\n",
    "- AIC\n",
    "- BIC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b958472a",
   "metadata": {},
   "source": [
    "## Adjusted $R^2$\n",
    "$$\\large \\text{Adjusted R}^2 = 1- \\frac{\\frac{\\text{RSS}}{n-p-1}}{\\frac{TSS}{n-1}}$$\n",
    "\n",
    "Definitions: \n",
    "- RSS: Residual sum of squares\n",
    "- TSS: Total sum of squares\n",
    "- Recall that the usual $R^2= 1- \\frac{\\text{RSS}}{\\text{TSS}}$\n",
    "- n: Number of observations\n",
    "- p: Number of parameters\n",
    "\n",
    "Notes: \n",
    "- Maximizing the adjusted $R^2$ is equivalent to minimizing $\\frac{\\text{RSS}}{n-p-1}$\n",
    "    - While RSS always decreases as the number of variables in the model increases\n",
    "    - $\\frac{\\text{RSS}}{n-p-1}$ may increase or decrease, due to the presence of p in the denominator.\n",
    "- The intuition behind the adjusted $R^2$ is that once all of the correct variables have been included in the model, adding additional variables just adds noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c09d091",
   "metadata": {},
   "source": [
    "## Akaike information criterion (AIC)\n",
    "\n",
    "- For a fitted least squares model containing p predictors\n",
    "$$\\large \\text{AIC}=\\frac{1}{n\\hat{\\sigma}^2}(\\text{RSS} + 2p\\hat{\\sigma}^2) = \\frac{\\text{RSS}}{n\\hat{\\sigma}^2} + \\frac{2p}{n}$$\n",
    "- Recall: MSE $= \\frac{1}{n} \\text{RSS}$\n",
    "\n",
    "\n",
    "- $\\hat{\\sigma}^2$ is an estimate of the variance of the error $\\varepsilon$ associated with each response measurement \n",
    "- $2p\\hat{\\sigma}^2$  is a statistical penalty on the training RSS\n",
    "    - In order to account for the fact that training error tends to underestimate the test error\n",
    "    - The penalty increases with the number of predictors $p$ to adjust for the corresponding decrease in training RSS\n",
    "- Smaller values of AIC indicates better fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9ca917",
   "metadata": {},
   "source": [
    "## Bayesian information criterion (BIC)\n",
    "$$\\large \\text{BIC}=\\frac{1}{n\\hat{\\sigma}^2}(\\text{RSS} + \\log(n)p\\hat{\\sigma}^2)$$\n",
    "- $n$ represents the number of observations\n",
    "- Notice that BIC replaces the $2p\\hat{\\sigma}^2$ used by AIC with a $\\log(n)p\\hat{\\sigma}^2$\n",
    "term, where n is the number of observations\n",
    "- Since $\\log(n) > 2$ for any n > 7, the BIC statistic generally places a heavier penalty on models with many variables compared with AIC \n",
    "- As before, smaller values of BIC indicates better fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934c618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dd = pd.DataFrame(data={'Degree':degrees, f'$R^2$': R2,f'Adjusted $R^2$':adj_R2_list, \"AIC\":aic, \"BIC\":bic})\n",
    "dd['Degree']=dd['Degree'].astype(np.int8)\n",
    "dd.head()\n",
    "#dd=pd.concat(DFs)\n",
    "#dd.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e96ef00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(2,2, figsize=(10,10),sharex=True)\n",
    "axes = axes.ravel() # access axes with a single position instead of 2\n",
    "for i, statistics in enumerate([f'$R^2$',f\"Adjusted $R^2$\",\"AIC\",\"BIC\"]):\n",
    "    sns.scatterplot(x='Degree',y=statistics,data=dd,ax=axes[i], color='black',marker='.')\n",
    "    sns.lineplot(x='Degree',y=statistics,data=dd,ax=axes[i], color='darkorange')\n",
    "    \n",
    "fig.suptitle(\"In-Sample Statistics\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6081c581",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In-class exercise: \n",
    "\n",
    "Q2: According to the corrected in-sample statistics, how many polynomial terms should we include? \n",
    "\n",
    "Q3: Does each method select a different number of polynomial terms? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c60ff99",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advanced In-class exercise: \n",
    "\n",
    "Q4: If we did this with other datasets and other circomstances, do you think AIC, BIC, and Adjusted $R^2$ will always select the same number of parameters? \n",
    "\n",
    "\n",
    "Q5: Lets say you select P using AIC (Model #1) and then with BIC (Model #2). Which one will have a higher bias? Which one will have a higher variance? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a43e1a",
   "metadata": {},
   "source": [
    "# How to set the tuning parameter N? \n",
    "\n",
    "### Different approaches: \n",
    "1. Economist approach: Use p-values and economic models/intuitions\n",
    "2. Corrected in-sample statistics \n",
    "3. __The validation set approach__\n",
    "4. __Cross-validation__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61495ff7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3: The Validation Set Approach\n",
    "\n",
    "- A simple way to know about the out of sample performance of our model is to split the dataset in two:\n",
    "    -  __training set__ \n",
    "    -__validation set__ or hold-out set \n",
    "    validation set or hold-out set.\n",
    "- You then compute the MSE on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf58a6a5",
   "metadata": {},
   "source": [
    "- Here again we will use [Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "- We will split the data between train and test sets, 50/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a217907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['horsepower'], df['mpg'], test_size=.5, random_state=1706)\n",
    "    \n",
    "print(f\"Shape of X_train: {X_train.shape}\") ; print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\") ; print(f\"Shape of y_test: {y_test.shape}\")\n",
    "\n",
    "polynomials=[1,2,3, 4]\n",
    "for i,n in enumerate(polynomials): # i corresponds to the index (0, 1, 2); n takes the values in the polynomial list (1,2,3)\n",
    "    # prepocess the data\n",
    "    polynomial_features =PolynomialFeatures(degree=n,include_bias=False) # iniate a class to get data of degree n\n",
    "    X_train_new=polynomial_features.fit_transform(X_train.values.reshape(-1, 1)) # get data of degree n\n",
    "    X_test_new=polynomial_features.fit_transform(X_test.values.reshape(-1, 1)) # get data of degree n\n",
    "\n",
    "    \n",
    "    # regression\n",
    "    reg = LinearRegression() # initiate the regression class\n",
    "    reg.fit(X_train_new,y_train) # fit the data\n",
    "    \n",
    "    # Out of Sample MSE:\n",
    "    mse=mean_squared_error(y_test, reg.predict(X_test_new))\n",
    "    print(f\"Polynomial regression of degree {n} has an out of sample MSE score of {mse}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30953b4c",
   "metadata": {},
   "source": [
    "## In-Class Exercise #3\n",
    "\n",
    "Q6: Using the Validation set approach, which polynomial degree is best? \n",
    "\n",
    "Q7: If I re-ran this exercise, would I get the same answer? \n",
    "\n",
    "Q8: What would happen to bias and variance if I changed the size of the leave-out sample from .5 to .2? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd427332",
   "metadata": {},
   "source": [
    "## Sampling properties\n",
    "- Remember when talked sampling properties of OLS?\n",
    "- The smaller the sample size the less precise the estimate\n",
    "    - What if our 50/50 split happens to train our data on a fairly unrepresentative sample?\n",
    "    - let's confirm this using different 50/50 splits for our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c159d90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "polynomials=[1,2,3,4,5]\n",
    "DFs=[] # empty list of dataframes\n",
    "for n, p in enumerate(polynomials):\n",
    "    MSEs=[] # empty list of MSE scores\n",
    "    for i in range(len(df)):\n",
    "        # split the data (a different sample with each iteration)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df['horsepower'], df['mpg'], test_size=.5, random_state=i)\n",
    "\n",
    "        # prepocess the data\n",
    "        polynomial_features =PolynomialFeatures(degree=p,include_bias=False) # iniate a class to get data of degree n\n",
    "        X_train_new=polynomial_features.fit_transform(X_train.values.reshape(-1, 1)) # get data of degree n\n",
    "        X_test_new=polynomial_features.fit_transform(X_test.values.reshape(-1, 1)) # get data of degree n\n",
    "\n",
    "        # regression\n",
    "        reg = LinearRegression() # initiate the regression class\n",
    "        reg.fit(X_train_new,y_train) # fit the data\n",
    "\n",
    "        # Out of Sample MSE:\n",
    "        mse=mean_squared_error(y_test, reg.predict(X_test_new))\n",
    "        MSEs.append(mse)\n",
    "    # store all MSEs for degree p in a dataframe and append to list of dataframes    \n",
    "    DFs.append(pd.DataFrame({'Degree':p, 'MSE':MSEs}))\n",
    "\n",
    "dd=pd.concat(DFs, ignore_index=True)\n",
    "# show the distribution of MSEs\n",
    "fig, ax = plt.subplots(1,1, figsize=(8,8))\n",
    "\n",
    "sns.kdeplot(data=dd, ax=ax, x='MSE', hue='Degree')\n",
    "ax.set_xlabel(\"MSE Scores - Validation Set Approach\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873d6a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Mean-Squared Error Across All Simulations\n",
    "dd.groupby('Degree').agg('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b809e70c",
   "metadata": {},
   "source": [
    "##  Leave-one-out cross-validation (LOOCV) \n",
    "- leave-one out cross validation is closely related to the validation set approach but it attempts to address that method’s drawbacks.\n",
    "- Like the validation set approach, _LOOCV_ involves splitting the set of observations into two parts, BUT \n",
    "    - a single observation $(x_1, y_1)$ is used for the validation set\n",
    "    - the remaining observations $\\{(x_2, y_2), . . . , (x_n, y_n)\\}$ make up the training set\n",
    "- The statistical learning method is fit on the n − 1 training observations, and a prediction $\\hat{y}_1$ is made for the excluded observation, using its value x1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eef173",
   "metadata": {},
   "source": [
    "##  Leave-one-out cross-validation (LOOCV), continued\n",
    "- Since $(x_1, y_1)$ was not used in the fitting process\n",
    "    - $MSE_{1}$ = $(y_1-\\hat{y}_1)^2$\n",
    "- This provides an approximately __unbiased estimate for the test error__.\n",
    "- But even though MSE is unbiased for the test error, it is a poor estimate because it is highly variable, since it is based upon a single observation $(x_1, y_1)$.\n",
    "- We can repeat the procedure by selecting (x2, y2) and so on:\n",
    "    -     - $MSE_{(n)}$ = $(y_n-\\hat{y}_n)^2$\n",
    "- The LOOCV estimate for the test MSE is the average of these n test error estimates:\n",
    "    - $MSE_{(LOOCV)} = \\frac{1}{n} \\sum_{i=1}^n MSE_{i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d7e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "polynomials=[1,2,3,4,5]\n",
    "DFs=[] # empty list of dataframes\n",
    "for n, p in enumerate(polynomials):\n",
    "    MSEs=[] # empty list of MSE scores\n",
    "    for i in range(len(df)):\n",
    "        # split the data (a different sample with each iteration)\n",
    "        X_train, X_test, y_train, y_test = df.loc[df.index!=i,'horsepower'],df.loc[df.index==i,'horsepower'] , df.loc[df.index!=i,'mpg'],df.loc[df.index==i,'mpg'].values[0]\n",
    "\n",
    "        # prepocess the data\n",
    "        polynomial_features =PolynomialFeatures(degree=p,include_bias=False) # iniate a class to get data of degree n\n",
    "        X_train_new=polynomial_features.fit_transform(X_train.values.reshape(-1, 1)) # get data of degree n\n",
    "        X_test_new=polynomial_features.fit_transform(np.array(X_test).reshape(-1, 1)) # get data of degree n\n",
    "\n",
    "        # regression\n",
    "        reg = LinearRegression() # initiate the regression class\n",
    "        reg.fit(X_train_new,y_train) # fit the data\n",
    "\n",
    "        # Out of Sample MSE:\n",
    "        mse=np.square(y_test-reg.predict(X_test_new)[0]) # Out of Sample MSE\n",
    "        MSEs.append(mse)\n",
    "    # store all MSEs for degree p in a dataframe and append to list of dataframes    \n",
    "    DFs.append(pd.DataFrame({'Degree':p, 'MSE':MSEs}))\n",
    "\n",
    "dd=pd.concat(DFs, ignore_index=True)    \n",
    "display(dd.groupby('Degree').agg(Mean_MSE=('MSE','mean')))\n",
    "# show the distribution of MSEs\n",
    "fig, ax = plt.subplots(1,1, figsize=(8,8))\n",
    "sns.kdeplot(data=dd, x='MSE', ax=ax, hue='Degree')\n",
    "ax.set_xlabel(\"MSE Scores - LOOCV\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a61a9c",
   "metadata": {},
   "source": [
    "## k-Fold Cross-Validation\n",
    "- An alternative to LOOCV is k-fold cross validation. \n",
    "- This approach involves randomly dividing the set of observations into k groups, or __folds__, of approximately equal size. \n",
    "- The first fold is treated as a validation set, and the method is fit on the remaining k − 1 folds. \n",
    "- The mean squared error, $MSE_1$, is then computed on the observations in the held-out fold. \n",
    "- This procedure is repeated k times; each time, a different group of observations is treated as a validation set.\n",
    "- This process results in k estimates of the test error, $MSE_1$,$MSE_2$, . . . ,$MSE_k$. \n",
    "- The k-fold CV estimate is computed by averaging these values:\n",
    "    - $MSE_{(k-fold CV)} = \\frac{1}{k} \\sum_{i=1}^k MSE_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb45885",
   "metadata": {},
   "source": [
    "# k-fold Cross-Validation, continued\n",
    "- Note that LOOCV is a special case of k-fold CV in which k is set to equal n. \n",
    "- Standard practices consist in using k = 5 or k = 10. \n",
    "- What is the advantage of using k = 5 or k = 10 rather than k = n? \n",
    "    - Computation time which will increase with dataset sizes. \n",
    "    - LOOCV requires fitting the statistical learning method n times. \n",
    "        - This has the potential to be computationally expensive \n",
    "    - Performing 10-fold CV requires fitting the learning procedure only ten times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726a9a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X = df['horsepower'].values\n",
    "y = df['mpg'].values\n",
    "polynomials=[1,2,3,4,5]\n",
    "kDFs=[]\n",
    "for k in [5,10]:\n",
    "    DFs=[]\n",
    "    # Split in k folds\n",
    "    kf = KFold(n_splits=k, random_state=1706, shuffle=True)\n",
    "    for p in polynomials:\n",
    "        MSEs=[] # empty list of MSE scores\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            # prepocess the data\n",
    "            polynomial_features =PolynomialFeatures(degree=p,include_bias=False) # iniate a class to get data of degree n\n",
    "            X_train_new=polynomial_features.fit_transform(X_train.reshape(-1, 1)) # get data of degree n\n",
    "            X_test_new=polynomial_features.fit_transform(X_test.reshape(-1, 1)) # get data of degree n\n",
    "\n",
    "            # regression\n",
    "            reg = LinearRegression() # initiate the regression class\n",
    "            reg.fit(X_train_new,y_train) # fit the data\n",
    "\n",
    "            # Out of Sample MSE:\n",
    "            mse=mean_squared_error(y_test, reg.predict(X_test_new))\n",
    "            MSEs.append(mse)\n",
    "            # store all MSEs for degree p in a dataframe and append to list of dataframes    \n",
    "        DFs.append(pd.DataFrame({'Degree':p, 'k-fold':k,'MSE':MSEs}))\n",
    "\n",
    "    dd=pd.concat(DFs) \n",
    "    kDFs.append(dd)\n",
    "\n",
    "dd=pd.concat(kDFs, ignore_index=True)\n",
    "mse_change=dd.groupby(['Degree','k-fold']).agg(Mean_MSE=('MSE','mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd40337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the distribution of MSEs\n",
    "fig, axes = plt.subplots(2,2, figsize=(10,10))\n",
    "sns.kdeplot(data=dd.loc[dd['k-fold']==5], x='MSE', ax=axes[0,0], hue='Degree')\n",
    "axes[0,0].set_xlabel(\"MSE Scores - 5-fold Cross Validation\",fontsize=12)\n",
    "sns.kdeplot(data=dd.loc[dd['k-fold']==10], x='MSE', ax=axes[0,1], hue='Degree')\n",
    "axes[0,1].set_xlabel(\"MSE Scores - 10-fold Cross Validation\",fontsize=12)\n",
    "\n",
    "gs = axes[1, 1].get_gridspec()\n",
    "for ax in axes[1, :]:\n",
    "    ax.remove()\n",
    "axbottom = fig.add_subplot(gs[1, :])\n",
    "sns.lineplot(data=mse_change.reset_index(),x='Degree', y='Mean_MSE',hue='k-fold',alpha=.7, ax=axbottom)\n",
    "sns.scatterplot(data=mse_change.reset_index(),x='Degree', y='Mean_MSE',hue='k-fold',style='Degree', ax=axbottom,legend=False)\n",
    "axbottom.set_xlabel(\"Degrees of the Polynomial Regression\",fontsize=12)\n",
    "axbottom.set_ylabel(\"Mean Squared Errors\",fontsize=12)\n",
    "axbottom.set_xticks(np.arange(1,len(polynomials)+1))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bd10e6",
   "metadata": {},
   "source": [
    "## Why do we use resampling methods?\n",
    "- Cross Validation is one of the several __resampling methods__ one can use to learn about predictive performance \n",
    "- Do we really care about the test MSE value we get from CV?\n",
    "    - If you want to know the performance of a given statistical model, yes\n",
    "        - e.g. Given my model how wrong should I expect my predictions to be \"on average\"?\n",
    "- Sometimes you may only be interested in the location of the minimum point in the test MSE\n",
    "    - If you want to compare perfomance of different models \n",
    "        - Like we did today\n",
    "    - The actual value of the test MSE is not important\n",
    "     - What matters is which models performs best\n",
    "         - within the same method using different levels of flexibility (like we did today)\n",
    "         - or across methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fb2ae3",
   "metadata": {},
   "source": [
    "## Bias-Variance Trade-Off, 1/3\n",
    "- LOOCV is a special case of k-fold (since k<n) \n",
    "- We said that a reason to privilege k-fold has to do with computational power\n",
    "    - There are not to be neglected\n",
    "    - But what if LOOCV performs better?\n",
    "- Turns out, there is another reason to use k-fold CV: __accuracy__\n",
    "    - k-fold validation usually gets closer to the true MSE than LOOCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9c53cb",
   "metadata": {},
   "source": [
    "## Bias-Variance Trade-Off - Bias Reduction, 2/3 \n",
    "\n",
    "- An issue with the _validation set_ approach is that it can overestimate the test MSE\n",
    "    - You train your model on half the data which can generate a __bias__\n",
    "- With LOOCV you use almost all (n-1) the data in your training\n",
    "     - This means that LOOCV will give approximately __unbiased estimates__ of the test error\n",
    "     - k-fold is somewhere in between the _validation set_ and _LOOCV_\n",
    "- So if you care about lowest bias, LOOCV is the best method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080df8dc",
   "metadata": {},
   "source": [
    "## Bias-Variance Trade-Off - Variance Reduction, 3/3\n",
    "\n",
    "- But we know that bias is not the only source for concern in an estimating procedure (wink, t-test and p-values)\n",
    "    - we must also consider the procedure’s variance. \n",
    "- LOOCV has __higher variance__ than does k-fold CV with k < n. \n",
    "- Remember that, for both, you take averages of test squared errors\n",
    "    - You actually take n averages in LOOCV from almost identical training datasets\n",
    "    - This implies that test $MSE_{(LOOCV),i}$ are highly correlated \n",
    "        - k-fold are less correlated: overlap between training sets is smaller\n",
    "\n",
    "- The mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated\n",
    "- the test error estimate resulting from LOOCV tends to have higher variance than does the test error estimate resulting from k-fold CV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b363c96b",
   "metadata": {},
   "source": [
    "## In-Class Exercise: \n",
    "\n",
    "\n",
    "Q9: As you increase the number of folds in K-fold cross-|validation\n",
    "- What happens to bias?\n",
    "- What happens to variance? "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
