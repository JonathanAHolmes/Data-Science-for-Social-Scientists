{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d5b024",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <div align=\"center\"> SPECIAL TOPICS III </div>\n",
    "## <div align=\"center\"> Data Science for Social Scientists  </div>\n",
    "### <div align=\"center\"> ECO 4199 </div>\n",
    "#### <div align=\"center\">Class 8 - Re-Sampling Methods and Model Selection </div>\n",
    "<div align=\"center\"> Jonathan Holmes, (he/him)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c37a3a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review: Machine Learning (in a nutshell)\n",
    "\n",
    "<div align=\"center\">  $x \\, \\,$ --> $\\, \\, \\hat{f}(x) \\, \\,$ --> $\\, \\, \\hat{y}$ </div>\n",
    "\n",
    "#### Our Goal: \n",
    "- Find the $\\hat{f}$ that gives us the best predictions $\\hat{y}$ of $y$. \n",
    "\n",
    "#### Content of the course: \n",
    "- Different ways you can design $\\hat{f}$\n",
    "- Different ways of figuring out if you have chosen the best $\\hat{f}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3659ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Different ways to design $\\hat{f}$\n",
    "\n",
    "So far: \n",
    "1. Linear models (OLS)\n",
    "    - Linear models with dummy variables\n",
    "    - Linear models with interaction terms ($x*z$)\n",
    "    - Linear models with polynomial terms ($x^2$, $x^3$)\n",
    "\n",
    "2. Classification algorithms\n",
    "    - Linear probability model\n",
    "    - Logistic model\n",
    "\n",
    "This class: \n",
    "- How to select which variables to use\n",
    "    \n",
    "    \n",
    "Still coming: \n",
    "1. Lasso\n",
    "2. Ridge Regression\n",
    "3. (Maybe) Tree models\n",
    "4. Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58266b8f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Different ways to figuring out if we have chosen the \"best\" $f$\n",
    "\n",
    "So far: \n",
    "1. Statistics based on mean-squared errors $r^2$, $RSS$, $MSE$\n",
    "\n",
    "2. For classification algorithms, True Postive rate and False Positive rate\n",
    "\n",
    "This class: \n",
    "1. Adjusted $r^2$, AIC, BIC\n",
    "\n",
    "2. Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35551531",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f25c06fd",
   "metadata": {},
   "source": [
    "# Roadmap\n",
    "\n",
    "- In the last lecture we saw the risk associated with overfitting. \n",
    "- Although we discussed the case of polynomial regressions, the risk of overfitting is common to all learning models\n",
    "- This risk usually increases with the number of parameters\n",
    "$$ Y = f(\\mathbf{X}) + \\varepsilon$$\n",
    "- Regression and classification used tools that you were already familiar with (at least to some extent)\n",
    "- But we only scratched the surface of what these functions may look like\n",
    "- Today, we will keep using these tools with a different goal in mind\n",
    "- This goal will require new tools..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660c5a17",
   "metadata": {},
   "source": [
    "## Dealing with overfitting\n",
    "- You may think that the issue came from allowing for (undue) higher degrees in our model\n",
    "- But the issue is broader than this.\n",
    "- We used a model in which Y was related to $X$ and $X^2$ not $X^n$ for n>2\n",
    "- In other words we were using too many predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ff4bda",
   "metadata": {},
   "source": [
    "## RSS and MSE\n",
    "- the training set MSE is generally an underestimate of the test MSE. \n",
    "    - Recall that MSE = RSS/n. \n",
    "- When we fit a model to the training data using least squares, we specifically estimate the regression coefficients such that the training RSS (but not the test RSS) is as small as possible. \n",
    "- In particular, the training error will decrease as more variables are included in the model, but the test error may not.\n",
    "- Therefore, training set RSS and training set R2 cannot be used to select from among a set of models with different numbers of variables.\n",
    "- However, a number of techniques for adjusting the training error for the model size are available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200ecc6c",
   "metadata": {},
   "source": [
    "## Auto.csv\n",
    "\n",
    "- Let's use the <span style=\"color:orange;\">Auto dataset</span> from [ISLR](https://www.statlearning.com/)\n",
    "- Using this data we want to predict fuel consumption (miles per gallon _mpg_) based on _horsepower_\n",
    "- This time we do not know what is the true function\n",
    "    - But increasing marginal cost of power may apply here...\n",
    "- So let's try a few polynomials in the relationship between these two variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681c3cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"Auto.csv\") # load data\n",
    "df['horsepower']=pd.to_numeric(df['horsepower'].replace(\"?\",np.nan)) # data cleaning\n",
    "df.dropna(subset=['horsepower','mpg'],inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "display(df.info()) # display info\n",
    "df.head().append(df.tail()) # show head and tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9b521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regress Sales on a constant term and TV\n",
    "results = smf.ols('mpg ~ horsepower', data=df).fit() # degree 1\n",
    "# Inspect the results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d5b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models=['mpg ~ horsepower', 'mpg ~ horsepower + np.square(horsepower)','mpg ~ horsepower + np.square(horsepower)+np.power(horsepower,3)']\n",
    "n=len(models)\n",
    "\n",
    "R2=np.full(n, np.nan); degrees=np.full(n, np.nan)\n",
    "beta_1=np.full(n, np.nan) ; beta_2=np.full(n, np.nan) ; beta_3=np.full(n, np.nan) \n",
    "p_1=np.full(n, np.nan) ; p_2=np.full(n, np.nan) ; p_3=np.full(n, np.nan)\n",
    "for i,m in enumerate(models):\n",
    "    results = smf.ols(m, data=df).fit()\n",
    "    #print(results.summary()) # uncomment if you want details\n",
    "    \n",
    "    R2[i]=results.rsquared ; degrees[i]=i+1\n",
    "    beta_1[i]=results.params['horsepower'] ; p_1[i]=results.pvalues['horsepower']\n",
    "    if i>0:\n",
    "        beta_2[i]=results.params['np.square(horsepower)'] ; p_2[i]=results.pvalues['np.square(horsepower)']\n",
    "        if i>1:\n",
    "            beta_3[i]=results.params['np.power(horsepower, 3)'] ; p_3[i]=results.pvalues['np.power(horsepower, 3)']\n",
    "        \n",
    "res=pd.DataFrame({'Degree':degrees, \n",
    "                  r'$R^2$': R2,\n",
    "                  r'$\\hat{\\beta}_1$':beta_1, r'p-value $\\hat{\\beta}_1$':p_1,\n",
    "                  r'$\\hat{\\beta}_2$':beta_2, r'p-value $\\hat{\\beta}_2$':p_2,\n",
    "                  r'$\\hat{\\beta}_3$':beta_3, r'p-value $\\hat{\\beta}_3$':p_3})\n",
    "\n",
    "res['Degree']=res['Degree'].astype(np.int8)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e201a60",
   "metadata": {},
   "source": [
    "## Do we need higher-order polynomials?\n",
    "- Is higher better?\n",
    "    - Here we do not even need out of sample MSE since:\n",
    "        - the p-value attached to the cubic term suggests it is not significant\n",
    "        - the value of the parameter is very small\n",
    "        - the $R^2$ is left unchanged\n",
    "- We may still want to decide based on out of sample measures of the MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61495ff7",
   "metadata": {},
   "source": [
    "## The Validation Set Approach\n",
    "\n",
    "- A simple way to know about the out of sample performance of our model is to split the dataset in two:\n",
    "    -  __training set__ \n",
    "    -__validation set__ or hold-out set \n",
    "    validation set or hold-out set.\n",
    "- You then compute the MSE on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf58a6a5",
   "metadata": {},
   "source": [
    "- Here again we will use [Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "- We will split the data between train and test sets, 50/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a217907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['horsepower'], df['mpg'], test_size=.5, random_state=1706)\n",
    "    \n",
    "print(f\"Shape of X_train: {X_train.shape}\") ; print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\") ; print(f\"Shape of y_test: {y_test.shape}\")\n",
    "\n",
    "polynomials=[1,2,3]\n",
    "for i,n in enumerate(polynomials): # i corresponds to the index (0, 1, 2); n takes the values in the polynomial list (1,2,3)\n",
    "    # prepocess the data\n",
    "    polynomial_features =PolynomialFeatures(degree=n,include_bias=False) # iniate a class to get data of degree n\n",
    "    X_train_new=polynomial_features.fit_transform(X_train.values.reshape(-1, 1)) # get data of degree n\n",
    "    X_test_new=polynomial_features.fit_transform(X_test.values.reshape(-1, 1)) # get data of degree n\n",
    "\n",
    "    \n",
    "    # regression\n",
    "    reg = LinearRegression() # initiate the regression class\n",
    "    reg.fit(X_train_new,y_train) # fit the data\n",
    "    \n",
    "    # Out of Sample MSE:\n",
    "    mse=mean_squared_error(y_test, reg.predict(X_test_new))\n",
    "    print(f\"Polynomial regression of degree {n} has an out of sample MSE score of {mse}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd427332",
   "metadata": {},
   "source": [
    "## Sampling properties\n",
    "- Remember when talked sampling properties of OLS?\n",
    "- The smaller the sample size the less precise the estimate\n",
    "    - What if our 50/50 split happens to train our data on a fairly unrepresentative sample?\n",
    "    - let's confirm this using different 50/50 splits for our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c159d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomials=[1,2,3,4,5]\n",
    "DFs=[] # empty list of dataframes\n",
    "for n, p in enumerate(polynomials):\n",
    "    MSEs=[] # empty list of MSE scores\n",
    "    for i in range(len(df)):\n",
    "        # split the data (a different sample with each iteration)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df['horsepower'], df['mpg'], test_size=.5, random_state=i)\n",
    "\n",
    "        # prepocess the data\n",
    "        polynomial_features =PolynomialFeatures(degree=p,include_bias=False) # iniate a class to get data of degree n\n",
    "        X_train_new=polynomial_features.fit_transform(X_train.values.reshape(-1, 1)) # get data of degree n\n",
    "        X_test_new=polynomial_features.fit_transform(X_test.values.reshape(-1, 1)) # get data of degree n\n",
    "\n",
    "        # regression\n",
    "        reg = LinearRegression() # initiate the regression class\n",
    "        reg.fit(X_train_new,y_train) # fit the data\n",
    "\n",
    "        # Out of Sample MSE:\n",
    "        mse=mean_squared_error(y_test, reg.predict(X_test_new))\n",
    "        MSEs.append(mse)\n",
    "    # store all MSEs for degree p in a dataframe and append to list of dataframes    \n",
    "    DFs.append(pd.DataFrame({'Degree':p, 'MSE':MSEs}))\n",
    "\n",
    "dd=pd.concat(DFs)    \n",
    "# show the distribution of MSEs\n",
    "fig, ax = plt.subplots(1,1, figsize=(8,8))\n",
    "\n",
    "sns.kdeplot(data=dd, x='MSE', ax=ax, hue='Degree')\n",
    "ax.set_xlabel(\"MSE Scores - Validation Set Approach\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b809e70c",
   "metadata": {},
   "source": [
    "##  Leave-one-out cross-validation (LOOCV) \n",
    "- leave-one out cross validation is closely related to the validation set approach but it attempts to address that method’s drawbacks.\n",
    "- Like the validation set approach, _LOOCV_ involves splitting the set of observations into two parts. \n",
    "    - However, instead of creating two subsets of comparable size \n",
    "    - a single observation $(x_1, y_1)$ is used for the validation set\n",
    "    - the remaining observations $\\{(x_2, y_2), . . . , (x_n, y_n)\\}$ make up the training set\n",
    "- The statistical learning method is fit on the n − 1 training observations, and a prediction $\\hat{y}_1$ is made for the excluded observation, using its value x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ca2554a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/23/6hhxql6s4gx8qpsz5jbjby400000gn/T/ipykernel_7142/484877334.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Auto.csv\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'horsepower'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'horsepower'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"?\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# data cleaning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'horsepower'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mpg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# display info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "##  Leave-one-out cross-validation (LOOCV), continued\n",
    "- Since $(x_1, y_1)$ was not used in the fitting process\n",
    "    - $MSE_{1}$ = $(y_1-\\hat{y}_1)^2$\n",
    "- This provides an approximately __unbiased estimate for the test error__.\n",
    "- But even though MSE is unbiased for the test error, it is a poor estimate because it is highly variable, since it is based upon a single observation $(x_1, y_1)$.\n",
    "- We can repeat the procedure by selecting (x2, y2) and so on:\n",
    "    -     - $MSE_{(n)}$ = $(y_n-\\hat{y}_n)^2$\n",
    "- The LOOCV estimate for the test MSE is the average of these n test error estimates:\n",
    "    - $MSE_{(LOOCV)} = \\frac{1}{n} \\sum_{i=1}^n MSE_{i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d7e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "polynomials=[1,2,3,4,5]\n",
    "DFs=[] # empty list of dataframes\n",
    "for n, p in enumerate(polynomials):\n",
    "    MSEs=[] # empty list of MSE scores\n",
    "    for i in range(len(df)):\n",
    "        # split the data (a different sample with each iteration)\n",
    "        X_train, X_test, y_train, y_test = df.loc[df.index!=i,'horsepower'],df.loc[df.index==i,'horsepower'] , df.loc[df.index!=i,'mpg'],df.loc[df.index==i,'mpg'].values[0]\n",
    "\n",
    "        # prepocess the data\n",
    "        polynomial_features =PolynomialFeatures(degree=p,include_bias=False) # iniate a class to get data of degree n\n",
    "        X_train_new=polynomial_features.fit_transform(X_train.values.reshape(-1, 1)) # get data of degree n\n",
    "        X_test_new=polynomial_features.fit_transform(np.array(X_test).reshape(-1, 1)) # get data of degree n\n",
    "\n",
    "        # regression\n",
    "        reg = LinearRegression() # initiate the regression class\n",
    "        reg.fit(X_train_new,y_train) # fit the data\n",
    "\n",
    "        # Out of Sample MSE:\n",
    "        mse=np.square(y_test-reg.predict(X_test_new)[0]) # Out of Sample MSE\n",
    "        MSEs.append(mse)\n",
    "    # store all MSEs for degree p in a dataframe and append to list of dataframes    \n",
    "    DFs.append(pd.DataFrame({'Degree':p, 'MSE':MSEs}))\n",
    "\n",
    "dd=pd.concat(DFs)    \n",
    "display(dd.groupby('Degree').agg(Mean_MSE=('MSE','mean')))\n",
    "# show the distribution of MSEs\n",
    "fig, ax = plt.subplots(1,1, figsize=(8,8))\n",
    "sns.kdeplot(data=dd, x='MSE', ax=ax, hue='Degree')\n",
    "ax.set_xlabel(\"MSE Scores - LOOCV\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a61a9c",
   "metadata": {},
   "source": [
    "## k-Fold Cross-Validation\n",
    "- An alternative to LOOCV is k-fold cross validation. \n",
    "- This approach involves randomly dividing the set of observations into k groups, or __folds__, of approximately equal size. \n",
    "- The first fold is treated as a validation set, and the method is fit on the remaining k − 1 folds. \n",
    "- The mean squared error, $MSE_1$, is then computed on the observations in the held-out fold. \n",
    "- This procedure is repeated k times; each time, a different group of observations is treated as a validation set.\n",
    "- This process results in k estimates of the test error, $MSE_1$,$MSE_2$, . . . ,$MSE_k$. \n",
    "- The k-fold CV estimate is computed by averaging these values:\n",
    "    - $MSE_{(k-fold CV)} = \\frac{1}{k} \\sum_{i=1}^k MSE_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb45885",
   "metadata": {},
   "source": [
    "# k-fold Cross-Validation, continued\n",
    "- Note that LOOCV is a special case of k-fold CV in which k is set to equal n. \n",
    "- Standard practices consist in using k = 5 or k = 10. \n",
    "- What is the advantage of using k = 5 or k = 10 rather than k = n? \n",
    "    - Computation time which will increase with dataset sizes. \n",
    "    - LOOCV requires fitting the statistical learning method n times. \n",
    "        - This has the potential to be computationally expensive \n",
    "    - Performing 10-fold CV requires fitting the learning procedure only ten times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726a9a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X = df['horsepower'].values\n",
    "y = df['mpg'].values\n",
    "polynomials=[1,2,3,4,5]\n",
    "kDFs=[]\n",
    "for k in [5,10]:\n",
    "    DFs=[]\n",
    "    # Split in k folds\n",
    "    kf = KFold(n_splits=k, random_state=1706, shuffle=True)\n",
    "    for p in polynomials:\n",
    "        MSEs=[] # empty list of MSE scores\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            # prepocess the data\n",
    "            polynomial_features =PolynomialFeatures(degree=p,include_bias=False) # iniate a class to get data of degree n\n",
    "            X_train_new=polynomial_features.fit_transform(X_train.reshape(-1, 1)) # get data of degree n\n",
    "            X_test_new=polynomial_features.fit_transform(X_test.reshape(-1, 1)) # get data of degree n\n",
    "\n",
    "            # regression\n",
    "            reg = LinearRegression() # initiate the regression class\n",
    "            reg.fit(X_train_new,y_train) # fit the data\n",
    "\n",
    "            # Out of Sample MSE:\n",
    "            mse=mean_squared_error(y_test, reg.predict(X_test_new))\n",
    "            MSEs.append(mse)\n",
    "            # store all MSEs for degree p in a dataframe and append to list of dataframes    \n",
    "        DFs.append(pd.DataFrame({'Degree':p, 'k-fold':k,'MSE':MSEs}))\n",
    "\n",
    "    dd=pd.concat(DFs) \n",
    "    kDFs.append(dd)\n",
    "\n",
    "dd=pd.concat(kDFs)\n",
    "mse_change=dd.groupby(['Degree','k-fold']).agg(Mean_MSE=('MSE','mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd40337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the distribution of MSEs\n",
    "fig, axes = plt.subplots(2,2, figsize=(10,10))\n",
    "sns.kdeplot(data=dd.loc[dd['k-fold']==5], x='MSE', ax=axes[0,0], hue='Degree')\n",
    "axes[0,0].set_xlabel(\"MSE Scores - 5-fold Cross Validation\",fontsize=12)\n",
    "sns.kdeplot(data=dd.loc[dd['k-fold']==10], x='MSE', ax=axes[0,1], hue='Degree')\n",
    "axes[0,1].set_xlabel(\"MSE Scores - 10-fold Cross Validation\",fontsize=12)\n",
    "\n",
    "gs = axes[1, 1].get_gridspec()\n",
    "for ax in axes[1, :]:\n",
    "    ax.remove()\n",
    "axbottom = fig.add_subplot(gs[1, :])\n",
    "sns.lineplot(data=mse_change.reset_index(),x='Degree', y='Mean_MSE',hue='k-fold',alpha=.7, ax=axbottom)\n",
    "sns.scatterplot(data=mse_change.reset_index(),x='Degree', y='Mean_MSE',hue='k-fold',style='Degree', ax=axbottom,legend=False)\n",
    "axbottom.set_xlabel(\"Degrees of the Polynomial Regression\",fontsize=12)\n",
    "axbottom.set_ylabel(\"Mean Squared Errors\",fontsize=12)\n",
    "axbottom.set_xticks(np.arange(1,len(polynomials)+1))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc29be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Why do we use resampling methods?\n",
    "- Cross Validation is one of the several __resampling methods__ one can use to learn about predictive performance \n",
    "- Do we really care about the test MSE value we get from CV?\n",
    "    - If you want to know the performance of a given statistical model, yes\n",
    "        - e.g. Given my model how wrong should I expect my predictions to be \"on average\"?\n",
    "- Sometimes you may only be interested in the location of the minimum point in the test MSE\n",
    "    - If you want to compare perfomance of different models \n",
    "        - Like we did today\n",
    "    - The actual value of the test MSE is not important\n",
    "     - What matters is which models performs best\n",
    "         - within the same method using different levels of flexibility (like we did today)\n",
    "         - or across methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fada782",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bias-Variance Trade-Off, 1/3\n",
    "- LOOCV is a special case of k-fold (since k<n) \n",
    "- We said that a reason to privilege k-fold has to do with computational power\n",
    "    - There are not to be neglected\n",
    "    - But what if LOOCV performs better?\n",
    "- Turns out, there is another reason to use k-fold CV: __accuracy__\n",
    "    - k-fold validation usually gets closer to the true MSE than LOOCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52433f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bias-Variance Trade-Off - Bias Reduction, 2/3 \n",
    "\n",
    "- An issue with the _validation set_ approach is that it can overestimate the test MSE\n",
    "    - You train your model on half the data which can generate a __bias__\n",
    "- With LOOCV you use almost all (n-1) the data in your training\n",
    "     - This means that LOOCV will give approximately __unbiased estimates__ of the test error\n",
    "     - k-fold is somewhere in between the _validation set_ and _LOOCV_\n",
    "- So if you care about lowest bias, LOOCV is the best method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7317d9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bias-Variance Trade-Off - Variance Reduction, 3/3\n",
    "\n",
    "- But we know that bias is not the only source for concern in an estimating procedure (wink, t-test and p-values)\n",
    "    - we must also consider the procedure’s variance. \n",
    "- LOOCV has __higher variance__ than does k-fold CV with k < n. \n",
    "- Remember that, for both, you take averages of test squared errors\n",
    "    - You actually take n averages in LOOCV from almost identical training datasets\n",
    "    - This implies that test $MSE_{(LOOCV),i}$ are highly correlated \n",
    "        - k-fold are less correlated: overlap between training sets is smaller\n",
    "\n",
    "- The mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated\n",
    "- the test error estimate resulting from LOOCV tends to have higher variance than does the test error estimate resulting from k-fold CV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419a0f65",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
